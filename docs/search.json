[
  {
    "objectID": "introR.html",
    "href": "introR.html",
    "title": "Introduction to R and RStudio",
    "section": "",
    "text": "At a Glance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#what-is-statistics",
    "href": "introR.html#what-is-statistics",
    "title": "Introduction to R and RStudio",
    "section": "What is Statistics?",
    "text": "What is Statistics?\n\nStatistics is the methodology of extracting useful information from a data set.\nNumerical results are not very useful unless they are accompanied with clearly stated actionable business insights.\nTo do good statistical analysis, you must do the following:\n\nFind the right data.\nUse the appropriate statistical tools.\nClearly communicate the numerical information in written language.\n\nWith knowledge of statistics:\n\nAvoid risk of making uninformed decisions and costly mistakes.\nDifferentiate between sound statistical conclusions and questionable conclusions.\n\nData and analytics capabilities have made a leap forward.\n\nGrowing availability of vast amounts of data.\nImproved computational power.\nDevelopment of sophisticated algorithms.\nThe rise of Generative AI.\n\n\n\nTwo Main Branches of Statistics\n\nDescriptive Statistics - collecting, organizing, and presenting the data.\nInferential Statistics - drawing conclusions about a population based on sample data from that population.\n\nA population consists of all items of interest.\nA sample is a subset of the population.\nA sample statistic is calculated from the sample data and is used to make inferences about the population parameter.\n\nReasons for sampling from the population:\n\nToo expensive to gather information on the entire population.\nOften impossible to gather information on the entire population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#setting-up-r",
    "href": "introR.html#setting-up-r",
    "title": "Introduction to R and RStudio",
    "section": "Setting up R",
    "text": "Setting up R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#r-script-files",
    "href": "introR.html#r-script-files",
    "title": "Introduction to R and RStudio",
    "section": "R Script Files",
    "text": "R Script Files\n\nUsing R Script Files:\n\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nUsing the Console\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you must re-enter them at the command line. Nothing saves for later.\n\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise.R script files help to solve this issue.\n\n\nCreate a New R Script File\n\nTo save your notes from today’s lecture, create a .R file named Chapter1.R and save it to your project file you made in the last class.\nThere are a couple of parts to this chapter, and we can add code from today’s chapter in one file so that our code is stacked nicely together.\n\nFor each new chapter, start a new file and save it to your project folder.\n\n\n\n\nScreenshot of R Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#text-in-r",
    "href": "introR.html#text-in-r",
    "title": "Introduction to R and RStudio",
    "section": "Text in R",
    "text": "Text in R\n\nComments\n\nUse comments to organize and explain your code in R scripts by including 1 or more than 1 hashtag.\nAim to write clear, self-explanatory code that minimizes the need for excessive comments.\nAdd helpful comments where necessary to ensure anyone, including your future self, can understand and run the code.\nIf something doesn’t work, avoid deleting it immediately. Instead, comment it out while troubleshooting or exploring alternatives.\nEssentially, we add comments to our code to document our work and add notes to our self or to others.\n\n\n# This is a comment for documentation or annotation\n\n\nAdd the code above to your R file and run each line using Ctl + Enter (PC) or Cmd + Return (MAC) or select all lines and click Run.\nTake note that nothing prints in the console after running a comment.\n\n\n\nA Prolog\n\nA prolog is a set of comments at the top of a code file that provides information about what is in the file. It also names the files and resources used that facilitates identification. Including a prolog is considered coding best practice.\nOn your own R Script File, add your own prolog following the template as shown.\nAn informal prolog is below:\n\n\n####################################\n# Project name: Chapter 1\n# Project purpose: To create an R script file to learn about R. \n# Code author name: [Enter Your Name]\n# Date last edited: [Enter Date Here]\n# Data used: NA\n# Libraries used: NA\n####################################\n\n\nThen, as we work through our .R script and add data files or libraries to our code, we go back and edit the prolog.\n\n\n\n\nChapter 1 R File\n\n\n\n\nString\n\nIn R, a string is a sequence of characters enclosed in quotes, used to represent text data.\nR accepts single quotes or double quotes when marking a string. However, if you use a single quote to start, use a single quote to end. The same for double quotes - ensure the pairing is the same quote type.\nYou sometimes need to be careful with nested quotes, but generally it does not matter which you use.\n\n\n\"This is a string\"\n\n[1] \"This is a string\"\n\n\"This is also a string\"\n\n[1] \"This is also a string\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#note-on-r-markdown",
    "href": "introR.html#note-on-r-markdown",
    "title": "Introduction to R and RStudio",
    "section": "Note on R Markdown",
    "text": "Note on R Markdown\n\nThese files were formatted with RMarkdown. RMarkdown is a simple formatting syntax for authoring documents of a variety of types, including PowerPoint and html files.\nOn the document, RMarkdown prints the command and then follows the command with the output after 2 hashtags.\nIn your R Script File, you only need to type in the command and then run your code to get the same output as presented here.\n\n\n\n\nReading our HTML file",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#running-commands",
    "href": "introR.html#running-commands",
    "title": "Introduction to R and RStudio",
    "section": "Running Commands",
    "text": "Running Commands\n\nThere are a few ways to run commands via your .R file.\n\nYou can click Ctr + Enter on each line (Cmd + Return on a Mac).\nYou can select all the lines you want to run and select Ctr + Enter (Cmd + Return on a Mac).\nYou can select all the lines you want to run and select the run button as shown in the Figure.\n\n\n\n\n\nRun Code\n\n\n\nNow that I have asked you to add a couple lines of code, after this point, when R code is shown on this file, you should add it to your .R script file along with any notes you want. I won’t explicitly say - “add this code.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#r-is-an-interactive-calculator",
    "href": "introR.html#r-is-an-interactive-calculator",
    "title": "Introduction to R and RStudio",
    "section": "R is an Interactive Calculator",
    "text": "R is an Interactive Calculator\n\nAn important facet of R is that it should serve as your sole calculator.\nTry these commands in your .R file by typing them in and clicking Ctr + Enter on each line.\n\n\n3 + 4\n\n[1] 7\n\n3 * 4\n\n[1] 12\n\n3/4\n\n[1] 0.75\n\n3 + 4 * 100^2\n\n[1] 40003\n\n\n\nTake note that order of operations holds in R: PEMDAS\n\nParentheses ()\nExponents ^ and \\(**\\)\nDivision \\(/\\), Multiplication \\(*\\), modulo, and integer division\nAddition + and Subtraction -\n\nNote that modulo and integer division have the same priority level as multiplication and division, where modulo is just the remainder.\n\n\n2 + 3 * 5 - 7^2%%4 + (5/2)\n\n[1] 18.5\n\n5/2  #parentheses: = 2.5\n\n[1] 2.5\n\n2 + 3 * 5 - 7^2%%4 + 2.5\n\n[1] 18.5\n\n7^2  #exponent:= 49\n\n[1] 49\n\n2 + 3 * 5 - 49%%4 + 2.5\n\n[1] 18.5\n\n3 * 5  #multiplication: = 15\n\n[1] 15\n\n2 + 15 - 49%%4 + 2.5\n\n[1] 18.5\n\n49%%4  #modulo: = 1 -- 49/4 is 48.25, so 1/4 or 1 is the remainder. \n\n[1] 1\n\n# The rest of the equation goes in order from left to right.\n2 + 15 - 1 + 2.5\n\n[1] 18.5\n\n2 + 15  #addition: = 17\n\n[1] 17\n\n17 - 1  #subtraction: = 16\n\n[1] 16\n\n16 + 2.5  #addition: = 18.5\n\n[1] 18.5\n\n## An example of integer division is below\n49%/%4  #integer division: = 12 or 49/4 is 12.25, so the whole number is 12.  \n\n[1] 12",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#observations-and-variables",
    "href": "introR.html#observations-and-variables",
    "title": "Introduction to R and RStudio",
    "section": "Observations and Variables",
    "text": "Observations and Variables\n\nGoing back to the basics in statistics, we need to define an observation and variable so that we can know how to use them effectively in R in creating objects.\nAn Observation is a single row of data in a data frame that usually represents one person or other entity.\nA Variable is a measured characteristic of some entity (e.g., income, years of education, sex, height, blood pressure, smoking status, etc.).\nIn data frames in R, the columns are variables that contain information about the observations (rows).\n\nNote that we will break this code down later.\n\n\nincome &lt;- c(34000, 123000, 215000)\nvoted &lt;- c(\"yes\", \"no\", \"no\")\nvote &lt;- data.frame(income, voted)\nvote\n\n  income voted\n1  34000   yes\n2 123000    no\n3 215000    no\n\n\nObservations: People being measured.\nVariables: Information about each person (income and voted).\n\n\n# Shows the number of columns or variables\nncol(vote)\n\n[1] 2\n\n# Shows the number of rows or observations\nnrow(vote)\n\n[1] 3\n\n# Shows both the number of rows (observations and columns\n# (variables).\ndim(vote)\n\n[1] 3 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#creating-objects",
    "href": "introR.html#creating-objects",
    "title": "Introduction to R and RStudio",
    "section": "Creating Objects",
    "text": "Creating Objects\n\nEntering and Storing Variables in R requires you to make an assignment.\n\nWe use the assignment operator ‘&lt;-’ to assign a value or expression to a variable.\nWe typically do not use the = sign in R even though it works because it also means other things in R.\n\nSome examples are below to add to your .R file.\n\n\nstates &lt;- 29\nA &lt;- \"Apple\"\n# Equivalent statement to above - again = is less used in R.\nA = \"Apple\"\nprint(A)\n\n[1] \"Apple\"\n\n# Equivalent statement to above\nA\n\n[1] \"Apple\"\n\nB &lt;- 3 + 4 * 12\nB\n\n[1] 51\n\n\n\n\n\nThe Assignment Operator\n\n\n\nNaming Objects\n\nLine length limit: 80\nAlways use a consistent way of annotating code.\nCamel case is capitalizing the first letter of each word in the object name, with the exception of the first word.\nDot case puts a dot between words in a variable name while camel case capitalizes each word in the variable name.\nObject names appear on the left of assignment operator. We say an object receives or is assigned the value of the expression on the right.\n\n\nNaming Constants: A Constant contains a single numeric value.\n\n\nThe recommended format for constants is starting with a “k” and then using camel case. (e.g., kStates).\n\n\nNaming Functions: Functions are objects that perform a series of R commands to do something in particular.\n\n\nThe recommended format for Functions is to use Camel case with the first letter capitalized. (e.g., MultiplyByTwo).\n\n\nNaming Variables: A Variable is a measured characteristic of some entity.\n\n\nThe recommended format for variables is to use either the dot case or camel case. e.g., filled.script.month or filledScriptMonth.\nA valid variable name consists of letters, numbers, along with the dot or underline characters.\nA variable name must start with a letter, or the dot when not followed by a number.\nA variable cannot contain spaces.\nVariable names are case sensitive: x is different from X just as Age is different from AGE.\nThe value on the right must be a number, string, an expression, or another variable.\nSome Examples Using Variable Rules:\n\n\nAB.1 &lt;- \"Allowed?\"\n# Does not follow rules - not allowed Try the statement below with no\n# hashtag to see the error message .123 &lt;- 'Allowed?'\nA.A123 &lt;- \"Allowed?\"\nG123AB &lt;- \"Allowed?\"\n# Recommended format for constants\nkStates &lt;- 29\n\n\nDifferent R coders have different preferences, but consistency is key in making sure your code is easy to follow and for others to read. In this course, we will generally use the recommendation in the text which are listed above.\nWe tend to use one letter variable names (i.e., x) for placeholders or for simple functions (like naming a vector).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#built-in-functions",
    "href": "introR.html#built-in-functions",
    "title": "Introduction to R and RStudio",
    "section": "Built-in Functions",
    "text": "Built-in Functions\n\nR has thousands of built-in functions including those for summary statistics. Below, we use a few built-in functions with constant numbers. The sqrt(), max(), and min() functions compute the square root of a number, and find the maximum and minimum numbers in a vector.\n\n\nsqrt(100)\n\n[1] 10\n\nmax(100, 200, 300)\n\n[1] 300\n\nmin(100, 200, 300)\n\n[1] 100\n\n\n\nWe can also create variables to use within built-in functions.\nBelow, we create a vector x and use a few built-in functions as examples.\n\nThe sort() function sorts a vector from small to large.\n\n\nx &lt;- c(1, 2, 3, 3, 100, -10, 40)  #Creating a Vector x\nsort(x)  #Sorting the Vector x from Small to Large\n\n[1] -10   1   2   3   3  40 100\n\nmax(x)  #Finding Largest Element of Vector x\n\n[1] 100\n\nmin(x)  #Finding Smallest Element of Vector x\n\n[1] -10\n\n\n\n\nBuilt-in Functions: Setting an Argument\n\nThe standard format to a built-in function is functionName(argument)\n\nFor example, the square root function structure is listed as sqrt(x), where x is a numeric or complex vector or array.\n\n\n\n# Here, we are setting a required argument x to a value of 100. When\n# a value is set, it turns it to a parameter of the function.\nsqrt(x = 100)\n\n[1] 10\n\n# Because there is only one argument and it is required, we can\n# eliminate its name x= from our function call. This is discussed\n# below.\nsqrt(100)\n\n[1] 10\n\n\n\nThere is a little variety in how we can write functions to get the same results.\nA parameter is what a function can take as input. It is a placeholder and hence does not have a concrete value. An argument is a value passed during function invocation.\nThere are some default values set up in R in which arguments have already been set.\nThere are a few functions with no parameters like Sys.time() which produces the date and time. If you are not sure how many parameters a function has, you should look it up in the help.\n\n\n\nDefault Values\n\nThere are many default values set up in R in which arguments have already been set to a particular value or field.\nDefault values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\nWhen only one argument is required, the argument is usually not set to have a default value.\n\n\n\nBuilt-in Functions: Using More than One Argument\n\nFor functions with more than one parameter, we must determine what arguments we want to include, and whether a default value was set and if we want to change it. Default values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\n\nFor example, the default S3 method for the seq() function is listed as the following: seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),length.out = NULL, along.with = NULL, …)\nDefault values have been set on each parameter, but we can change some of them to get a meaningful result.\nFor example, we set the from, to, and by parameter to get a sequence from 0 to 30 in increments of 5.\n\n\n\n# We can use the following code.\nseq(from = 0, to = 30, by = 5)\n\n[1]  0  5 10 15 20 25 30\n\n\n\nWe can simplify this function call even further:\n\nIf we use the same order of parameters as the instructions, we can eliminate the argument= from the function.\nSince we do list the values to the arguments in same order as the function is defined, we can eliminate the from=, to=, and by= to simplify the statement.\n\n\n# Equivalent statement as above\nseq(0, 30, 5)\n\n[1]  0  5 10 15 20 25 30\n\n\nIf you leave off the by parameter, it defaults at 1.\n\n\n# Leaving by= to default value of 1\nseq(0, 30)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\n\n\nThere can be a little hurdle deciding when you need the argument value in the function call. The general rule is that if you don’t know, include it. If it makes more sense to you to include it, include it.\n\n\n\nTips on Arguments\n\nAlways look up a built-in function to see the arguments you can use.\nArguments are always named when you define a function.\nWhen you call a function, you do not have to specify the name of the argument.\nArguments have default values, which is used if you do not specify a value for that argument yourself.\nAn argument list comprises of comma-separated values that contain the various formal arguments.\nDefault arguments are specified as follows: parameter = expression\n\n\ny &lt;- 10:20\nsort(y)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\nsort(y, decreasing = FALSE)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#saving",
    "href": "introR.html#saving",
    "title": "Introduction to R and RStudio",
    "section": "Saving",
    "text": "Saving\n\nYou can save your work in the file menu or the save shortcut using Ctrl + S or Cmd+S depending on your Operating System.\nYou will routinely be asked to save your workspace image, and you don’t need to save this unless specifically asked. It saves the output we have generated so far.\nYou can stop this from happening by setting the Tools &gt; Global Options &gt; Under Workspace changing this to Never.\nBe careful with this option because it won’t save what you don’t run.\n\n\n\n\nEvalulating Your Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#calling-a-library",
    "href": "introR.html#calling-a-library",
    "title": "Introduction to R and RStudio",
    "section": "Calling a Library",
    "text": "Calling a Library\n\nIn R, a package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library.\nLibraries need to be activated one time in each new R session.\nYou can access a function from a library one time using library::function()\n\n\n# Use to activate library in an R session.\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\nYou can access a function from a library one time only using library::function()\n\nUseful if only using one function from the library.\n\nWe will return to this in data prep.\n\n\n## Below is an example that would use dplyr for one select function\n## to select variable1 from the oldData and save it as a new object\n## NewData. Since we don’t have datasets yet, we will revisit this.\n## NewData &lt;- dplyr::select(oldData, variable1)\n\n\nSome libraries are part of other global libraries:\n\ndplyr is part of tidyverse, there is actually no need to activate it if tidyverse is active, however, sometimes it helps when conflicts are present\nAn example of a conflict is the use of a select function which shows up in both the dplyr and MASS package. If both libraries are active, R does not know which to use.\ntidyverse has many libraries included in it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#the-environment",
    "href": "introR.html#the-environment",
    "title": "Introduction to R and RStudio",
    "section": "The Environment",
    "text": "The Environment\n\nYou can evaluate your Environment Tab to see your Variables we have defined in R Studio.\nUse the following functions to view and remove defined variables in your Global Environment\n\n\nls()  #Lists all variables in Global Environment \n\n [1] \"A\"       \"A.A123\"  \"AB.1\"    \"B\"       \"G123AB\"  \"income\"  \"kStates\"\n [8] \"states\"  \"vote\"    \"voted\"   \"x\"       \"y\"      \n\nrm(states)  #Removes variable named states\nrm(list = ls())  #Clears all variables from Global Environment\n\n\n\n\nEvalulating Your RStudio Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#entering-and-loading-data",
    "href": "introR.html#entering-and-loading-data",
    "title": "Introduction to R and RStudio",
    "section": "Entering and Loading Data",
    "text": "Entering and Loading Data\n\nCreating a Vector\n\nA vector is the simplest type of data structure in R.\n\nA vector is a set of data elements that are saved together as the same type.\nWe have many ways to create vectors with some examples below.\n\nUse c() function, which is a generic function which combines its arguments into a vector or list.\n\n\nc(1, 2, 3, 4, 5)  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nIf numbers are aligned, can use the “:“ symbol to include numbers and all in between. This is considered an array.\n\n\n1:5  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nUse seq() function to make a vector given a sequence.\n\n\nseq(from = 0, to = 30, by = 5)  #Creates a sequence vector from 0 to 30 in increments on 5 \n\n[1]  0  5 10 15 20 25 30\n\n\n\nUse rep() function to repeat the elements of a vector.\n\n\nrep(x = 1:3, times = 4)  #Repeat the elements of the vector 4 times\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nrep(x = 1:3, each = 3)  #Repeat the elements of a vector 3 times each\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\n\n\nCreating a Matrix\n\nA matrix is another type of object like a vector or a list.\n\nA matrix has a rectangular format with rows and columns.\nA matrix uses matrix() function\nYou can include the byrow = argument to tell the function whether to fill across or down first.\nYou can also include the dimnames() function in addition to the matrix() to assign names to rows and columns.\n\nUsing matrix() function, we can create a matrix with 3 rows and 3 columns as shown below.\n\nTake note how the matrix fills in the new data.\n\n\n# Creating a Variable X that has 9 Values.\nx &lt;- 1:9\n# Setting the matrix.\nmatrix(x, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Note – we do not need to name the arguments because we go in the\n# correct order.  The function below simplifies the statement and\n# provides the same answer as above.\nmatrix(x, 3, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\nSetting More Arguments in a Matrix\n\nThe byrow argument fills the Matrix across the row\nBelow, we can use the byrow statement and assign it to a variable m.\n\n\nm &lt;- matrix(1:9, 3, 3, byrow = TRUE)  #Fills the Matrix Across the Row and assigns it to variable m\nm  #Printing the matrix in the console\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\nThe dimnames() function adds labels to either the row and the column. In this case below both are added to our matrix m.\n\n\ndimnames(x = m) &lt;- list(c(\"2020\", \"2021\", \"2022\"), c(\"low\", \"medium\", \"high\"))\nm  #Printing the matrix in the console\n\n     low medium high\n2020   1      2    3\n2021   4      5    6\n2022   7      8    9\n\n\n\nYou try to make a matrix of 25 items, or a 5 by 5, and fill the matrix across the row and assign the matrix to the name m2.\nYou should get the answer below.\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n[5,]   21   22   23   24   25\n\n\n\n\nDifferences between Data Frames and Matrices\n\nIn a data frame the columns contain different types of data, but in a matrix all the elements are the same type of data. A matrix is usually numbers.\nA matrix can be looked at as a vector with additional methods or dimensions, while a data frame is a list.\n\n\n\n\nCreating a Data Frame\n\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. In a data frame the rows are observations and columns are variables.\n\nData frames are generic data objects to store tabular data.\nThe column names should be non-empty.\nThe row names should be unique.\nThe data stored in a data frame can be of numeric, factor or character type.\nEach column should contain same number of data items.\nCombing vectors into a data frame using the data.frame() function\n\nBelow, we can create vectors for state, year enacted, personal oz limit medical marijuana.\n\n\nstate &lt;- c(\"Alaska\", \"Arizona\", \"Arkansas\")\nyear.legal &lt;- c(1998, 2010, 2016)\nounce.lim &lt;- c(1, 2.5, 3)\n\n\nThen, we can combine the 3 vectors into a data frame and name the data frame pot.legal.\n\n\npot.legal &lt;- data.frame(state, year.legal, ounce.lim)\n\n\nNext, check your global environment to confirm data frame was created.\n\n\n\n\nGlobal Environment\n\n\n\n\nImporting a Data Frame into R\n\nWhen importing data from outside sources, you can do the following:\n\n\nYou can import data from an R package using data() function.\nYou can also link directly to a file on the web.\nYou can import data through from your computer through common file extensions:\n\n.csv: comma separated values;\n.txt: text file;\n.xls or .xlsx: Excel file;\n.sav: SPSS file;\n.sasb7dat: SAS file;\n.xpt: SAS transfer file;\n.dta: Stata file.\n\n\n\nEach different file type requires a unique function to read in the file. With all the variety in file types, it is best to look it up in the R Community to help.\n\n\nUse data() function\n\nAll we need is the data() function to read in a data set that is part of R. R has many built in libraries now, so there are many data sets we can use for testing and learning statistics in R.\n\n\n# The mtcars data set is part of R, so no new package needs to be\n# downloaded.\ndata(\"mtcars\")\n\n\nLoad a data frame from a unique package in R.\n\nThere are also a lot of packages that house data sets. It is fairly easy to make a package that contains data and load it into CRAN. These packages need to be installed into your R one time. Then, each time you open R, you need to reload the library using the library() function.\nWhen your run the install.packages() function, do not include the # symbol. Then, after you run it one time, comment it out. There is no need to run this code a second time unless something happens to your RStudio.\n\n\n# install.packages('MASS') #only need to install package one time in\n# R\nlibrary(MASS)\n\n\n\ndata(\"Insurance\")\nhead(Insurance)\n\n  District  Group   Age Holders Claims\n1        1    &lt;1l   &lt;25     197     38\n2        1    &lt;1l 25-29     264     35\n3        1    &lt;1l 30-35     246     20\n4        1    &lt;1l   &gt;35    1680    156\n5        1 1-1.5l   &lt;25     284     63\n6        1 1-1.5l 25-29     536     84\n\n\n\n\nAccessing Variables\n\nYou can directly access a variable from a dataset using the $ symbol followed by the variable name.\nThe $ symbol facilitates data manipulation operations by allowing easy access to variables for calculations, transformations, or other analyses. For example:\n\n\nhead(Insurance$Claims)  #lists the first 6 Claims in the Insurance dataset.\n\n[1]  38  35  20 156  63  84\n\nsd(Insurance$Claims)  #provides the standard deviation of all Claims in the Insurance dataset.\n\n[1] 71.1624",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#absolute-file-paths",
    "href": "introR.html#absolute-file-paths",
    "title": "Introduction to R and RStudio",
    "section": "Absolute File Paths",
    "text": "Absolute File Paths\n\nAbsolute vs. Relative Links\nAn absolute file path provides the complete location of a file, starting from the root directory of your computer.\n\nAlways points to the same file.\nIndependent of the script’s location.\nExample: read.csv(\"C:/Users/username/Documents/data.csv\")\n\nPro\n\nReliable for your system\nNo Ambiguity in locating files\n\nCons\n\nNot portable; requires the same file structure across systems.\nHarder to share code with collaborators.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#relative-file-paths",
    "href": "introR.html#relative-file-paths",
    "title": "Introduction to R and RStudio",
    "section": "Relative File Paths",
    "text": "Relative File Paths\n\nA relative file path specifies the file location based on the working directory of your R project or script.\n\nChanges based on the working directory.\nOften starts from the project folder.\nExample: read.csv(\"data/myfile.csv\")\n\nPros\n\nMore portable; works across systems if the project structure is consistent.\nEasier collaboration when sharing code and project files.\n\nCons\n\nRequires setting the working directory correctly (getwd() and setwd() can help).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#setting-up-a-working-directory",
    "href": "introR.html#setting-up-a-working-directory",
    "title": "Introduction to R and RStudio",
    "section": "Setting up a Working Directory",
    "text": "Setting up a Working Directory\n\nYou should have the data files from our LMS in a data folder on your computer. Your project folder would contain that data folder.\nBefore importing and manipulating data, you must find and edit your working directory to directly connect to your project folder!\nThese functions are good to put at the top of your R files if you have many projects going at the same time.\n\n\ngetwd()  #Alerts you to what folder you are currently set to as your working directory\n# For example, my working directory is set to the following:\n# setwd('C:/Users/Desktop/ProbStat') #Allows you to reset the working\n# directory to something of your choice.\n\n\nIn R, when using the setwd() function, notice the forward slashes instead of backslashes.\nYou can also go to Tools &gt; Global Options &gt; General and reset your default working directory when not in a project. This will pre-select your working directory when you start R.\nOr if in a project, like we should be, you can click the More tab as shown in the Figure below, and set your project folder as your working directory.\n\n\n\n\nSetting Your Working Directory\n\n\n##Reading in Data from .csv\n\nReading in a .csv file is extremely popular way to read in data.\nThere are a few functions to read in .csv files. And these functions would change based on the file type you are importing.\n\n\nread.csv() function\n\nExtremely popular way to read in data.\nread.csv() is a base R function that comes built-in with R: No library necessary\nAll your datasets should be in a data folder in your working directory so that you and I have the same working directory. This creates a relative path to our working directory.\nThe structure of the function is datasetName &lt;- read.csv(“data/dataset.csv”).\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n# or equivalently\ngss.2016 &lt;- read.csv(\"data/gss2016.csv\")\n# Examine the contents of the file\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n# Or equivalently, we can shorten this to the following code\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#read_csv-function",
    "href": "introR.html#read_csv-function",
    "title": "Introduction to R and RStudio",
    "section": "read_csv function",
    "text": "read_csv function\n\nread_csv() is a function from the readr package, which is part of the tidyverse ecosystem.\nread_csv() is generally faster than read.csv() as it’s optimized for speed, making it more efficient, particularly for large datasets.\nIn R, both read.csv() and read_csv() are used to read CSV files, but they come from different packages and have important differences. read.csv() is part of base R and is widely used for loading CSV files into data frames, as in data &lt;- read.csv(\"data/data.csv\"). It can be slower with large datasets and automatically converts strings to factors unless stringsAsFactors = FALSE.\nread_csv(), from the readr package in the tidyverse, is faster and better suited for large datasets. You’d use it like data &lt;- readr::read_csv(\"data/data.csv\"). It doesn’t convert strings to factors by default and provides clearer error messages.read_csv() is often preferred for performance and better handling of data types, especially in larger datasets or tidyverse projects.\n\n\n# install.packages(tidyverse) ## Only need to install one time on\n# your computer. #install.packages links have been commented out\n# during processing of RMarkdown.  Activate the library, which you\n# need to access each time you open R and RStudio\nlibrary(tidyverse)\n\n\n# Now open the data file to evaluate with tidyverse\ngss.2016b &lt;- read_csv(file = \"data/gss2016.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Lesson Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-qualitative-data",
    "href": "descriptives.html#summarizing-qualitative-data",
    "title": "Descriptive Statistics",
    "section": "Summarizing Qualitative Data",
    "text": "Summarizing Qualitative Data\n\nQualitative data is information that cannot be easily counted, measured, or easily expressed using numbers.\n\nNominal variables: a type of categorical variable that represents discrete categories or groups with no inherent order or ranking\n\ngender (male, female)\nmarital status (single, married, divorced)\neye color (blue, brown, green)\n\nOrdinal variables: categories possess a natural order or ranking\n\na Likert scale measuring agreement with a statement (e.g., strongly disagree, disagree, neutral, agree, strongly agree)\n\n\nA frequency distribution shows the number of observations in each category for a factor or categorical variable.\nGuidelines when constructing frequency distribution:\n\nClasses or categories are mutually exclusive (they are all unique).\nClasses or categories are exhaustive (a full list of categories).\n\nTo calculate frequencies, first, start with a variable that has categorical data.\n\n\n# Create a vector with some data that could be categorical\nSample_Vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"A\", \"B\", \"A\", \"C\", \"A\", \"B\")\n# Create a data frame with the vector\ndata &lt;- data.frame(Sample_Vector)\n\n\nTo count the number of each category value, we can use the table() command.\nThe output shows a top row of categories and a bottom row that contains the number of observations in the category.\n\n\n# Create a table of frequencies\nfrequencies &lt;- table(data$Sample_Vector)\nfrequencies\n\n\nA B C \n5 3 2 \n\n\n\nRelative frequency is how often something happens divided by all outcomes.\nThe relative frequency is calculated by \\(f_i/n\\), where \\(f_i\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nWe can use the prop.table() command to calculate relative frequency by dividing each category’s frequency by the sample size.\n\n\n# Calculate proportions\nproportions &lt;- prop.table(frequencies)\n\n\nThe cumulative relative frequency is given by \\(cf_i/n\\), where \\(cf_i\\) is the cumulative frequency of class \\(i\\).\nThe cumsum() function calculates the cumulative distribution of the data\n\n\n# Calculate cumulative frequencies\ncumulfreq &lt;- cumsum(frequencies)\n# Calculate cumulative proportions\ncumulproportions &lt;- cumsum(prop.table(frequencies))\n\n\nThe rbind() function is used to combine multiple data frames or matrices by row. The name “rbind” stands for “row bind”. Since the data produced by the table is in rows, we can use rbind to link them together.\n\n\n# combine into table\nfrequency_table &lt;- rbind(frequencies, proportions, cumulfreq, cumulproportions)\n# Print the table\nfrequency_table\n\n                   A   B    C\nfrequencies      5.0 3.0  2.0\nproportions      0.5 0.3  0.2\ncumulfreq        5.0 8.0 10.0\ncumulproportions 0.5 0.8  1.0\n\n\n\nWe can transpose a table using the t() command, which flips the dataset.\n\n\nTransposedData &lt;- t(frequency_table)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0\n\n\n\nFinally, sometimes we need to transform our calculations into a dataset.\nThe as.data.frame function is used to coerce or convert an object into a data frame.\nas.data.frame() is used when you have an existing object that needs to be coerced into a data frame. data.frame(), on the other hand, is for creating a data frame from scratch by specifying the data directly. Therefore, both as.data.frame() and data.frame() are used to convert or create data frames in R.\nas.data.frame() coerces an existing object (such as a list, matrix, or vector) into a data frame. Data.frame is used to create a new data frame from individual vectors or lists.\nas.data.frame() accepts a wider variety of inputs (like lists, matrices, and vectors), while data.frame() directly accepts vectors and lists to construct the data frame.\n\n\nTransposedData &lt;- as.data.frame(TransposedData)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-quantitative-data",
    "href": "descriptives.html#summarizing-quantitative-data",
    "title": "Descriptive Statistics",
    "section": "Summarizing Quantitative Data",
    "text": "Summarizing Quantitative Data\n\nDefining and Calculating Central Tendency\n\nThe term central location refers to how numerical data tend to cluster around some middle or central value.\nMeasures of central location attempt to find a typical or central value that describes a variable.\nWhy frequency distributions do not work for numeric variables:\n\nNumeric variables measured on a continuum.\nInstead, we calculate descriptive statistics including central tendency and spread of the values for a numeric variable.\n\nWe will examine the three mostly widely used measures of central location: mean, median and mode.\nThen we discuss a percentile: a measure of relative position.\n\n\nUsing the Mean\n\nThe arithmetic mean or simply the mean is a primary measure of central location. It is often referred to as the average. Simply add up all the observations and divide by the number of observations.\nThe numerator (top of the fraction) is the sum (sigma) of all the values of x from the first value (i = 1) to the last value (n) divided by the number of values (n).\n\\(m_x = (\\sum_{i=1}^{n} x_{i})/n\\)\nConsider the salaries of employees at a company: \nWe can use the mean() command to calculate the mean in R.\n\n\n# Create Vector of Salaries\nsalaries &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000)\n# Calculate the mean using the mean() command\nmean(salaries)\n\n[1] 154285.7\n\n\n\nNote that due to at least one outlier this mean does not reflect the typical salary - more on that later.\nIf we edit our vector to include NAs, we have to account for this. This is a common way to handle NAs in functions that do not allow for them.\n\n\nsalaries2 &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000, NA,\n    NA)\n# Calculate the mean using the mean() command Notice that it does not\n# work\nmean(salaries2)\n\n[1] NA\n\n# Add in na.rm parameter to get it to produce the mean with no NAs.\nmean(salaries2, na.rm = TRUE)\n\n[1] 154285.7\n\n\n\nNote that there are other types of means like the weighted mean or the geometric mean.\n\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\), where are the weights associated to the values.\nAn example is below.\n\n\nvalues &lt;- c(4, 7, 10, 5, 6)\nweights &lt;- c(1, 2, 3, 4, 5)\nweighted_mean &lt;- weighted.mean(values, weights)\nweighted_mean\n\n[1] 6.533333\n\n\n\n\nUsing the Median\n\nThe median is another measure of central location that is not affected by outliers.\nWhen the data are arranged in ascending order, the median is:\n\nThe middle value if the number of observations is odd, or\nThe average of the two middle values if the number of observations is even.\n\nConsider the sorted salaries of employees presented earlier which contains an odd number of observations.\n\nOn the same salaries vector created above, use median() command to calculate the median in R.\n\n\n# Calculate the median using the median() command\nmedian(salaries)\n\n[1] 90000\n\n\n\nNow compare to the mean and note the large difference in numbers signifying that at least one outlier is most likely present.\nSpecifically, if the mean and median are different, it is likely the variable is skewed and contains outliers.\n\n\nmean(salaries)\n\n[1] 154285.7\n\n\n\nFor another example, consider the sorted data below that contains an even number of values.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\n\n\nWhen data contains an even number of values, the median is the average of the 2 sorted middle numbers (12.56 and 13.47).\n\n\nmedian(GrowthFund)\n\n[1] 13.015\n\n(12.56 + 13.47)/2\n\n[1] 13.015\n\n# The mean is still the average\nmean(GrowthFund)\n\n[1] 10.088\n\n\n\n\nUsing the Mode\n\nThe mode is another measure of central location.\nThe mode is the most frequently occurring value in a data set.\nThe mode is useful in summarizing categorical data but can also be used to summarize quantitative data.\nA data set can have no mode, one mode (unimodal), two modes (bimodal) or many modes (multimodal).\nThe mode is less useful when there are more than three modes.\n\n\n\nExample of Function with Salary Variable\n\nWhile this is a small vector, when working with a large dataset and a function like sort(x = table(salaries), decreasing = TRUE), appending [1:5] is a way to focus on the top results after the frequencies have been computed and sorted. Specifically, table(salaries) calculates the frequency of each unique salary, sort(…, decreasing = TRUE) orders these frequencies from highest to lowest, and [1:5] selects the first five entries in the sorted list. This is useful when the dataset contains many unique values, as it allows you to quickly identify and extract the top 5 most frequent salaries, providing a concise summary without being overwhelmed by the full distribution.\nConsider the salary of employees presented earlier. 40,000 appears 2 times and is the mode because that occurs most often.\n\n\n# Try this command with and without it.\nsort(x = table(salaries), decreasing = TRUE)[1:5]\n\nsalaries\n 40000  65000  90000 145000 150000 \n     2      1      1      1      1 \n\n\n\n\nFinding No Mode\n\nLook at the sort(table()) commands with the GrowthFund Vector we made earlier.\nI added a 1:5 in square brackets at the end of the statement to produce the 3 highest frequencies found in the vector.\n\n\nsort(table(GrowthFund), decreasing = TRUE)[1:5]\n\nGrowthFund\n-38.32   1.71   3.17   5.99  12.56 \n     1      1      1      1      1 \n\n\n\nEven if you use this command, you still need to evaluate the data more systematically to verify the mode. If the highest frequency of the sorted table is 1, then there is no mode.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#defining-and-calculating-spread",
    "href": "descriptives.html#defining-and-calculating-spread",
    "title": "Descriptive Statistics",
    "section": "Defining and Calculating Spread",
    "text": "Defining and Calculating Spread\n\nSpread is a measure of distance values are from the central value.\nEach measure of central tendency has one or more corresponding measures of spread.\nMean: use variance or standard deviation to measure spread.\n\nskewness and kurtosis help measure spread as well.\n\nMedian: use range or interquartile range (IQR) to measure spread.\nMode: use the index of qualitative variation to measure spread.\n\nNot formally testing here with a function.\n\n\n\nSpread to Report with the Mean\n\nEvaluating Skewness\n\nSkewness is a measure of the extent to which a distribution is skewed.\nCan evaluate skewness visually with histogram.\n\nA histogram is a visual representation of a frequency or a relative frequency distribution.\nBar height represents the respective class frequency (or relative frequency).\nBar width represents the class width.\n\n\n\n\n\nEvaluating Skewness Visually\n\n\n\n\nSkewed Distributions: Median Not Same as Mean\n\nSometimes, a histogram is difficult to tell if skewness is present or if the data is relatively normal or symmetric.\nIf Mean is less than Median and Mode, then the variable is Left-Skewed.\nIf the Mean is greater than the Median and Mode, then the variable is Right-Skewed.\nIf the Mean is about equal to the Median and Mode, then the variable has a symmetric distribution.\nIn R, we can easily look at mean and median with the summary() command.\n\n\n\n\nEvaluating Skewness Using Mean and Median\n\n\n\nMean is great when data are normally distributed (data is not skewed).\nMean is not a good representation of skewed data where outliers are present.\n\nAdding together a set of values that includes a few very large or very small values like those on the far left of a left-skewed distribution or the far right of the right-skewed distribution will result in a large or small total value in the numerator of Equation and therefore the mean will be a large or small value relative to the actual middle of the data.\n\n\n\n\nUsing skew() Command in R\n\nThe skew() command is from the semTools package. The install.packages() command is commented out below, but install it one time on your R before commenting it out.\n\n\n# install the semTools package if necessary.\n# install.packages('semTools') Activate the library\nlibrary(semTools)\n\n\nAfter the package is installed and loaded, run the skew() command on the salaries vector made above.\n\n\nskew(salaries)\n\nskew (g1)        se         z         p \n    2.311     0.926     2.496     0.013 \n\n\n\n\nInterpreting the skew() Command Results\n\nse = standard error\nz = skew/se\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nSalary: Our sample size was small, &lt;50, so the z value of 2.496 in regards to the salary vector indicates there is a problem with skewness.\nGrowthFund: We can check the skew of GrowthFund.\n\n\nskew(GrowthFund)\n\nskew (g1)        se         z         p \n   -1.381     0.775    -1.783     0.075 \n\n\n\nGrowthFund was also considered a small sample size, so the same -2/2 thresholds are used. Here, our z value is -1.78250137, which is in normal range. This indicates there is no problem with skewness.\n\n\n\n\nHistograms\n\nA histogram is a graphical representation of the distribution of numerical data.\nIt consists of a series of contiguous rectangles, or bars, where the area of each bar corresponds to the frequency of observations within a particular range or bin of values.\nThe x-axis typically represents the range of values being measured, while the y-axis represents the frequency or count of observations falling within each range.\nHistograms are commonly used in statistics and data analysis to visualize the distribution of a dataset and identify patterns or trends.\nThey are particularly useful for understanding the central tendency, variability, and shape of the data distribution - this includes our observation of skewness.\nWorks much better with larger datsets.\n\n\nCommands to Make a Histogram\n\nhist() command in base R.\ngeom_histogram() command in ggplot2 package.\na hist using the GrowthFund dataset does not look that great because its sample size is so small.\n\n\nhist(GrowthFund)\n\n\n\n\n\n\n\n\n\n\nhist vs geom_histogram\n\nIn R, hist() and geom_histogram() are both used to create histograms, but they belong to different packages and have slightly different functionalities.\n\n\n# Making an appropriate data.frame to use the hist() command\nHousePrice &lt;- c(430, 520, 460, 475, 670, 521, 670, 417, 533, 525, 538,\n    370, 530, 525, 430, 330, 575, 555, 521, 350, 399, 560, 440, 425, 669,\n    660, 702, 540, 460, 588, 445, 412, 735, 537, 630, 430)\nHousePrice &lt;- data.frame(HousePrice)\n\n\nhist(): This function is from the base R graphics package and is used to create histograms. It provides a simple way to visualize the distribution of a single variable.\n\n\n# Using base R to create the histogram.\nhist(HousePrice$HousePrice, breaks = 5, main = \"A Histogram\", xlab = \"House Prices (in $1,000s)\",\n    col = \"yellow\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\ngeom_histogram(): This function is from the ggplot2 package, which is part of the tidyverse. It is used to create histograms as part of a more flexible and powerful plotting system.\n\n\n# Using geom_histogram() command to create the histogram.\nggplot(HousePrice, aes(x = HousePrice)) + geom_histogram(binwidth = 100,\n    boundary = 300, color = \"black\", fill = \"yellow\") + labs(title = \"A Histogram\",\n    x = \"House Prices (in $1,000s)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe could add more parameters here to make the 2 histograms look identical, but this configuration of parameters is very close. Take note that there are a lot more parameters you can add to the geom_histogram() command than you can with base R to make it look more professional. Be sure to look them up and also check with the notes in the book, which focuses on geom_histogram instead of hist().\nVariance is a measure of spread for numeric variables that is essentially the average of the squared differences between each observation value on some variable and the mean for that variable with population variance. \\[Population Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2}/N\\] \\[Sample Var(x) = s^2 = \\sum{(x_i-\\bar{x})^2}/(n-1)\\]\nStandard deviation is the square root of the variance.\n\nUse var() command and sd() command to calculate sample variance and sample standard deviation.\n\n\n\n## Calculated from Small Sample\nx &lt;- c(1, 2, 3, 4, 5)\nsum((x - mean(x))^2/(5 - 1))\n\n[1] 2.5\n\nvar(x)\n\n[1] 2.5\n\nsqrt(var(x))\n\n[1] 1.581139\n\nsd(x)\n\n[1] 1.581139\n\nsd(HousePrice$HousePrice)  #102.6059\n\n[1] 102.6059\n\nvar(HousePrice$HousePrice)  #10527.97\n\n[1] 10527.97\n\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n    0.317     0.408     0.777     0.437 \n\n\nLooking at Spread for a Larger Dataset\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\nsummary(customers$Spending, na.rm = TRUE)  #mean and median\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\nmean(customers$Spending, na.rm = TRUE)  #mean by itself\n\n[1] 659.555\n\nmedian(customers$Spending, na.rm = TRUE)  #median by itself\n\n[1] 662\n\n### Spread to Report with the Mean\nsd(customers$Spending, na.rm = TRUE)\n\n[1] 350.2876\n\nvar(customers$Spending, na.rm = TRUE)\n\n[1] 122701.4\n\n\n\n\nKurtosis in Evaluating Mean Spread\n\nKurtosis is the sharpness of the peak of a frequency-distribution curve or more formally a measure of how many observations are in the tails of a distribution.\nThe formula for kurtosis is as follows: Kurtosis = \\(\\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum \\left( \\frac{(X_i - \\bar{X})^4}{s^4} \\right) - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\n\nWhere:\n\n\\(n\\) is the sample size\n\\(X_i\\) is each individual value\n\\(\\bar{X}\\) is the mean of the data\n\\(s\\) is the standard deviation\nA normal distribution will have a kurtosis value of three, where distributions with kurtosis around 3 are described as mesokurtic, significantly higher than 3 indicate leptokurtic, and significantly under 3 indicate platykurtic.\nThe kurtosis() command from the semTools package subtracts 3 from the kurtosis, so we can evaluate values by comparing them to 0. Positive values will be indicative to a leptokurtic distribution and negative will indicate a platykurtic distribution. To see if kurtosis (leptokurtic or platykurtic) is significant, we confirm them by first evaluating the z-score to see if the variable is normal or not. The same cutoff values from skew also apply for the z for small, medium, and large sample sizes in kurtosis. These are the same basic rules for the rules in judging skewness.\n\n\n\n\nEvaluate Kurtosis\n\n\n\nThe rules of determining problematic distributions with regards to kurtosis are below.\n\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nIf kurtosis is found, then evaluate the excess kur score to see if it is positive or negative to determine whether it is leptokurtic or platykurtic.\n\n\n\n# z-value is 3.0398, which is &gt; 2 indicating leptokurtic Small sample\n# size: range is -2 to 2\nkurtosis(salaries)\n\nExcess Kur (g2)              se               z               p \n          5.629           1.852           3.040           0.002 \n\n# z-value is 2.20528007, which is &gt; 2 indicating leptokurtic Small\n# sample size: range is -2 to 2\nkurtosis(GrowthFund)\n\nExcess Kur (g2)              se               z               p \n          3.416           1.549           2.205           0.027 \n\n# Small sample size: range is -2 to 2 Skewness and kurtosis are both\n# in range.\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n    0.317     0.408     0.777     0.437 \n\nkurtosis(HousePrice$HousePrice)  #normal\n\nExcess Kur (g2)              se               z               p \n         -0.540           0.816          -0.661           0.508 \n\n\n\nLet’s do a few more examples using the customers dataset.\n\n\n# Noted sample size at 200 observations or a medium sample size.\n# Using threshold –3.29 to 3.29 to assess normality.\n\n#-3.4245446445 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$Spending)\n\nExcess Kur (g2)              se               z               p \n         -1.186           0.346          -3.425           0.001 \n\ngeom_histogram(binwidth = 100, fill = \"pink\", color = \"black\")\n\ngeom_bar: na.rm = FALSE, orientation = NA\nstat_bin: binwidth = 100, bins = NULL, na.rm = FALSE, orientation = NA, pad = FALSE\nposition_stack \n\nsemTools::skew(customers$Spending)  ##normal indicating no skewness\n\nskew (g1)        se         z         p \n   -0.018     0.173    -0.106     0.916 \n\n# Normal: 2.977622119 is in between -3.29 and 3.29\nkurtosis(customers$Income)\n\nExcess Kur (g2)              se               z               p \n          1.031           0.346           2.978           0.003 \n\nggplot(customers, aes(Income)) + geom_histogram(binwidth = 10000, fill = \"pink\",\n    color = \"black\")\n\n\n\n\n\n\n\nsemTools::skew(customers$Income)  #Skewed right\n\nskew (g1)        se         z         p \n    0.874     0.173     5.047     0.000 \n\n#-3.7251961028 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$HHSize)\n\nExcess Kur (g2)              se               z               p \n         -1.290           0.346          -3.725           0.000 \n\nggplot(customers, aes(HHSize)) + geom_histogram(binwidth = 1, fill = \"pink\",\n    color = \"black\")\n\n\n\n\n\n\n\nsemTools::skew(customers$HHSize)  #normal\n\nskew (g1)        se         z         p \n   -0.089     0.173    -0.513     0.608 \n\n# Normal: -0.20056607 is in between -3.29 and 3.29\nkurtosis(customers$Orders)\n\nExcess Kur (g2)              se               z               p \n         -0.069           0.346          -0.201           0.841 \n\ngeom_histogram(binwidth = 5, fill = \"pink\", color = \"black\")\n\ngeom_bar: na.rm = FALSE, orientation = NA\nstat_bin: binwidth = 5, bins = NULL, na.rm = FALSE, orientation = NA, pad = FALSE\nposition_stack \n\nsemTools::skew(customers$Orders)  ##skewed right\n\nskew (g1)        se         z         p \n    0.789     0.173     4.553     0.000 \n\n\n\n\n\nSpread to Report with the Median\n\nRange = Maximum Value – Minimum Value.\n\nSimplest measure.\nFocuses on Extreme values.\nUse commands diff(range()) or max() – min().\n\nIQR: Difference between the first and third quartiles.\n\nUse IQR() command or quantile() command.\n\n\nsummary(customers$Spending, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\ndiff(range(customers$Spending, na.rm = TRUE))\n\n[1] 1200\n\nmax(customers$Spending, na.rm = TRUE) - min(customers$Spending, na.rm = TRUE)\n\n[1] 1200\n\nIQR(customers$Spending, na.rm = TRUE)\n\n[1] 578.5\n\n\n\n\n\nSpread to Report with the Mode\n\nWhile there is no great function to test for spread, you can look at the data and see if it is concentrated around 1 or 2 frequencies. If it is, then the spread is distorted towards those high frequency values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#using-ai",
    "href": "descriptives.html#using-ai",
    "title": "Descriptive Statistics",
    "section": "Using AI",
    "text": "Using AI\nUse the following prompts on a generative AI, like chatGPT, to learn more about descriptive statistics.\n\nWhat is the difference between mean, median, and mode in describing data distributions, and how can each be used to understand the shape of a distribution? * How do mean and median help identify whether a distribution is skewed, and what does it tell us about the dataset?\nCan you explain how the mean, median, and mode behave in normal, positively skewed, and negatively skewed distributions?\nWhat are standard deviation (SD) and variance, and how do they measure the spread of data in a distribution?\nExplain the differences between range, interquartile range (IQR), and standard deviation in describing the variability in a dataset.\nHow does a high standard deviation or variance affect the interpretation of a dataset compared to a low standard deviation?\nWhat is skewness, and how does it affect the shape of a distribution? How can we identify positive and negative skew?\nHow is kurtosis defined in the semTools package in R, and what does it tell us about the tails of a distribution?\nHow would you compare and contrast the roles of skewness and kurtosis in identifying the shape and behavior of a distribution?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summary",
    "href": "descriptives.html#summary",
    "title": "Descriptive Statistics",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson, we worked through descriptive statistics including skewness and kurtosis. We learned about variables and scales of measurement, how to summarize qualitative and quantitative data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability And Business Statistics",
    "section": "",
    "text": "Why R?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 0: Software Know-How</span>"
    ]
  },
  {
    "objectID": "index.html#customizing-rstudio",
    "href": "index.html#customizing-rstudio",
    "title": "Probability And Business Statistics",
    "section": "Customizing RStudio",
    "text": "Customizing RStudio\n\nCustomizing RStudio through global options allows you to create a coding environment that aligns with your workflow and personal preferences. By tailoring settings such as appearance, pane layout, and editor behavior, you can improve productivity, enhance readability, and streamline your data analysis process. These adjustments not only make coding more efficient but also help you avoid common pitfalls, like workspace clutter or inconsistent settings. RStudio is highly flexible, so take the time to explore and experiment with its options to make the platform work best for you.\n\n\n\n\nWhat are Global Options\n\nSettings in RStudio that let you personalize the environment to suit your workflow.\n\nWhy change global options?\n\nImprove productivity by tailoring RStudio to your preferences.\nEnhance readability and usability of code and outputs.\nCreate a consistent environment across projects.\n\nWhere to Find Global Options:\n\nGo to Tools &gt; Global Options in the RStudio menu.\n\nKey Settings to Adjust\n\nGlobal Options &gt; General: You can set your general information including your default working directory (when not in a project).Set your R Version if appropriate.\nGlobal Options &gt; Code: Enable auto-complete, or soft-wrap for easier coding.\nGlobal Options &gt; Appearance: You can customize the appearance to a theme that accommodates your learning style and visual preferences.\nGlobal Options &gt; Spelling: You can turn on a spell check.\n\n\n\n\n\nTools &gt; Global Options.R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 0: Software Know-How</span>"
    ]
  },
  {
    "objectID": "index.html#quick-keys-in-r",
    "href": "index.html#quick-keys-in-r",
    "title": "Probability And Business Statistics",
    "section": "Quick Keys in R",
    "text": "Quick Keys in R\n\nThere are a lot of quick keys in R to make you able to use it faster and more effectively. You may look over these and try on your own.\n\n\n\n\nConsole Quick Keys\n\n\n\n\n\nSource Quick Keys\n\n\n\n\n\nEditing Quick Keys",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 0: Software Know-How</span>"
    ]
  },
  {
    "objectID": "index.html#getting-help-in-r",
    "href": "index.html#getting-help-in-r",
    "title": "Probability And Business Statistics",
    "section": "Getting Help in R",
    "text": "Getting Help in R\n\nThere are lots of ways to get help in R.\nIn R, use the help search box to find information on a function, parameter, or package.\n\n?mean\nhelp.search(‘swirl’)\nhelp(package = ‘MASS’)\n?install.packages\n\n\n\n\n\nGetting Help\n\n\n\nYou should try to look up the tapply command to see what it does.\n\nUse ?tapply in your .R file to pull up tapply() command or type tapply in the Help box. * Formally, you should see that the command applies a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors. This means that the command does some math calculation (mean, sum, etc.) on a continuous variable after dividing the data by group.\n\nThe format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean.\nMore on that function later in the first lessons.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 0: Software Know-How</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "At a Glance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#layering-in-ggplot",
    "href": "dataviz.html#layering-in-ggplot",
    "title": "Data Visualization",
    "section": "Layering in ggplot",
    "text": "Layering in ggplot\n\nLayering is a fundamental concept in ggplot2 that allows you to build complex visualizations by adding different components (or “layers”) on top of each other. Each layer in a ggplot2 plot can represent different types of data, aesthetics, or annotations, enabling flexibility and control over how data is visualized. Layers can include geometries (e.g., points, lines, bars), statistical summaries, labels, grids, and themes.\nSeparation of Plot Components: Each layer can handle a different part of the plot. For example, one layer may be used for bars, another for labels, and another for a trend line. This allows you to build up a plot step by step.\nCustomization and Enhancement: By adding multiple layers, you can customize different aspects of the plot such as labels, colors, annotations, and theme elements. Each layer can be independently controlled.\nModularity: Layering allows you to modularize your plot construction, making it easier to add, remove, or modify parts of the plot without changing the entire structure.\nCombining Data Sources: Different layers can use different datasets or aesthetics, which is useful when you need to overlay one dataset on top of another (e.g., adding a regression line over a scatter plot).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-categorical-variable",
    "href": "dataviz.html#graphs-for-a-single-categorical-variable",
    "title": "Data Visualization",
    "section": "Graphs for a Single Categorical Variable",
    "text": "Graphs for a Single Categorical Variable\n\nA categorical variable has categories that are either ordinal with a logical order or nominal with no logical order.\nCategorical variables need to be set as the factor data type in R to be able to be analyzed and visualized correctly.\nSome common graphing options for single categorical variable:\n\nBar graph\nPie chart\n\nIn any graph, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With categorical variables, this may require recoding the factor(s) of interest and possibly renaming it/them to something meaningful if needed.\n\n\nBar Graph\n\nA bar graph depicts the frequency or the relative frequency for each category of the qualitative data as a bar rising vertically from the horizontal axis. A bar graph is also known as a bar chart and is often used to examine similarities and differences across categories of things; bars can represent frequencies, percentages, means, or other statistics.\nWe can learn a lot from a bar graph, like the marital status group with the highest and lowest frequencies according to the census.gov.\n\n\n\n\n\n\n\n\n\n\n\ngeom_bar()\n\nCreate a Bar Graph using ggplot() Command\nLike histograms, ggplot has many more parameters available over base R to construct bar graphs.\nFirst, we load tidyerse to access ggplot() command and others. You can always do this at the start of all your code to keep all the libraries together that are being used.\n\n\nlibrary(\"tidyverse\")\n\n\nggplot() works in layers, so you will routinely see the + symbol to kick off a new layer with added functionality.\nUsing the ggplot, we always include the aes() command first inside the ggplot() command. The aes() command is a quoting function that describes the variables being used. From there, it depends on the plot.\n\nFirst layer: ggplot() and aes() which calls the dataset and variables used.\nSecond layer: Graph type: Bar graph: geom_bar().\nAdditional layers: labs - for labels including titles; themes; and geom_text. Recreate the example below adding one layer at a time to see how the visualization changes.\n\n\n\n\nPertinent Parameters to the geom_bar()\n\nstat=“identity”: In ggplot2, the argument stat=“identity” is used inside the geom_bar when creating bar charts to specify that the actual values of the data should be plotted, rather than calculated summary statistics. By default, ggplot assumes that bar charts use stat=“count”, which counts the number of observations in each category. When you want to display the actual values in your dataset (e.g., sales numbers, average scores), you need to set stat=“identity” to ensure that the y-values are mapped directly from the data rather than being automatically counted.\nposition=“dodge”: If we have more than one categorical variable, we might set the position=“dodge” argument. This argument controls how the bars are positioned relative to each other when you are plotting multiple categories within a bar chart. By default, bars in a bar chart are stacked, and setting position=“dodge” ensures that they are placed side-by-side. This is particularly useful when you are comparing multiple groups or categories within the same plot, as it allows for a clear visual distinction between each group. In the example below, we only have one variable, so position=“dodge” argument is not needed.\n\nIn combination, using stat=“identity” and position=“dodge” is common when you want to compare different categories or groups with specific values in a side-by-side manner, ensuring the chart is easy to interpret. For example, if you are comparing sales figures across different products for multiple years, this approach would give you a bar for each product in each year, clearly separated.\nshow.legend = FALSE: In ggplot2, the argument show.legend = FALSE is used to hide the legend for a particular layer or the entire plot. By default, ggplot2 automatically adds a legend if you include aesthetics like color, size, or shape that distinguish groups in your data. If you don’t want the legend to appear, you can set show.legend = FALSE.\nscale_fill_manual(): When creating grouped bar charts or other plots with multiple categories, it’s important to ensure that the visual distinction between groups is clear. This is where manually setting the colors for each category comes into play. The scale_fill_manual() function in ggplot2 allows you to manually define the colors used for the filled areas, such as the bars in a bar chart or the shaded areas in an area chart. By using the values argument within scale_fill_manual(), you can specify a custom color palette that suits your design or presentation needs. For example, you might choose a palette of yellow and brown to represent different categories. A key aspect of using scale_fill_manual() effectively is knowing how many colors to provide. The number of colors you define in the values argument must match the number of levels or categories in your data. If you are comparing three product categories, for instance, you’ll need to provide exactly three colors—one for each category. Failing to match the number of colors to the number of categories can result in errors or misrepresentation in the plot. For example, if you have four levels (e.g., four different product categories or groups) and only provide two colors, ggplot may not know how to properly assign colors to the additional categories. Therefore, it’s crucial to know the number of distinct categories in your data and plan your color palette accordingly to maintain clarity and visual consistency in your chart.\n\n\n## inputting probabilities calculated from a 2023 multiple choice\n## question.  From what you learned about R so far, how do you expect\n## its market share to change?\nGoUp &lt;- 0.54285\nGoDown &lt;- 0.03809\nRemainStable &lt;- 0.34285\nNoOpinion &lt;- 0.07619\n# designing the data frame\ndata_frame &lt;- data.frame(Category = c(\"Go Up\", \"Go Down\", \"Remain Stable\",\n    \"No Opinion\"), Percentage = c(GoUp, GoDown, RemainStable, NoOpinion))\n# Making the graph\nMarketShare &lt;- ggplot(data_frame, aes(x = Category, y = Percentage, fill = Category)) +\n    geom_bar(stat = \"identity\", show.legend = FALSE) + labs(title = \"How do you expect R's market share to change?\",\n    x = \"Opinion Category\", y = \"Percentage (%)\") + theme_minimal() + geom_text(aes(label = Percentage),\n    vjust = -0.5, size = 4) + scale_fill_manual(values = c(\"red\", \"blue\",\n    \"purple\", \"green\"))\n\nMarketShare\n\n\n\n\n\n\n\n\n\n\n\nBar Graph with Data Wrangling\n\nLet’s start an example from scratch so that we can see each parameter take effect. In doing so, lets use a dataset to make a bar graph instead of relying on pre-calculated data.\n\nLets examine the AUQ300 variable from the nhanes survey to run an example.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n\n\nNext, we need to check the import by looking at the summary or head of the data.\n\n\n# Results hidden to save space, but gives you the first 6 records in\n# the data set.\nhead(nhanes)\n\n\nWe can also check the summary of data of only the variable of interest, AUQ300, to get a sense of what we are evaluating.\n\n\nsummary(nhanes$AUQ300)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   2.000   1.656   2.000   7.000    4689 \n\n\n\nThe AUQ300 variable represents gun use. A screenshot of the codebook is copied below so that we can see what AUQ300 really refers to. It is available on https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm. This is always a necessary step because variable names can be convoluted and not representative of the variable definition.\n\n\n\n\nAUQ300\n\n\n\nRecode Variable if Needed\n\nLook to see if the AUQ300 needs recoding after looking at the codebook and making sense of the variable.\nAUQ300 needs to be a factor variable with 1 equaling a Yes and 2 equaling a No. We can use recode_factor to accomplish 2 things at once with the mutate function.\nrecode_factor() transforms the levels of a categorical variable (factor) into a new set of levels and is specific to categorical variables.\nrecode() is generic and can apply to numerical, categorical, or textual data, but still transforms data from one format or code to another.\n\n\nnhanes.clean &lt;- nhanes %&gt;%\n    select(AUQ300) %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\"))\n\n\nThen, we need to check the recode for accuracy. You should see the No’s and Yes’s alongside the rest being coded as NA’s.\n\n\nsummary(nhanes.clean)\n\n  AUQ300    \n Yes :1613  \n No  :3061  \n NA's:4690  \n\n\n\n\nGet Bar Roughly Plotted\n\nStart with the basic plot using the ggplot() and geom_bar() commands.\nBelow writes the statement with and without the piping operator.\n\nSince we are also going to use data preparation techniques, the piping operator is recommended.\n\n\n# Without piping operator\nggplot(nhanes.clean, aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n# With piping operator\nnhanes.clean %&gt;%\n    ggplot(aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nAdd Functions to Clean Chart\n\nOmit the NA category from AUQ300 variable, which represents gun use. Then plot the graph below.\nThe drop_na() function is a good way to drop NA values from either the entire dataset or just one variable. It was introduced in the data prep lesson. Since we are only interested in dropping NA values from our one variable of interest that is to be graphed (AUQ300), we can put it in the parentheses so that we do not unintentionally drop lots of observations for no reason.\nAdd an axis labels under labs(x = …, y=…).\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(x = AUQ300)) + geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\")\n\n\n\n\n\n\n\n##Here, we really benefit from the piping operator because we are doing more than one thing.\n\n\nFrom the bar graph, we can see that almost double the amount of people have not fired a firearm for any reason than those that fired one.\n\n\n\nAdding Color\n\nThere are many ways to add color to a bar graph. Below, the color is filled in directly in the aes() command by choosing it to give a different color to each categorical value of AUQ300.\n\nWhen fill is mapped to a variable, the fill color of the geom will vary based on the values of that variable. This is useful for distinguishing different groups or categories within the data. In this case the fill=AUQ300 gives a distinct color pattern based on how many categories there are considering the fact we are using the default “scale.”\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(AUQ300, fill=AUQ300)) +\n  geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\", \n       subtitle = \"Filled inside the aes()\") \n\n\n\n\n\n\n\n\n\n\n\nData Prep and Then Visualized\n\nIn the command below, we create a gss.2016.cleaned object to make a barplot. In doing so, we do the following:\nCreate a bar graph using the ggplot() command, which requires an aes() quoting function. This function says that we want to use the grass variable in our bar graph.\nDrop all NAs from the grass variable so that legal and not legal are the only categories.\nWe then create the bars and fill them with 2 colors, red and purple. Many color codes can be used here, and will be discussed in a later lesson.\nWe then add labels to our graph on both x and y axis.\nFinally, we print the new graph, which is saved under the legalize.bar object.\nBelow, I brought back over the code from the last part in Data Preparation. You should still have this in your Chapter1.R file. We are going to use that file to create a graphics in R.\n\n\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(x = grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    mutate(age = as.numeric(x = age)) %&gt;%\n    mutate(age.cat = cut(x = age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n        \"30 - 59\", \"60 - 74\", \"75+\")))\n\n\nOnce the data is prepped, we can graph the variable or variables.\n\n\nggplot(gss.2016.cleaned, aes(grass)) + geom_bar()  ##with no piping operator\n\n\n\n\n\n\n\ngss.2016.cleaned %&gt;%\n    ggplot(aes(grass)) + geom_bar()  ##with piping operator\n\n\n\n\n\n\n\n\n\n# Make a Bar Graph for Grass Variable\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(grass)) + geom_bar(fill = c(\"red\", \"blue\")) + labs(x = \"Should marijuana be legal\",\n    y = \"Frequency of Responses\")\n\n\n\n\n\n\n\n\n\n\nEdit The Graphic\n\nNext, we can edit these commands to include the age variable. The aes() quoting function has expanded to have the bars filled color using the grass variable, the age category has replaced the grass variable on the x axis.\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, fill = grass)) + geom_bar() + labs(x = \"Age Category\",\n    y = \"Frequency of responses\")\n\n\n\n\n\n\n\n\n\nWe can add the position set at “dodge” inside the geom_bar() layer to make the barchart unstacked (or grouped).\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, fill = grass)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Age Category\", y = \"Frequency of responses\")\n\n\n\n\n\n\n\n\n\nWe can edit further to include a new a formula on the y axis to sum and count.\nIn the formula you provided, after_stat(count) is used within the ggplot2 framework to refer to the computed statistic generated by the geom_bar() function. Specifically, in the context of bar plots, count refers to the number of observations (or frequency) for each category within the data.\nWhen you use after_stat(count), you are referencing the count that is computed after geom_bar() has processed the data and calculated how many observations fall into each group (in this case, within each age category and the “grass” variable, which likely refers to attitudes toward marijuana legalization).\nWe also gave this a theme and updated the labels.\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, y = 100 * (after_stat(count))/sum(after_stat(count)),\n        fill = grass)) + geom_bar(position = \"dodge\") + theme_minimal() +\n    labs(x = \"Age Category\", y = \"Percent of responses\")\n\n\n\n\n\n\n\n\n\nEvaluate these graphs and see what information you can get from them.\n\n\n\n\nPie Chart\n\nA pie chart is a segmented circle whose segments portray the relative frequencies of the categories of a qualitative variable.\nSlices of pie in different colors represent the parts.\nIn this example, the firearm is divided by type to show parts of a whole, where the total of the proportions must add to 1.0 and the total of the percentages must add to 100%.\n\n\n# Importing data from working directory\nfbi.deaths &lt;- read.csv(\"data/fbi_deaths.csv\", stringsAsFactors = TRUE)\n# Selecting rows of interest for pie chart\nfbi.deaths.small &lt;- fbi.deaths[c(3, 4, 5, 6, 7), ]\nfbi.deaths.small &lt;- fbi.deaths.small %&gt;%\n    rename(Weapon = X)\n# Checking summary of fbi deaths\nsummary(fbi.deaths.small)\n\n                       Weapon      X2012          X2013          X2014     \n Firearms, type not stated:1   Min.   : 116   Min.   : 123   Min.   :  93  \n Handguns                 :1   1st Qu.: 298   1st Qu.: 285   1st Qu.: 258  \n Other guns               :1   Median : 310   Median : 308   Median : 264  \n Rifles                   :1   Mean   :1779   Mean   :1691   Mean   :1662  \n Shotguns                 :1   3rd Qu.:1769   3rd Qu.:1956   3rd Qu.:2024  \n Asphyxiation             :0   Max.   :6404   Max.   :5782   Max.   :5673  \n (Other)                  :0                                               \n     X2015          X2016     \n Min.   : 177   Min.   : 186  \n 1st Qu.: 258   1st Qu.: 262  \n Median : 272   Median : 374  \n Mean   :1956   Mean   :2201  \n 3rd Qu.:2502   3rd Qu.:3077  \n Max.   :6569   Max.   :7105  \n                              \n\n\n\nAgain, ggplot works in layers, so in order to make a pie, you need a few layers and have a few optional ones.\n\nThe aes() command specifies the variable to create the pie, in this case x2016.\ngeom_col() sets the borders of the pie and makes it visible.\ncoord_polar() command makes the pie circular.\ntheme_void() command is optional and adjusts the theme of the pie. to remove axis, background, etc.\n\n\n\nggplot(fbi.deaths.small, aes(x=\"\", y=X2016, fill=Weapon)) +\n  geom_col() + \n  coord_polar(\"y\", start=0) + \n  theme_void() \n\n\n\n\n\n\n\n\n\nFrom the pie, we can see that the majority of weapons that caused fbi gun related deaths are handguns followed by a type of firearm that is not stated.\n\n\n\nComparison of Charts\n\nRecommended graphs for single categorical or factor type variable:\n\nBar graph, for showing relative group sizes.\nPie charts are available in R but are not recommended because they tend to be less clear for comparing group sizes.\n\nPie charts are difficult to read since the relative size of pie pieces is often hard to determine.\nPie charts take up a lot of space to convey little information.\nPeople often use fancy formatting like 3D, which takes up more space and makes understanding relative size of pie pieces even more difficult.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-continuous-variable",
    "href": "dataviz.html#graphs-for-a-single-continuous-variable",
    "title": "Data Visualization",
    "section": "Graphs for a Single Continuous Variable",
    "text": "Graphs for a Single Continuous Variable\n\nA continuous variable refers to a variable that can take any value over a range of values.\nA continuous variable needs to be numeric, and could be integer type or numeric type in R. Just like with graphs that include categorical variables, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With continuous variables, this may require recoding the variable to coerce it to the appropriate data type and/or renaming it to something meaningful if needed.\nIt is also beneficial to make sure the numerical variable is indeed supposed to be numerical (as opposed to a factor). For instance, you commonly see numbers listed for categories like the Yes/No coded as a 1/2, such as with the AUQ300 variable.\n\nSome common graphing options for single continuous variable:\n\nHistograms (From Lesson 2)\nDensity plots\nBoxplots\nViolin plots\n\n\nHistograms\n\nA histogram is a useful plot to determine central tendency and spread.\nWe went over histograms in Lesson 2, so refer back for information on how to create a histogram using base R and ggplot.\nRemember that you can tell the distribution from a histogram, and that distribution can be normal or skewed (Right or Left).\n\nWith each chart based on quantitative data, you should be able to get a sense of the distribution.\nThe histogram below looks right skewed.\n\n\n\n\n\n\n\n\n\n\n\n\nDensity Plots\n\nA density plot is similar to a histogram but more fluid in appearance because it does not have the separate bins.\n\nProbability density is not very useful for interpreting what is happening at any given value of the variable on the x-axis, but it is useful in computing the percentage of values that are within a range along the x-axis.\nThe area under the curve in a density plot could be interpreted as the probability of a single observation or a range of observations.\nWe can use random normal data to create the density plot like shown below with a sample of 1000, a mean of 10 and a standard deviation of 2. To do this, we need to make the vector and assign it to a data frame.\nIn R, set.seed() is a function used to set the seed for random number generation. By setting a seed using set.seed(), you ensure reproducibility of your code. If you run the same code with the same seed, you’ll get the same sequence of random numbers every time. This is particularly useful for debugging, testing, or when you want to ensure that your results are reproducible.\nWe use set.seed before any function with a random normal generator to ensure reproducibility.\nIf a dataset is provided, then you do not need to generate your own random data as shown in the step below.\n\n\n\nset.seed(1)\nx &lt;- rnorm(1000, mean = 10, sd = 2)\ndf &lt;- data.frame(x)\n\n\nNext, we can make the density plot using the ggplot2 package under tidyverse.\n\nLayer 2 includes the geom_density() command in addition to the standard Layer 1 ggplot() command to create the density plot.\n\n\n\nggplot(df, aes(x)) + geom_density()\n\n\n\n\n\n\n\n\n\nThere are a lot of arguments you can change. I selected a couple below. Be sure to look at the help file on the geom_density() layer to get the variety on what you can do.\n\ncolor = sets a line color\nlwd = makes the line thicker. Increase this number for thicker line.\nfill= colors the area under the curve.\nalpha= sets the transparency to the area under the curve.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", fill = \"lightblue\",\n    alpha = 0.5)\n\n\n\n\n\n\n\n\n\nWe can even add a mean line, which we know in this case is 10 because we used random normal data with that mean set as a parameter.\ngeom_vline() is a function used to add vertical lines to a plot created with ggplot. This function is useful for visually indicating specific points or ranges on the x-axis.\nYou can do a line break in your R code after a comma (\\(,\\)) or after a plus sign (\\(+\\)). I find things easier to read on less lines, but it is personal preference how many lines you use given still following the rules in R.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", fill = \"lightblue\",\n    alpha = 0.5) + geom_vline(aes(xintercept = mean(x)), color = \"red\",\n    linetype = \"dashed\", lwd = 1)\n\n\n\n\n\n\n\n\n\nYou do not need the rnorm function if you are provided a dataset with a numerical variable. The following code uses the customers dataset to do 2 examples of density plots with 2 numerical variables.\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\n\nstr(customers)\n\n'data.frame':   200 obs. of  10 variables:\n $ CustID   : int  1530016 1531136 1532160 1532307 1532356 1532387 1533017 1533561 1533697 1533766 ...\n $ Sex      : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ Race     : chr  \"Black\" \"White\" \"Black\" \"White\" ...\n $ BirthDate: chr  \"12/16/1986\" \"5/9/1993\" \"5/22/1966\" \"9/16/1964\" ...\n $ College  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ HHSize   : int  5 5 2 4 5 2 3 5 3 2 ...\n $ Income   : int  53000 94000 64000 60000 47000 67000 84000 76000 42000 71000 ...\n $ Spending : int  241 843 719 582 845 452 153 1079 247 708 ...\n $ Orders   : int  3 12 9 13 7 9 2 23 3 4 ...\n $ Channel  : chr  \"SM\" \"TV\" \"TV\" \"SM\" ...\n\nggplot(customers, aes(Income)) + geom_density()\n\n\n\n\n\n\n\nggplot(customers, aes(Orders)) + geom_density(color = \"#745033\", fill = \"#740000\",\n    alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\nA boxplot is a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups.\nBoxplots are often used to compare the distribution of a continuous variable across several groups.\nA box plot allows you to:\n\nGraphically display the distribution of a data set.\nCompare two or more distributions.\nIdentify outliers in a data set.\n\n\n\n\n\nA Boxplot with Outliers on Left\n\n\n\nBoxplots include the following information:\n\nA line representing the median value.\nA box containing the middle 50% of values.\nWhiskers extending to 1.5 times the IQR.\nOutliers more than 1.5 times the IQR away from the median.\n\n\n\n\n\nA Boxplot with No Outliers\n\n\n\nThis boxplot above displays 5 summary values:\n\nS = smallest value.\nL = largest value.\nQ1 = first quantile = 25th percentile.\nQ2 = median = second quantile = 50th percentile.\nQ3 = third quantile = 75th percentile.\n\nFor example, use the GrowthFund Vector from the last lesson. It is executed again below.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\nGrowthFund &lt;- as.data.frame(GrowthFund)\n\n\nThe quantile() function returns the five point summary when no arguments are specified where the 25th percentile is Quarter 1, and the 75 percentile is Quarter 3. The 50th percentile is the median.\n\n\nQuanData &lt;- quantile(GrowthFund$GrowthFund)\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\n\n\n\nDetecting Outliers\n\nWe see an outlier visually, but without the tool available, we can detect them through the statistics. First we calculate the IQR, which is just quarter 3 minus quarter 1.\n\n\nsummary(GrowthFund$GrowthFund)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-38.320   3.875  13.015  10.088  16.942  36.290 \n\nIQRvalue &lt;- 16.9425 - 3.875\nIQRvalue\n\n[1] 13.0675\n\nIQRvalue &lt;- IQR(GrowthFund$GrowthFund)\nIQRvalue\n\n[1] 13.0675\n\n\n\nThen, multiply the IQR by 1.5.\n\n\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 19.60125\n\n\n\nFinally conduct 2 checks to determine if outliers are past the low whisker and/or high whisker.\n\nA TRUE value indicates that at least one outlier is present at the small end of the distribution.\nA FALSE value indicates that no outliers are at the high end of the distribution.\n\n\n\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n 25% \nTRUE \n\n# True indicating an outlier to the left\n3.875 - -38.32  #42.195\n\n[1] 42.195\n\n42.195 &gt; 19.60125  #TRUE\n\n[1] TRUE\n\nQuanData[5] - QuanData[4] &gt; OutlierValue\n\n 100% \nFALSE \n\n# False indicating no outlier to the right.\n36.29 - 16.9425  #19.3475\n\n[1] 19.3475\n\n19.3475 &gt; 19.60125  #FALSE \n\n[1] FALSE\n\n\n\nYou can also more formally test by using the following formulas.\n\n\\(Lower bound=Q1−1.5×IQR\\)\n\\(Upper bound=Q3+1.5×IQR\\)\nA data point \\(x\\) is an outlier if \\(x\\) less than lower bound or \\(x\\) is greater than the upper bound. Confirming what we found above, we determine one outlier is present to the left.\n\n\n\nLowerBound &lt;- 3.875 - 1.5 * IQRvalue\nLowerBound\n\n[1] -15.72625\n\nQ1 &lt;- QuanData[2]\nQ1  #3.875\n\n  25% \n3.875 \n\nLowerBound &lt;- Q1 - OutlierValue\nLowerBound  #-15.72625\n\n      25% \n-15.72625 \n\nUpperBound &lt;- 16.9425 + 1.5 * IQRvalue\nUpperBound\n\n[1] 36.54375\n\nQ3 &lt;- QuanData[4]\nQ3  #16.9425\n\n    75% \n16.9425 \n\nUpperBound &lt;- Q3 + OutlierValue\nUpperBound  #36.54375\n\n     75% \n36.54375 \n\n## Insert Lower bound and Upper bound in vector to determine if\n## outliers are present: (-38.32, LowerBound 1.71, 3.17, 5.99, 12.56,\n## 13.47, 16.89, 16.96, 32.16, 36.29, UpperBound)\n\n## one outlier to the left, -38.32.\n\n\nGrowthFund Boxplot\n\nWe can use ggplot to retrieve our graph and associated numbers.\nThe outlier is visually depicted on the graph as -38.32.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe can add a little color to the plot with the fill parameter, but then we also need to be sure to turn off the legends in the geom_boxplot.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot(fill = \"red\")\n\n\n\n\n\n\n\n\n\nYou can add parameters to make this visualization more professional, but this gets you started. Be sure to look at some examples in the R community or on ChatGPT.\n\n\n\n\nViolin Plots\n\nA visual display of data that combines features of density plots and boxplots to show the distribution of numeric variables, often across groups.\n\n\nGrowthFund Example\n\nWe can look at the GrowthFund example we had as a boxplot above as a violin plot. In order to make the change, we alter the second layer from geom_boxplot() to geom_violin().\n\n\nGrowthFund %&gt;% ggplot(aes(x=\"\", y=GrowthFund))+ \n  geom_violin() + theme_minimal() + coord_flip()\n\n\n\n\n\n\n\n\n\n\n2nd Example Checking for outliers with mtcars dataset mpg variable\n\nWe can also view mpg from the mtcars data set as a violin plot because it is a numerical variable. In the plot below, I graphed mpg as a violin plot. You can also embed other visual markers like mean and median or layer on another graph like a boxplot.\nYou can see that this violin plot is vertical. The coord_flip() command we used above flips the chart horizontal the same way it flips a boxplot.\n\n\nmtcars %&gt;%\n  ggplot(aes(x=\"\", y = mpg)) +\n  geom_violin(fill=\"lightgreen\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nWe could also merge both a violin and a boxplot. The code below shows two separate charts (and also flipped) using the following code and then merges them into one visualization.\nThe width parameter controls the width of the boxes in a boxplot or the width of the violins in a violin plot. You can vary this parameter to create better depth.\n\n\ndata(mtcars)\n\nmtcars %&gt;%\n    ggplot(aes(x = \"\", y = mpg)) + geom_boxplot(color = \"#986D00\", fill = \"#24585E\") +\n    theme_minimal() + coord_flip()\n\n\n\n\n\n\n\n\n\nLet’s get some summary statistics to check for outliers, skewness, and kurtosis to see how our visual aids help us in understanding those results.\n\n\nIQRvalue &lt;- IQR(mtcars$mpg)\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue  #11.0625\n\n[1] 11.0625\n\nQuanData &lt;- quantile(mtcars$mpg)\nQuanData\n\n    0%    25%    50%    75%   100% \n10.400 15.425 19.200 22.800 33.900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n  25% \nFALSE \n\n# Using the numbers from QuadData\n15.425 - 10.4 &gt; 11.0625\n\n[1] FALSE\n\n# False indicating no outlier to the left\n\n\nQuanData[5] - QuanData[4] &gt; OutlierValue\n\n100% \nTRUE \n\n33.9 - 22.8 &gt; 11.0625\n\n[1] TRUE\n\n# TRUE indicating an outlier to the right.\n\n\nWe can see more specific information on outliers if we calculate the lower bound and upper bound and insert the values into the vector.\n\n\nQ1 &lt;- QuanData[2]\nQ1\n\n   25% \n15.425 \n\nLowerBound &lt;- Q1 - OutlierValue\nLowerBound  #4.3625 \n\n   25% \n4.3625 \n\nQ3 &lt;- QuanData[4]\nQ3\n\n 75% \n22.8 \n\nUpperBound &lt;- Q3 + OutlierValue\nUpperBound  #33.8625 \n\n    75% \n33.8625 \n\nsort(mtcars$mpg)\n\n [1] 10.4 10.4 13.3 14.3 14.7 15.0 15.2 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7\n[16] 19.2 19.2 19.7 21.0 21.0 21.4 21.4 21.5 22.8 22.8 24.4 26.0 27.3 30.4 30.4\n[31] 32.4 33.9\n\n\n[LowerBound: 4.3625] 10.4 10.4 13.3 14.3 14.7 15.0 15.2 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.2 19.7 21.0 21.0 21.4 21.4 21.5 22.8 22.8 24.4 26.0 27.3 30.4 30.4 32.4 [UpperBound: 33.8625)] 33.9\n\nsemTools::skew(mtcars$mpg)  #normal\n\nskew (g1)        se         z         p \n    0.672     0.433     1.553     0.120 \n\nsemTools::kurtosis(mtcars$mpg)  #mesokurtic\n\nExcess Kur (g2)              se               z               p \n         -0.022           0.866          -0.025           0.980 \n\n\n\nLooks like the mpg variable is quite normal with one potential outlier to the right, but no major signs of skewness or kurtosis.\nWe can also get a histogram of mpg and are able to make the same claims towards normality. You can see a slight pull to the right, but it is seemingly normal. A higher sample size could help here.\n\n\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 5, color = \"black\",\n    fill = \"green\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-two-variables-at-once",
    "href": "dataviz.html#graphs-for-two-variables-at-once",
    "title": "Data Visualization",
    "section": "Graphs for Two Variables At Once",
    "text": "Graphs for Two Variables At Once\n\nCombinations of 2 Variable Types for Graphing\n\nTwo categorical/ factor.\nOne categorical/ factor and one continuous/ numeric.\nTwo continuous/ numeric.\n\n\n\nBar Graphs for Two Categorical Variables\n\nThere are two formats available for bar charts:\n\nGrouped\nStacked\n\n\n\n\n# A tibble: 6 × 3\n# Groups:   vs, gear [6]\n     vs  gear     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     0     3    12\n2     0     4     2\n3     0     5     4\n4     1     3     3\n5     1     4    10\n6     1     5     1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrouped Bar Graph\n\nGrouped bar graph allow comparison of multiple sets of data items, with a single color used to denote a specific series across all sets.\nFor example, we can look at both the vs and gear variables in the ggplot command.\n\nYou can do a little grouping and counting before you began to generate a new table with frequencies based on vs and gear variables. Once a new dataset object is made, you can make the graph with the geom_bar layer specifying the stat=“identity”.\nSince there are two variables, you can set the position to dodge to view the fill categorical variable side by side.\n(stat = “identity”) tells ggplot that the y values are already computed and should be used as-is for the heights of the bars. In this case, they are frequencies calculated in the countsDF dataset.\n\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(vs = as.factor(vs)) %&gt;%\n    mutate(gear = as.factor(gear))\n\n\ncountsDF &lt;- mtcars %&gt;%\n    group_by(vs, gear) %&gt;%\n    count()\n\nsummary(countsDF)\n\n vs    gear        n         \n 0:3   3:2   Min.   : 1.000  \n 1:3   4:2   1st Qu.: 2.250  \n       5:2   Median : 3.500  \n             Mean   : 5.333  \n             3rd Qu.: 8.500  \n             Max.   :12.000  \n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\") + labs(title = \"Grouped Car Distribution by Gears and VS\",\n    x = \"Number of Gears\", y = \"Count\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\nStacked Bar Graph\n\nA Stacked bar graph extends the standard bar graph from looking at numeric values across one categorical variable to two. Each bar in a standard bar graph is divided into a number of sub-bars stacked end to end, each one corresponding to a level of the second categorical variable.\nUsing ggplot, we can also stack these charts by removing the position = dodge statement.\n\n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Stacked Car Distribution\",\n       x = \"Number of Gears\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBar Graph for Continuous Across Groups\n\nIn comparison to a bar graph for a single categorical variable, a bar chart for a continuous variable across groups includes both a x and y axis. The continuous variable is put on the y axis, and the categorical (factor) variable is placed on the x axis showing the groups.\nTherefore, instead of counting data based on group, we can see another continuous variable based on group data.\nThe frequency data (i.e., counts) can be replaced with another numerical variable like mean.\nIn the below example, instead of counting observations per group, here, we took the average mpg (a continuous variable) based on groups of gear and vs and summarized the data into a variable avg_mpg. We then used that variable in a ggplot() command to create a unique chart to that above.\n\n\navg_mpg &lt;- mtcars %&gt;%\n    group_by(gear, vs) %&gt;%\n    summarise(mpg = mean(mpg, na.rm = TRUE))\n\n\nggplot(avg_mpg, aes(gear,\n  mpg, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  ggtitle(\"Average MPG by VS and Gear\")\n\n\n\n\n\n\n\n\n\nBelow, we can add color using the scale_fill_manual. Since there are two colors, we use the c() command to combine them together inside the layer.\n\n\nggplot(avg_mpg, aes(gear, mpg, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\", color = \"black\") + ggtitle(\"Average MPG by VS and Gear\") +\n    scale_fill_manual(values = c(\"yellow\", \"brown\"))\n\n\n\n\n\n\n\n\n\n\n\nBoxplot for Continuous Across Groups\n\nA boxplot requires one continuous variable (like we did above). When we include an additional grouping variable, we get multiple boxplots, one for each group. This allows us to directly compare distributions.\nThe categorical variable should correctly be a factor data type before you begin.\nIn the example below, mpg is the continuous variable, and gear is the categorical variable.\nWe see 3 boxplots for three values of gear (3, 4, 5). ggplot() and geom_boxplot() are required components of the command. The scale_fill_manual() and theme_minimal() layers are optional ways to change the style and color.\n\n\nmtcars %&gt;%\n  ggplot(aes(x = gear, y = mpg, fill = gear)) +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_manual(values = c(\"gray\", \"red\", \"blue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLets alter the functions above to depict mpg based on vs with categorical states 0 and 1.\n\n\nmtcars %&gt;%\n    ggplot(aes(x = vs, y = mpg, fill = vs)) + geom_boxplot(show.legend = FALSE) +\n    scale_fill_manual(values = c(\"gray\", \"red\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\n\nScatterplot for Two Continuous Variables\n\nScatterplots\n\nA scatterplot is used to determine if two continuous variables are related.\n\nEach point is a pairing: \\((x_1, y_1),(x_2, y_2),\\) etc.\n\nOur goal with a scatterplot is to characterize the relationship by visual inspection. This includes determining if the relationship looks positive, negative, or not existent.\n\n\n\n\nScatterPlot Results\n\n\n\nSometimes, it is really clear how to characterize the relationship. Other times, additional statistical tests are needed to confirm the relationship (which we will go over in later lessons). This is true especially with big data, where the plot window can look like a giant blog of observations.\nLet’s work a clean example examining the relationship between income and the years of education one has had.\nThis plot has a clear positive trend, meaning that as one has more years of education, we see higher income. And similarly, as we see higher income, we also see more years of education. This means that a scatter can help characterize the relationship, and does not state that one variable is causing another to occur.\n\n\nEdu &lt;- read.csv(\"data/education.csv\")\nplot(Edu$Income ~ Edu$Education, ylab = \"Income\", xlab = \"Education\")\n\n\n\n\n\n\n\n\n\nWorking with ggplot instead of base R, we would use the following code.\n\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs (for labels) as shown below.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\")\n\n\n\n\n\n\n\n\ngeom_point() has some parameters you can change like shape = where you can add depth to your chart. Some common shapes of geom_point are as follows.\n\n\n# shape = 0, square shape = 1, circle shape = 2, triangle point up\n# shape = 3, plus shape = 4, cross shape = 5, diamond shape = 6,\n# triangle point down shape = 7, square cross shape = 8, star shape =\n# 9, diamond plus shape = 10, circle plus shape = 11, triangles up\n# and down shape = 12, square plus shape = 13, circle cross shape =\n# 14, square and triangle down shape = 15, filled square shape = 16,\n# filled circle shape = 17, filled triangle point-up shape = 18,\n# filled diamond\n\n\nFor instance, the code below changes the color, shape, and size of the geom_point().\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 10) + labs(y = \"Income\", x = \"Education\")\n\n\n\n\n\n\n\n\n\ngeom_line() in R’s ggplot2 package is used to create line plots, which connect data points with straight lines. It is commonly used to visualize trends over time or continuous relationships between variables.\nggplot allows us to add a geom_line, which is helpful in drawing a line through the data. Here, I am also resetting the color off of the default value. You see this a lot on time series models like stock charts.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_line(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn R’s ggplot2 package, geom_smooth() is used to add a trendline (also called a smoothing line or a regression line) to a plot. It fits a line or curve through your data points based on a smoothing method, allowing you to visualize the general relationship between the x and y variables. The trendline can help reveal patterns such as linear relationships or trends in noisy data. The trendline give us 2 parts:\n\nTrendline: This line represents the fitted relationship between the two variables, which could be linear, polynomial, or a more flexible curve depending on the smoothing method used.\nConfidence Interval: The shaded area (often gray by default) around the trendline shows the confidence interval, giving a sense of the uncertainty of the fit.\n\nWe can change our line to a geom_smooth line, which is considered a trendline to help us visualize the relationship between the variables.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_smooth(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nWe can change the type of trendline. The most common is the to develop the trendline using the lm method, which stands for the linear method that we are going to learn in the Regression lesson. For now, lets insert _method=“lm” into our geom_smooth() later to see the change.\nThe gray band on a scatter plot in R usually represents the confidence interval around a fitted line when you plot it using geom_smooth() in ggplot2. This gray area visualizes the uncertainty or variability of the estimated regression line, providing a sense of how confident we are about the predictions at different points along the line.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\") + geom_smooth(method = \"lm\", color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nUnlike geom_smooth(), which fits a trendline or smoothing line to data, geom_line() directly connects the raw data points in the order they appear (typically by the x-axis values). In business statistics class that teaches linear regression, we use more trendlines than geom_lines.\nLet’s look at a few more examples and see if the relationship is considered positive, negative, or not existent.\nBelow, we see a negative trend.\n\n\nggplot(mtcars, aes(x = disp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the hp variable, below, we see another negative trend.\n\n\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the qsec variable, below, we see a weak positive trend. This relationship would need to verified later on.\n\n\nggplot(mtcars, aes(x = qsec, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nThe plot() command also works when you do not have 2 continuous variables, and instead have one categorical variable paired with one continuous variable. However, the plot is not as adequate as others are in inferring the relationship from the variables.\nFor example, the plot below would be better served as a boxplot.\n\n\nplot(mtcars$mpg ~ mtcars$cyl)\n\n\n\n\n\n\n\nboxplot(mtcars$mpg ~ mtcars$cyl)\n\n\n\n\n\n\n\n\n\nUsing ggplot, we would have the same issue. Since vs is a categorical variable, it does not look right when we use the geom_point() later.\n\n\nggplot(mtcars, aes(cyl, mpg)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nInstead, we would want the geom_boxplot() layer like shown below. The geom_smooth() is also not an applicable layer to a boxplot and would need to be removed.\n\nWhen you create multiple boxplots in ggplot2 and one or more variables are numeric but not converted to factors, only one boxplot may show up because ggplot2 treats numeric variables as continuous. Boxplots group data by categorical variables, so if your variable is continuous, ggplot interprets it as a single category and produces only one boxplot.\nBelow, we still have an error.\n\n\nggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nSomething still is not quite right here, so we would need to make sure cyl is correctly a factor before making the boxplot. Now, we should see multiple boxplots, one for each categorical level. To fix this, you need to convert the grouping variable to a factor using as.factor() in your data, so ggplot will recognize it as a categorical variable and display multiple boxplots accordingly.\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(cyl = as.factor(cyl))\nggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nTry to Recreate\n\nYou try examples of scatterplots using the UScrime data set that is part of the MASS package to examine a few relationships using ggplot2. A few examples are below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what information you can take away from the scatterplots above and create some more to practice.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#full-example-using-nhanes-dataset",
    "href": "dataviz.html#full-example-using-nhanes-dataset",
    "title": "Data Visualization",
    "section": "Full Example Using nhanes Dataset",
    "text": "Full Example Using nhanes Dataset\n\nA big example of where charts are helpful is to explore the nhanes dataset, which is actually a dataset related to auditory issues, in which the number of guns fired is a variable.\nSpecifically, AUQ060 refers to “Hear a whisper from across a quiet room?” AUQ070 refers to “Hear normal voice across a quiet room?” and AUQ080 refers to “Hear a shout from across a quiet room?” While AUQ300 refers to “Ever used firearms for any reason? AUQ310 refers to”How many total rounds ever fired?” and AUQ320 refers to “Wear hearing protection when shooting?” I got these references at the following website: https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm.\nIt would be interesting to determine whether there is a relationship between hearing loss and gun use. Below, I clean the data set and make some charts. All variables are categorical, and I only look at 2 categorical variables at a time - one auditory related (AUQ 060, 070, 080) and one gun related (AUQ 300, 310, 320).\nThe first step is data cleaning, and in particular recoding the variables of interest. We did some of these before. Let’s also filter out the unused variables using the select statement.\n\nThe select statement in dplyr has a conflict with the select statement in MASS. Since we used MASS earlier, we have to specify which package we want to use select from. We want to use select from dplyr, so we add dplyr:: before the function.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n# summary(nhanes)\n\nnhanes.clean &lt;- nhanes %&gt;%\n    dplyr::select(AUQ300, AUQ310, AUQ320, AUQ060, AUQ070, AUQ080) %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ310 = recode_factor(AUQ310, `1` = \"1 to less than 100\", `2` = \"100 to less than 1000\",\n        `3` = \"1000 to less than 10k\", `4` = \"10k to less than 50k\", `5` = \"50k or more\",\n        `7` = \"Refused\", `9` = \"Don't know\")) %&gt;%\n    mutate(AUQ060 = recode_factor(AUQ060, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ070 = recode_factor(AUQ070, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ080 = recode_factor(AUQ080, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ320 = recode_factor(AUQ320, `1` = \"Always\", `2` = \"Usually\",\n        `3` = \"About half the time\", `4` = \"Seldom\", `5` = \"Never\"))\nsummary(nhanes.clean)\n\n  AUQ300                       AUQ310                     AUQ320    \n Yes :1613   1 to less than 100   : 701   Always             : 583  \n No  :3061   100 to less than 1000: 423   Usually            : 152  \n NA's:4690   1000 to less than 10k: 291   About half the time: 123  \n             10k to less than 50k : 106   Seldom             : 110  \n             50k or more          :  66   Never              : 642  \n             Don't know           :  26   NA's               :7754  \n             NA's                 :7751                             \n  AUQ060      AUQ070      AUQ080    \n Yes :2128   Yes : 564   Yes : 159  \n No  : 745   No  : 210   No  :  53  \n NA's:6491   NA's:8590   NA's:9152  \n                                    \n                                    \n                                    \n                                    \n\n\n\nFrom here, NA’s are an issue, but we don’t want to broadly omit because it would slice down our dataset too much. I am going to leave them in and handle it on a chart by chart basis.\nThe most applicable chart to graph 2 categorical variables is a barchart. This requires calculating frequencies, and then graphing. Using ggplot, the frequencies are calculated automatically.\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Whisper Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Normal Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Shout Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n############# Wear hearing protection when shooting\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Whisper Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Normal Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Shout Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThat is a few of the charts. See if you can determine anything interesting from them and also try to run the other combinations. You will see that it is easier to see with less categories when you have multiple variables like this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#using-ai",
    "href": "dataviz.html#using-ai",
    "title": "Data Visualization",
    "section": "Using AI",
    "text": "Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about data visualization capabilities in ggplot2.\nHow can I modify the appearance of a ggplot bar chart to include custom colors for each bar, and what are the best practices for choosing colors in data visualization?\nWhat is the role of layering in ggplot, and how can adding multiple layers, such as labels, themes, and lines, improve the readability of a plot?\nWhen should a density plot be used instead of a histogram, and how does each visualization help in understanding the distribution of continuous data?\nHow can I use a boxplot in R to identify and visualize outliers in my dataset, and what additional steps should I take to handle these outliers?\nHow can I create a scatter plot in ggplot to explore relationships between two continuous variables, and how do I add a trendline to help interpret the results?\nWhat are the steps for creating a grouped bar chart in ggplot, and how does this visualization help in comparing multiple categories or groups within a dataset?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#summary",
    "href": "dataviz.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\n\nPractice many more examples with the help of ChatGPT and work towards constructing high-quality charts and graphs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Probability and Probability Distributions",
    "section": "",
    "text": "At a Glance",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#some-common-statistics-terms",
    "href": "probability.html#some-common-statistics-terms",
    "title": "Probability and Probability Distributions",
    "section": "Some Common Statistics Terms",
    "text": "Some Common Statistics Terms\n\nStatistical inference is one of the foundational ideas in statistics. Since it is often impossible to collect information on every single person or organization, scientists take samples of people or organizations and examine the observations in the sample. Inferential statistics are then used to take the information from the sample and use it to understand (or infer to) the population. In conducting probability calculations, we can make inferences to understand the probability associated with the population.\nResearchers often work with samples instead of populations, where samples are subsets of different populations. In the case of the state data on opioid policies that your book discusses, all states are included, so this is the entire population of states. Statisticians sample from the population to understand the probabilities associated with it.\nWhen selecting a sample, we hope to select a representative sample from the population, and use properties of the normal distribution to understand what is likely happening in the whole population. A normal distribution is one of the most fundamental distributions to use in calculating probabilities. We will look at both discrete and normal distributions, but also seek to understand how a normal distribution and the central limit theorem can help us shed light on many statistics we are inferring.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-distribution",
    "href": "probability.html#probability-distribution",
    "title": "Probability and Probability Distributions",
    "section": "Probability Distribution",
    "text": "Probability Distribution\n\nA probability distribution is the numeric or visual representation of the set of probabilities that each value or range of values of a variable occurs.\nTwo important characteristics:\n\nThe probability of each real value of some variable is non-negative. Instead, it is either 0 or positive. More specifically, the probability of each value x is a value between 0 and 1. Or equivalently. \\(0 &lt;= P(X=x) &lt;= 1\\).\nThe sum of the probabilities of all possible values of a variable is 1.\n\nConsider the probability distribution that reflects the number of credit cards that a bank’s customers carry.\n\n\n\n  NumberOfCards Percentage\n1             0       2.5%\n2             1       9.8%\n3             2      16.6%\n4             3      16.5%\n5     4 or more       54.6\n\n\n\nGiven the characteristics of a probability distribution, we can ask whether this is a valid probability distribution.\n\nYes, because \\(0 &lt;= P(X=x) &lt;= 1\\) and the sum of the percentages is 1.\n\n\n0.025 + 0.098 + 0.166 + 0.165 + 0.546\n\n[1] 1\n\n\nSecond, with the information in the table, we can calculate a number of things, like the probability that a reader carries no credit cards.\n\n\\(P(X=0)= .025\\)\n\nThe probability that a reader carries fewer than two credit cards.\n\n\\(P(X&lt;2)= P(X=0)+P(X=1)= .025 +.098= .123\\)\n\nThe probability that a reader carries at least two credit cards.\n\n\\(P(X&gt;=2) = P(X=2)+P(X=3)+P(X=4)  = .166+.165+.546= .877\\)\nOr \\(1-P(X&lt;2) = 1-.123 = .877\\)\n\nBecause of the 2 characteristics of a probability distribution, sometimes there are a couple ways to calculate the correct answer, like we did above with either calculating probabilities associated with above a value, or beneath and equal to a value knowing that the total sum of all probabilities is 1.\nWhen you produce a percentage, you multiple the calculated probability by 100, so instead of finding a value between 0 and 1, you find a percentage between 0% and 100%. This means that if you use the pnorm function to calculate a probability, you can multiple that probability by 100 to get the percentage.\nWhen you produce the value given a probability calculation, you multiply the probability calculated by the sample size (n).\n\n\nRandom Variables\n\nA Random Variable is a function that assigns numerical values to the outcomes of a random experiment.\nDenoted by uppercase letters (e.g., \\(X\\) ).\nCorresponding values of the random variable: \\(x_1,x_2, x_3,...\\)\nRandom variables may be classified as:\n\nDiscrete - The random variable assumes a countable number of distinct values.\n\n\nDiscrete probability distributions show probabilities for variables that can only have certain values, which includes categorical variables and variables that must be measured in whole numbers like number of people texting during class.\nThe Binomial Distribution is a discrete distribution that evaluates the probability of a “yes” or “no” outcome occurring over a given number of trials\n\n\nContinuous - The random variable is characterized by (infinitely) uncountable values within any interval.\n\n\nContinuous probability distributions show probabilities for values, or ranges of values, of a continuous variable that can take any value in some range.\nThe Normal Distribution is a continuous distribution and is the most important of all probability distributions. Its graph is bell-shaped and this bell-shaped curve is used in almost all disciplines.\n\nFor example, consider an experiment in which two shirts are selected from the production line and each is either defective (D) or non-defective (N).\n\nSince only 2 shirts are selected, here is the sample space, which are all the available options: \\({(D,D), (D,N), (N,D), (N,N)}\\)\nThe random variable X is the number of defective shirts.\nThe possible number of defective shirts is the set \\(X={0,1,2}\\),\nSince these are the only countable number of possible outcomes, this is a discrete random variable.\n\n\n\n\nUseful Commands for Random Variables\n\nset.seed() command is useful when conducting random sampling since it will result in the same sample to be taken each time the code is run, which makes sampling more reproducible.\n\nWe briefly looked at this when making our density plot with random normal data using the rnorm() command.\n\nsample_n() command can be used to take a sample. The arguments for sample_n() are size = which is where to put the size of the sample to take and replace = which is where you choose whether or not you want R to sample with replacement (replacing each value into the population after selection, so that it could be selected again) or without replacement (leaving a value out of the sampling after selection).\nLet’s look at an example using the pdmp_2017.csv file.\n\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\nBelow, I am using the read.csv() command to read in the data set and use strings as factors = TRUE to ensure character variables are coerced as factors. This helps bypass the need to coerce later on. We do need to note that it will change all string variables to factors with the TRUE parameter.\n\n\nopioidpolicy  &lt;- read.csv(\"data/pdmp_2017.csv\", stringsAsFactors = TRUE) \n# Set a starting value for sampling\nset.seed(3)\n# Sample 25 states and save as sample and check summary\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nYou should have the same answers as I do above (8 No’s, and 17 Yes’s) because we set the same seed. If you don’t, I mention the reason below at the end of this short sampling experiment.\n\n\n# Sample another 25 states and check summary.\n# Note the different answer than above. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 14  11 \n\n\n\n# Sample another 25 states and check summary\n# Again, note the differences in numbers each time. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 12  13 \n\n\n\n# Sample another 25 states and check summary using same set seed as our first run (3). \nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nAgain, you should have the same numbers as I do above, and these numbers should be equivalent to our first run (8 No’s, and 17 Yes’s). If you don’t have the same numbers as me is possible that your random number generator is on a different setting. Post R version 3.6.0 or later, we should be on Rejection sample.kind. The next line sets the RNGkind().\n\n\nRNGkind(sample.kind = \"Rejection\")\n# Run the same code again as above for replication results\nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\n\nSummary Measures for a Random Variable\n\nExpected Value\n\nWe can calculate the expected value, or value we think is going to occur based on the type of distribution.\n\nExpected value is also known as the population mean \\(\\mu\\), and is the weighted average of all possible values of \\(X\\).\nMore specifically, \\(E(X)\\) is the long-run average value of the random variable over infinitely many independent repetitions of an experiment.\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the expected value of \\(X\\) is the probability weighted average of the values. In the case of one random variable, that means: \\(E(X) = \\mu = \\sum{x_iP(X=x_i)}\\)\n\n\n\n\nVariance\n\nVariance of a random variable is the average of the squared differences from the mean.\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the variance is defined as: \\(Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2*P(X=x_i)}\\)\n\n\n\n\nStandard Deviation\n\nStandard deviation is consistently the square root of the variance. \\(SD(X) = \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\)\n\n\n\n\nExample of Summary Measures for a Random Variable\n\ndefectdata &lt;- read.csv(\"data/defects.csv\")\nhead(defectdata)\n\n  SerialNumber NumDefects\n1            1          6\n2            2          6\n3            3          0\n4            4          1\n5            5          6\n6            6          6\n\nstr(defectdata)\n\n'data.frame':   500 obs. of  2 variables:\n $ SerialNumber: int  1 2 3 4 5 6 7 8 9 10 ...\n $ NumDefects  : int  6 6 0 1 6 6 7 0 3 2 ...\n\n# Calculate the probability of the defective pixels per monitor for\n# each member of the sample space [0, 1, 2, 3, 4, 5, 6, 7].\nsampleSpace &lt;- 0:7\nfrequency &lt;- table(defectdata$NumDefects)\nfrequency\n\n\n 0  1  2  3  4  5  6  7 \n49 47 62 83 60 65 66 68 \n\nproportions &lt;- prop.table(frequency)\nproportions\n\n\n    0     1     2     3     4     5     6     7 \n0.098 0.094 0.124 0.166 0.120 0.130 0.132 0.136 \n\ncumulativeproportions &lt;- cumsum(prop.table(proportions))\ncumulativeproportions\n\n    0     1     2     3     4     5     6     7 \n0.098 0.192 0.316 0.482 0.602 0.732 0.864 1.000 \n\n\n\nOnce we calculate vectors for the frequency table, we bind them together and transpose them into columns and combine them into a data frame.\n\n\nDefects &lt;- t(rbind(sampleSpace, frequency, proportions, cumulativeproportions))\nDefects &lt;- as.data.frame(Defects)\nstr(Defects)\n\n'data.frame':   8 obs. of  4 variables:\n $ sampleSpace          : num  0 1 2 3 4 5 6 7\n $ frequency            : num  49 47 62 83 60 65 66 68\n $ proportions          : num  0.098 0.094 0.124 0.166 0.12 0.13 0.132 0.136\n $ cumulativeproportions: num  0.098 0.192 0.316 0.482 0.602 0.732 0.864 1\n\n\n\nNext, we calculate summary statistics based on the formulas above.\n\n\n# How many defects should the manufacturer expect per monitor? E(X)?\nExDefects &lt;- sum(Defects$sampleSpace * Defects$proportions)\nExDefects  #3.714\n\n[1] 3.714\n\n# Variance of the number of defects per monitor?\ndeviations &lt;- (Defects$sampleSpace - ExDefects)^2 * Defects$proportions\ndeviations\n\n[1] 1.35179201 0.69238482 0.36428670 0.08462614 0.00981552 0.21499348 0.68980507\n[8] 1.46850026\n\nvarDefects &lt;- sum(deviations)\nvarDefects  #4.876204\n\n[1] 4.876204\n\n# Standard deviation of the number of defects per monitor?\nstDefects &lt;- sqrt(varDefects)\nstDefects  #2.208213\n\n[1] 2.208213",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-binomial-distribution",
    "href": "probability.html#the-binomial-distribution",
    "title": "Probability and Probability Distributions",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\nThe binomial distribution is a discrete probability distribution and applies to probability for binary categorical variables with specific characteristics.\nProperties of a binomial random variable:\n\nA variable is measured in the same way \\(n\\) times, signifying that \\(n\\) is the sample size.\nThere are only two possible values of the variable, often called “success” and “failure”\nEach observed value is independent of the others.\nThe probability of “success”, \\(p\\), and the probability of “failure”, \\(1-p\\), is the same for each observation, so each time the trial is repeated, the probabilities of success and failure remain the same.\nThe random variable is the number of successes in \\(n\\) measurements.\n\n\n\nSummary Measures for a Binomial Random Variable\n\nExpected value, variance, and standard deviation were introduced and defined in section 2. We can take derivatives of the formulas to simplify our calculations given knowing a \\(n\\) and \\(p\\).\nThe formula for the expected value of a binomial random variable expands from \\(\\sum{x_iP(X=x_i)}\\) to \\(= n*p\\).\nThe variance of a binomial random variable expands from \\(\\sum{(x_i-\\mu)^2*P(X=x_i)}\\) to \\(= n*p*(1-p)\\)\nThe standard deviation a binomial random variable expands from \\(\\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\) to \\(= \\sqrt{np*(1-p)}\\)\nExample summary statistics of a binomial random variable\n\nA current WM student has a career free-throw percentage of 89.4%. Suppose he shoots six free throws in tonight’s game. What is the expected number of free throws that he will make?\n\n\nex &lt;- 6 * 0.894\nex\n\n[1] 5.364\n\nvarx &lt;- 6 * 0.894 * (1 - 0.894)\nvarx\n\n[1] 0.568584\n\nsdx &lt;- sqrt(varx)\nsdx\n\n[1] 0.7540451\n\n\nIf the student shoots 6 free throws and typically makes 89.4% of them, we can multiply those two values together for the expected value, 5.364. We also find that this data has a variance of .568584 and standard deviation of .754051.\n\n\n\nThe Probability Mass Function\n\nThe Probability Mass Function for a discrete random variable X is a list of the values of X with the associated probabilities, that is, the list of all possible pairs: \\((X, P(X=x))\\)\n\nA probability mass function computes the probability that an exact number of successes happens for a discrete random variable, given \\(n\\) and \\(p\\) defined above.\nDistribution of probabilities of different numbers of successes.\nA probability mass function is used to describe discrete random variables in a binomial distribution.\nEvery random variable is associated with a probability distribution that describes the variable completely.\nUses dbinom() command to calculate in R.\n\nExample using the probability mass function: Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that none of the workers is afraid that they will never be able to retire?\nAgain, we can use the dbinom() command to calculate this in R given x = none or 0, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n\n# P(X=0)\ndbinom(0, 10, 0.2)\n\n[1] 0.1073742\n\n\n\nThe answer suggests that there is a .107 or 10.737% chance that no workers think they won’t be able to retire.\n\n\n\nCumulative Distribution Function\n\nAnother way to look at a probability distribution is to examine its cumulative probability distribution. Here, you can determine the probability of getting some range of values, which is often more useful than finding the probability of one specific number of successes.\nA cumulative distribution function may be used to describe either discrete or continuous random variables.\n\nThe cumulative distribution function for X is defined as: \\(P(X&lt;=x)\\)\nThe less than and equal to sign is the standard way to look at the cumulative distribution function. You can calculate &gt;, &gt;=, &lt; from \\(P(X&lt;=x)\\) given the two rules of probability discussed above.\n\n\\(0 &lt;= P(X=x) &lt;= 1\\)\nThe sum of the probabilities of all possible values of a variable is 1\n\nUses pbinom() command to calculate in R with the default value of lower.tail = TRUE is for n or fewer successes\nCan change lower.tail parameter to lower.tail = FALSE is computing higher than n rather than n or higher.\n\nExample using the cumulative distribution function, Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that less than 3 of the workers are afraid that they will never be able to retire?\nWe can use the pbinom() command to calculate this in R given q = less than 3 or &lt;=2, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n# P(X&lt;3) or P(X&lt;=2)\npbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\nOr likewise, we could use multiple dbinom() commands to get individual probabilities and add them up. This statement is much longer, but does give you the same answer. Examine the figure below to see why.\n\n\n\ndbinom(0, 10, 0.2) + dbinom(1, 10, 0.2) + dbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\n\n\nVisual of Binomial Distribution Example\n\n\n\n\nVariations in binom() Command\n\nIn order to find \\(P(X = 70)\\) given 100 trials and .68 probability of success, we enter:\n\n\n# P(X = 70)\ndbinom(70, 100, 0.68)\n\n[1] 0.07907911\n\n\n\nIn order to find \\(P(X &lt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt;= 70)\npbinom(70, 100, 0.68)\n\n[1] 0.7006736\n\n\n\nIn order to find \\(P(X &lt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt; 70)\npbinom(69, 100, 0.68)\n\n[1] 0.6215945\n\n\n\nIn order to find \\(P(X &gt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt; 70)\npbinom(70, 100, 0.68, lower.tail = FALSE)\n\n[1] 0.2993264\n\n\n\nIn order to find \\(P(X &gt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt;= 70)\npbinom(69, 100, 0.68, lower.tail = FALSE)  #Or\n\n[1] 0.3784055\n\n1 - pbinom(69, 100, 0.68)\n\n[1] 0.3784055\n\n\n\nExamples of binomial distribution calculations in a word problems: A current WM student has a career free-throw percentage of 90.3%. Suppose he shoots five free throws in tonight’s game. What is the probability that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903)\n\n[1] 0.6003973\n\n\n\nWhat is the percentage that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903) * 100  #60.04%\n\n[1] 60.03973\n\n\n\nA current WM student has a career free-throw percentage of 80.5%. Suppose he shoots six free throws in tonight’s game. What is the probability that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE)\n\n[1] 0.6676464\n\n# Or\ndbinom(5, 6, 0.805) + dbinom(6, 6, 0.805)\n\n[1] 0.6676464\n\n\n\nWhat is the percentage that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE) * 100  # 66.76%\n\n[1] 66.76464\n\n\n\nThirty five percent of consumers with credit cards carry balances from month to month. Six consumers with credit cards are randomly selected. What is the probability that fewer than two consumers carry a credit card balance?\n\n\n# P(X&lt;=2)\ndbinom(0, 6, 0.35) + dbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n# Or\npbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n\n\nWhat is the percentage that fewer than two consumers carry a credit card balance?\n\n\npbinom(1, 6, 0.35) * 100  #31.9%\n\n[1] 31.90799\n\n\n\n\nFollow Up Binomial Example\nFor a discrete binomial distribution with a sample size of 4 (i.e., the number of trials is 4), calculate the probability of each possible outcome (ranging from 0 to 4 successful outcomes) using a probability of success \\(p=0.3\\).\n\n\n\n\n\n\n\n\n\n\n## dbinom (X=x) given size at 4 and p=.3 P(X=0) = 0.2401\ndbinom(0, 4, 0.3)\n\n[1] 0.2401\n\n# P(X=1) = 0.4116\ndbinom(1, 4, 0.3)\n\n[1] 0.4116\n\n# P(X=2) = 0.2646\ndbinom(2, 4, 0.3)\n\n[1] 0.2646\n\n# P(X=3) = 0.0756\ndbinom(3, 4, 0.3)\n\n[1] 0.0756\n\n# P(X=4) = 0.0081\ndbinom(4, 4, 0.3)\n\n[1] 0.0081\n\n\n\nCalculate the probability when the value is less than or equal to 2. All calculations below give same answer.\nTo confirm accuracy, sum the individual probabilities you calculated using dbinom() above. This total should match the value obtained using the pbinom() function in R, which gives the cumulative probability up to a specified number of successes.\nLess than or equal to is the default state of the function in which we use the exact number given.\n\n\n### P(X&lt;=2) =0.9163\npbinom(2, 4, 0.3)\n\n[1] 0.9163\n\ndbinom(0, 4, 0.3) + dbinom(1, 4, 0.3) + dbinom(2, 4, 0.3)\n\n[1] 0.9163\n\n0.2401 + 0.4116 + 0.2646\n\n[1] 0.9163\n\n\n\nCalculate the probability when the value is less than 2. All calculations below give same answer.\nTo calculate less than instead of less than or equal to, we go down 1 unit (or 1 integer) in our pbinom() function, i.e., using 1 instead of 2.\n\n\n## P(X&lt;2) = 0.6517\npbinom(1, 4, 0.3)\n\n[1] 0.6517\n\ndbinom(0, 4, 0.3) + dbinom(1, 4, 0.3)\n\n[1] 0.6517\n\n0.2401 + 0.4116\n\n[1] 0.6517\n\n\n\nCalculate the probability when the value is greater than 2. All calculations below give same answer.\nWe use 2 inside the function because greater than is the opposite tail to less than or equal to. This means that the numbers should be the same as the numbers in a less than or equal to problem, but we need to include the lower.tail parameter at False.\n\n\n## P(X&gt;2) = 0.0837\n1 - pbinom(2, 4, 0.3)\n\n[1] 0.0837\n\npbinom(2, 4, 0.3, lower.tail = F)\n\n[1] 0.0837\n\ndbinom(3, 4, 0.3) + dbinom(4, 4, 0.3)\n\n[1] 0.0837\n\n0.0756 + 0.0081\n\n[1] 0.0837\n\n\n\nCalculate the probability when the value is greater than or equal to 2.\nWe use 1 inside the function because greater than or equal to is the opposite tail to less than. This means that the numbers should be the same as the numbers in a less than problem, but we need to include the lower.tail parameter at False.\n\n\n## P(X&gt;=2) = 0.3483\n1 - pbinom(1, 4, 0.3)\n\n[1] 0.3483\n\npbinom(1, 4, 0.3, lower.tail = FALSE)\n\n[1] 0.3483\n\ndbinom(2, 4, 0.3) + dbinom(3, 4, 0.3) + dbinom(4, 4, 0.3)\n\n[1] 0.3483\n\n0.2646 + 0.0756 + 0.0081\n\n[1] 0.3483",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-continuous-distribution",
    "href": "probability.html#the-continuous-distribution",
    "title": "Probability and Probability Distributions",
    "section": "The Continuous Distribution",
    "text": "The Continuous Distribution\n\nProperties of a Continuous Random Variable\n\nThe random variable is characterized by (infinitely) uncountable values within any interval.\nBecause of the definition infinite uncountable, When computing probabilities for a continuous random variable, \\(P(X=x)=0\\)\nTherefore, we cannot assign a nonzero probability to each infinitely uncountable value and still have the probabilities sum to one.\nThus, \\(P(X=a)\\) and \\(P(X=b)\\) both equal zero, and the following holds for continuous random variables: \\(P(a &lt;= X &lt;= b) = P(a &lt; X &lt; b) = P(a &lt;= X &lt; b) = P(a &lt; X &lt;= b)\\).\n\nThis is important to consider and compare to discrete probability.\n\n\nDensity Functions for Continuous Distributions\n\nProbability Density Function\n\nThe Probability Density Function is used to describe continuous random variables.\nProbability Density Function \\(f(x)\\) of a continuous random variable X describes the relative likelihood that \\(X\\) assumes a value within a given interval (e.g., \\(P(a&lt;=X&lt;=b)\\), where \\(f(x)&gt;=0\\) for all possible values of \\(X\\) and the area under \\(f(x)\\) over all values of \\(x\\) equals one.\n\n\n\nCumulative Density Function\n\nThe Cumulative Density Function \\(F(x)\\) of a continuous random variable X suggests that for any value x of the random variable X, the cumulative distribution function \\(F(x)\\) is computed as, \\(F(x) = P(X &lt;= x)\\) as a result, \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nThe goal of cumulative distributions is to find the area under the curve. The pnorm() function computes the probability at point q to find the area under the curve.\n\nThree arguments of the pnorm() command:\n\n\nq is the value of interest;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.\nWe can also work backwards to find a value given a probability. The qnorm() function computes the quantile value at p to find the value associated with a probability.\n\nThree arguments of the qnorm() command:\n\n\np is the cumulative probability;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.\n\n\n\n\nThe Normal Distribution\n\nThe normal distribution serves as the cornerstone of statistical inference.\nData is symmetric about its mean.\nMean=Median=Mode.\nThe distribution is bell-shaped.\nThe distribution is asymptotic, which means that the tails get closer and closer to the horizontal axis, but never touch it.\nClosely approximates the probability distribution of a wide range of random variables, such as the following:\n\nHeights and weights of newborn babies.\nScores on SAT.\nCumulative debt of college graduates.\n\nThe normal distribution is completely described by two parameters: population mean \\(\\mu\\), which describes the central location of the distribution,and population variance \\(\\sigma^2\\), which describes the dispersion of the distribution.\n\n\n\n\nNormal Distribution\n\n\n\nA special case of the normal distribution:\n\nMean is equal to zero (E(Z) = 0).\nStandard deviation is equal to one (SD(Z) = 1).\n\n\n\n# Solving the figure above using the pnorm() command in R.  P(Z&lt;=0)\npnorm(q = 0, mean = 0, sd = 1)\n\n[1] 0.5\n\n# Or the following because the default value of the mean and sd are 0\n# and 1.\npnorm(0)\n\n[1] 0.5\n\n\n\nIf we assume a mean of 0 and a standard deviation of 1, and we are looking for the probability when the mean is 0, we will get a .5 probability or 50 percent (.5*100). This is because the curve is normal with identical sides.\nWe can also solve this backwards. Below is the qnorm() command, which solves the figure above looking at probability instead of values.\n\n\n# P(Z &gt;= z) = .5\nqnorm(0.5, 0, 1)\n\n[1] 0\n\n# Or\nqnorm(0.5)\n\n[1] 0\n\n\n\n\nEmpirical Rule\n\nChevyshev’s Theorem states that at least \\(1 - 1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.\nWith a normal distribution, we can assume approximately the following under the empirical rule:\n\n68% of values within one SD of the mean;\n95% of values within two SD of the mean;\n99.7% of values within three SD of the mean.\n\n\n\n\n\nEmpirical Rule\n\n\n\n\nCalculating z-scores\n\nA z-score allows description and comparison of where an observation falls compared to the other observations for a normally distributed variable.\nA z-score is calculated as the number of standard deviations an observation is away from the mean.\nA normally distributed variable can be used to create z-scores.\nPurpose: This formula calculates the z-score of an individual data point.\nInterpretation: It tells us how many standard deviations a specific value \\(x\\) is from the mean of the dataset.\nThe \\(x_i\\) represents the value of variable \\(x\\) for a single observation, \\(\\mu_x\\) is the mean of the \\(x\\) variable, \\(\\sigma_x\\) is the standard deviation of the \\(x\\) variable. So, \\(z_i\\) is the difference between the observation value and the mean value for a variable and is converted by the denominator into standard deviations. The final z-score for an observation is the number of standard deviations it is from the mean. \\[z_i = (x_i - \\mu_x)/\\sigma_x\\].\nA z score or z value specifies by how many standard deviations the corresponding x value falls above (z &gt; 0) or below (z &lt; 0) the mean.\n\nA positive z indicates by how many standard deviations the corresponding x lies above mean.\nA zero z indicates that the corresponding x equals mean.\nA negative z indicates by how many standard deviations the corresponding x lies below mean.\n\nExample of a z-score calculation: Scores on a management aptitude exam are normally distributed with a mean of 72 and a standard deviation of 8.\n\nWhat is the probability that a randomly selected manager will score above 60?\nFirst, we could transform the random variable X to Z score using the transformation formula:\n\n\n\n(60 - 72)/8\n\n[1] -1.5\n\n\n\nThen, you can calculate the probability using the standard normal distribution, which has a mean of 0 and a standard deviation of 1.\n\n\n# P(Z &gt; -1.5)\npnorm(-1.5, 0, 1, lower.tail = FALSE)\n\n[1] 0.9331928\n\n# Or similarly, we could use the line below because of the default\n# values associated with the pnorm() command\npnorm(-1.5, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nAlso, because R handles the standard normal transformation on our behalf with its inclusion of parameters, we can use the pnorm() command with the mean and standard deviation provided above to calculate the probability in less steps.\n\n\n# Note the same answer as above.  P(X &gt; 60)\npnorm(60, 72, 8, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nTo answer the question, there is a 0.933 probability or a 93.3% chance that a randomly selected manager will score above a 60 on the managerial aptitude exam.\nIn order to get the percentage in R, we simply multiply the answer by 100.\n\n\npnorm(60, 72, 8, lower.tail = FALSE) * 100\n\n[1] 93.31928\n\n\n\n\nFinding Utility in Calculating Probability\n\nExample using pnorm() with Word Problems: Suppose the life of a particular brand of laptop battery is normally distributed with a mean of 6 hours and a standard deviation of 0.9 hours. Use R to calculate the probability that the battery will last more than 8 hours before running out of power and document that probability below.\n\n\n# P(X &gt; 8)\npnorm(8, 6, 0.9, lower.tail = FALSE)\n\n[1] 0.01313415\n\n\n\nThe time for a professor to grade a student’s homework in business statistics is normally distributed with a mean of 15 minutes and a standard deviation of 3.5 minutes. What is the probability that randomly selected homework will require less than 16 minutes to grade?\n\n\n# P(X &lt; 16)\npnorm(16, 15, 3.5)\n\n[1] 0.6124515\n\n\n\nWhat percentage of randomly selected homeworks will require less than 16 minutes to grade?\n\n\npnorm(16, 15, 3.5) * 100\n\n[1] 61.24515\n\n\n\n\nFinding Probability Between Two Values\n\nWe mentioned above that in order to find probability between 2 values a and b, we could use the following equation: \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nTo use this formula, find P(−1.52 &lt;= Z &lt;= 1.96). This would equal P(Z&lt;=1.96)−P(Z&lt;=−1.52) given a standard normal random variable Z we get the commands below.\n\n\n# P(Z &lt;= 1.96) - P(Z &lt;= -1.52)\npnorm(1.96, 0, 1) - pnorm(-1.52, 0, 1)\n\n[1] 0.9107466\n\n\n\n\nFinding Value Given a Probability\n\nWe use R’s pnorm() and qnorm() commands to solve problems associated with the normal distribution.\nIf we want to find a particular x value for a given cumulative probability (p), then we enter “qnorm(p, μ, σ)”.\n\n\n# P(X &gt; x) = 0.10\nqnorm(0.9, 7.49, 6.41)\n\n[1] 15.70475\n\n# or\nqnorm(0.1, 7.49, 6.41, lower.tail = FALSE)\n\n[1] 15.70475\n\n\n\n\nFinding Utility in Calculating Values from Probability\n\nExample of qnorm() with Word Problems: The stock price of a particular asset has a mean and standard deviation of $58.50 and $8.25, respectively. What is the 95th percentile of this stock price?\n\n\n# P(X&lt;=x) =.95\nqnorm(0.95, 58.5, 8.25)\n\n[1] 72.07004\n\n\n\nThe salary of teachers in a particular school district is normally distributed with a mean of $50,000 and a standard deviation of $2,500. Due to budget limitations, it has been decided that the teachers who are in the top 2.5% of the salaries would not get a raise. What is the salary level that divides the teachers into one group that gets a raise and one that does not?\n\n\n# P(X&gt;=x) =.025\nqnorm(0.025, 50000, 2500, lower.tail = FALSE)\n\n[1] 54899.91\n\n\n\nYou are planning a May camping trip to a National Park in Alaska and want to make sure your sleeping bag is warm enough. The average low temperature in the park for May follows a normal distribution with a mean of 32°F and a standard deviation of 8°F. Above what temperature must the sleeping bag be suited such that the temperature will be too cold only 5% of the time?\n\n\n# P(X&lt;=x) =.05\nqnorm(0.05, 32, 8)\n\n[1] 18.84117",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#central-limit-theorem-clt",
    "href": "probability.html#central-limit-theorem-clt",
    "title": "Probability and Probability Distributions",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\nCLT refers to the fact that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. Therefore, CLT suggests that for any population X with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) the sampling distribution of \\(X\\) will be approximately normal if the sample size \\(n\\) is sufficiently large.\nThe CLT tells us that, regardless of the original distribution of a population, the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger. We use the CLT when we want to estimate population parameters (like the mean) from sample data and apply techniques that rely on normality, such as confidence intervals and hypothesis testing. This allows us to make inferences about the population using the normal distribution even when the population itself isn’t normally distributed.\nThe Central Limit Theorem (CLT) holds true for continuous variables, regardless of whether they are normally distributed or not. Generally, the normal distribution approximation is justified when the sample size is sufficiently large, typically \\(n≥30\\). If the sample means are approximately normal, we can transform them into a standard normal form. The standard deviation of the sample means (also called the standard error) can be estimated using the population standard deviation and the sample size that makes up the distribution.\nIf \\(\\bar{X}\\) is approximately normal, then we can transform it using an updated formula of the z-score formula with standard error \\(𝑍=(𝑋- \\mu)/(\\sigma/\\sqrt(n))\\)\nNext, we want to create an experiment where we simulate why the CLT holds true. The rnorm() command pulls random data from a normal distribution. However, even when the sample is small, it can appear not normal - even though it is from a normal distribution.\n\n\nset.seed(1)\n# Sample of n = 10\nhist(rnorm(10), xlim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nIf we increase the sample size, we should all see a nice normal bell shape distribution like the one below.\n\n\nset.seed(1)\n# Sample of n = 1000\nhist(rnorm(1000), xlim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nThe x limits are set from -3 to 3 because a normal distribution follows the empirical rule discussed above (almost all data is within 3 sd of the mean).\nRelating this to the CLT, the CLT states that the sum or mean of a large number of independent observations from the same underlying distribution has an approximate normal distribution. If we were to take a 6 sided dice, in any given roll, we would expect the average. This means that the expected value E(X) will be the mean as discussed above.\n\n\n# Creating a sample\nd &lt;- 1:6\n# Calculating the expected value E(X) which equals the population\n# mean\nmean(d)  #3.5\n\n[1] 3.5\n\n\n\nRolling a dice only one time would not give us enough data to assume a normal distribution under the CLT. However, if we were to roll a higher number of times, and repeat that experiment \\(n\\) number of times, we would expect that as \\(n\\) grows, we would approach a normal distribution.\n\n\n# First, let's roll the dice 1000 times.  We would expect the average\n# like shown below.\nset.seed(1)\nNumberofRolls &lt;- 1000\nx &lt;- sample(d, NumberofRolls, replace = TRUE)\n# The mean(x) is 3.514 and our mean of 1 through 6 is 3.5.  We\n# estimated about the average.\nmean(x)\n\n[1] 3.514\n\nhist(x)\n\n\n\n\n\n\n\n\n\nNext, if we repeat the dice roll experiment that we ran 1,000 times above, we can see the normal distribution start to take shape. The example below has a loop for simulation purposes. This loop rolls x with allowed values 1 to 6 - 1,000 times - and then does that 100 times. The more we do this, the closer we get to approximating the mean (3.5) as our histogram starts to get more narrow.\n\n\nset.seed(1)\nt &lt;- 0\nfor (i in 1:100) {\n    NumberofRolls &lt;- 1000\n    x &lt;- sample(d, NumberofRolls, replace = TRUE)\n    t[i] &lt;- mean(x)\n}\nhist(t, xlim = c(3, 4))\n\n\n\n\n\n\n\n\n\nImportant to note that for any sample size \\(n\\), the sampling distribution of \\(\\bar{x}\\) is normal if the population X from which the sample is drawn is normally distributed, meaning, there is no need to use CLT.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#standard-error-se",
    "href": "probability.html#standard-error-se",
    "title": "Probability and Probability Distributions",
    "section": "Standard Error (SE)",
    "text": "Standard Error (SE)\n\nThe \\(SE\\) is the standard deviation of the sampling distribution for all samples of size \\(n\\).\nIt is unusual to have the entire population for computing the population standard deviation, and it is also unusual to have a large number of samples from one population. A close approximation to this value is called the standard error of the mean. \\(SE=sd/\\sqrt(n)\\).\nStandard Deviation vs. Standard Error\n\nSD: Measure of variability in the sample.\nSE: Estimate of how closely the sample represents the population.\n\nPurpose: This is the standard error formula used to assess the distribution of sample means around the population mean.\nInterpretation: This measures how much the sample mean, \\(\\bar{X}\\) is expected to vary from the population mean given the sample size \\(n\\).\nThe standard deviation measures the spread of individual data points, while the standard error measures the spread of sample means.\nThe standard deviation and standard error are both measures of variability but are used in different contexts and serve different purposes. The standard deviation quantifies the spread of individual data points within a dataset. It is often used when analyzing the distribution of a single sample or population. On the other hand, the standard error is a measure of the precision of the sample mean in estimating the population mean. It tells us how much the sample mean is likely to vary from the true population mean if we were to take repeated samples. The standard error is derived from the standard deviation but is scaled by the square root of the sample size, meaning it decreases as the sample size increases. This makes it particularly useful in inferential statistics, where it helps assess the accuracy of estimates based on sample data.\nExample using SD: Given that \\(\\mu\\) = 16 inches and \\(\\sigma\\) = 0.8 inches, determine the following: What is the probability that a randomly selected pizza is less than 15.5 inches?\n\n\n# P(X &lt; 15.5)\npnorm(15.5, 16, 0.8)\n\n[1] 0.2659855\n\n\n\nExample using SE: What is the probability that 2 randomly selected pizzas average less than 15.5 inches?\n\n\\(P(\\bar{X} &lt; 15.5)\\)\n\n\npnorm(15.5, 16, 0.8/sqrt(2))\n\n[1] 0.1883796\n\n\nAdditional Examples using SE:\n\nAnne wants to determine if the marketing campaign has had a lingering effect on the amount of money customers spend on coffee. Before the campaign, \\(\\mu\\) = $4.18 and \\(\\sigma\\) = $0.84. Based on 50 customers sampled after the campaign, \\(\\mu\\) = $4.26.\n\\(P(\\bar{X} &gt; 4.26)\\)\n\n\npnorm(4.26, 4.18, 0.84/sqrt(50), lower.tail = FALSE)\n\n[1] 0.2503353\n\n\n\nOver the entire six years that students attend an Ohio elementary school, they are absent, on average, 28 days due to influenza. Assume that the standard deviation over this time period is \\(\\sigma\\) = 9 days. Upon graduation from elementary school, a random sample of 36 students is taken and asked how many days of school they missed due to influenza. What is the probability that the sample mean is between 25 and 30 school days?\n\n\\(P(\\bar{X} &lt; 30)-P(\\bar{X} &lt; 25)\\)\n\n\npnorm(30, 28, 9/sqrt(36)) - pnorm(25, 28, 9/sqrt(36))\n\n[1] 0.8860386\n\n\n\nAccording to the Bureau of Labor Statistics, it takes an average of 22 weeks for someone over 55 to find a new job. Assume that the probability distribution is normal and that the standard deviation is two weeks. What is the probability that eight workers over the age of 55 take an average of more than 20 weeks to find a job?\n\\(P(\\bar{X} &gt;20)\\)\n\n\npnorm(20, 22, 2/sqrt(8), lower.tail = FALSE)\n\n[1] 0.9976611",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#sampling-distribution-of-the-sample-proportion",
    "href": "probability.html#sampling-distribution-of-the-sample-proportion",
    "title": "Probability and Probability Distributions",
    "section": "Sampling Distribution of the Sample Proportion",
    "text": "Sampling Distribution of the Sample Proportion\n\nEstimator Sample proportion \\(\\bar{P}\\) is used to estimate the population parameter \\(p\\).\nEstimate: A particular value of the estimator \\(\\bar{p}\\).\nThe expected value of \\(\\bar{P}=E(\\bar{P})=𝑝\\).\nThe standard deviation of \\(\\bar{P}\\) is referred to as the standard error of the sample proportion, which equals \\(se(\\bar{P}) = \\sqrt(p*(1-p)/n\\).\nPurpose: This formula calculates the z-score for a sample proportion \\(\\hat{P}\\) by comparing it to a population proportion, \\(P\\).\nInterpretation: It measures how far the sample proportion deviates from the population proportion in terms of the standard deviation (for proportions).\nThe Central Limit Theorem for the Sample Proportion.\n\nFor any population proportion \\(p\\), the sampling \\(\\bar{P}\\) is approximately normal if the sample size \\(n\\) is sufficiently large.\nAs a general guideline, the normal distribution approximation is justified when \\(n*p&gt;=5\\) and \\(n((1-p)&gt;=5\\).\nIf \\(\\bar{P}\\) is normal, we can transform it into the standard normal random variable as \\(Z=(\\hat{P}-E(\\bar{P})/se(\\bar{P})\\) = \\((\\hat{P}-p)/(\\sqrt(p*(1-p)/n))\\).\nUsing the pnorm() function, this would translate to the following: \\(pnorm(\\hat{P}, E(\\bar{P}), se(\\bar{P})\\) , where \\(se(\\bar{P}) = \\sqrt(p*(1-p)/n)\\)\n\n\n\nExamples Using Proportions\n\nAnne wants to determine if the marketing campaign has had a lingering effect on the proportion of customers who are women and teenage girls. Before the campaign, p = 0.43 for women and p = 0.21 for teenage girls. Based on 50 women and 50 teenage girls sampled after the campaign, p = 0.46 and p = 0.34, respectively.\nTo calculate the probability that the marketing campaign had a lingering effect on women, we use \\(P(\\hat{P} &gt;= .46)\\).\n\n\npnorm(0.46, 0.43, sqrt(0.43 * (1 - 0.43)/50), lower.tail = FALSE)\n\n[1] 0.3341494\n\n\n\nThe probability that the observed proportion is 0.46 or higher, assuming the true proportion remains 0.43, is approximately 0.3341.\nTo calculate the probability that the marketing campaign had a lingering effect on teenage girls, we use \\(P(\\hat{P} &gt;= .34)\\).\n\n\npnorm(0.34, 0.21, sqrt(0.21 * (1 - 0.21)/50), lower.tail = FALSE)\n\n[1] 0.01200832\n\n\n\nThe probability that the observed proportion is 0.34 or higher, assuming the true proportion remains 0.21, is approximately 0.0120.\nThe result for teenage girls is statistically significant, while the result for women suggests no strong evidence of an effect.\nThe labor force participation rate is the number of people in the labor force divided by the number of people in the country who are of working age and not institutionalized. The BLS reported in February 2012 that the labor force participation rate in the United States was 63.7%. A marketing company asks 120 working-age people if they either have a job or are looking for a job, or, in other words, whether they are in the labor force. What is the probability that between 60% and 62.5% of those surveyed are members of the labor force?\nHere we find \\(P(\\hat{P} &lt;= .625)-P(\\hat{P} &lt;= .6)\\).\n\n\npnorm(0.625, 0.637, sqrt(0.637 * (1 - 0.637)/120)) - pnorm(0.6, 0.637,\n    sqrt(0.637 * (1 - 0.637)/120))\n\n[1] 0.192639\n\n\n\nThere is a 19.26% chance that 60% to 62.5% of those surveyed are members of the labor force.\nSometimes it is helpful to assign variables so that you can use the consistent functions. The example below does that.\n\n\n\n## Between .625 and .6 - given sample size of 120 and a p of .637\np &lt;- 0.637\nn &lt;- 120\nphat1 &lt;- 0.625\nphat2 &lt;- 0.6\nQ1 &lt;- pnorm(phat1, p, sqrt(p * (1 - p)/n)) - pnorm(phat2, p, sqrt(p * (1 -\n    p)/n))\nQ1  #0.192639\n\n[1] 0.192639",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#transformations-of-variables",
    "href": "probability.html#transformations-of-variables",
    "title": "Probability and Probability Distributions",
    "section": "Transformations of Variables",
    "text": "Transformations of Variables\n\nIf data is not normally distributed, we need to conduct a transformation. When we transform a variable, we hope to change the shape to normal so that we can continue to calculate under the rules of the normal distribution. For variables that are right skewed, a few transformations that could work to make the variable more normally distributed are: square root, cube root, reciprocal, and log.\nLet’s do an example with the opioid data set discussed earlier, this time using opioidFacility.csv.\nFirst, read in the opioid data set from so we can see a variable that is considered not normal.\n\n\n\n# Distance to substance abuse facility with medication-assisted\n# treatment\ndist.mat &lt;- read.csv(\"data/opioidFacility.csv\")\n# Review the data\nsummary(dist.mat)\n\n    STATEFP         COUNTYFP          YEAR       INDICATOR        \n Min.   : 1.00   Min.   :  1.0   Min.   :2017   Length:3214       \n 1st Qu.:19.00   1st Qu.: 35.0   1st Qu.:2017   Class :character  \n Median :30.00   Median : 79.0   Median :2017   Mode  :character  \n Mean   :31.25   Mean   :101.9   Mean   :2017                     \n 3rd Qu.:46.00   3rd Qu.:133.0   3rd Qu.:2017                     \n Max.   :72.00   Max.   :840.0   Max.   :2017                     \n     VALUE           STATE           STATEABBREVIATION     COUNTY         \n Min.   :  0.00   Length:3214        Length:3214        Length:3214       \n 1st Qu.:  9.25   Class :character   Class :character   Class :character  \n Median : 18.17   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 24.04                                                           \n 3rd Qu.: 31.00                                                           \n Max.   :414.86                                                           \n\n\n\n# Graph the distance variable which is called Value but represents\n# miles.  Note that this graph does not look normal - instead, it\n# looks right or positive skewed.\ndist.mat %&gt;%\n    ggplot(aes(VALUE)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Miles to nearest substance abuse facility\",\n    y = \"Number of counties\")\n\n\n\n\n\n\n\n\n\nNext, transform the variable to the 4 recommended transformations to see which one works best. We cannot see that result yet until we graph these results.\n\nThis requires 4 separate calculations using mutate() commands.\n\n\ndist.mat.cleaned &lt;- dist.mat %&gt;%\n    mutate(miles.cube.root = VALUE^(1/3)) %&gt;%\n    mutate(miles.log = log(x = VALUE)) %&gt;%\n    mutate(miles.inverse = 1/VALUE) %&gt;%\n    mutate(miles.sqrt = sqrt(x = VALUE))\n\nNow, graph the variable with the 4 recommended transformations to see which is most normal (bell shaped).\n\n\ncuberoot &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.cube.root)) + geom_histogram(fill = \"#7463AC\",\n    color = \"white\") + theme_minimal() + labs(x = \"Cube root of miles to nearest facility\",\n    y = \"Number of counties\")\n\nlogged &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.log)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Log of miles to nearest facility\", y = \"\")\n\ninversed &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.inverse)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + xlim(0, 1) + labs(x = \"Inverse of miles to nearest facility\",\n    y = \"Number of counties\")\n\nsquareroot &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.sqrt)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Square root of miles to nearest facility\",\n    y = \"\")\n\n\nWe can show all 4 graphs at one time to directly compare. Ensure your plot window is large enough to see this.\n\n\ngridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)\n\n\n\n\n\n\n\n\n\nFinally, determine if any of the transformations help. In this example, we determined that the cuberoot had the most normal transformation. The cube root graph contains a nice bell shape curve.\nLet’s use that new variable in the analysis. Start by summarizing the descriptive statistics, including retrieving the mean and standard deviation for cube root of miles, which are values that are required in the probability calculations.\n\n\ndist.mat.cleaned %&gt;%\n    drop_na(miles.cube.root) %&gt;%\n    summarize(mean.tran.dist = mean(x = miles.cube.root), sd.tran.dist = sd(x = miles.cube.root))\n\n  mean.tran.dist sd.tran.dist\n1       2.662915    0.7923114\n\n\n\n2.66 and .79 are the values we pulled for mean and standard deviation. We can use that information to calculate probabilities based on the functions we mentioned above.\nSo, what happens if the cuberoot of X &lt; 3 or less than 27 miles from the facility?\nWe estimate that about 66% of counties fall in the shaded area, having to travel less than 27 miles to nearest facility (27 = 3^3).\nThis means that (1- 0.6665403)*100 is the percentage of countries having to travel more than 27 miles to the nearest facility.\n\n\n27^(1/3)\n\n[1] 3\n\n3^3\n\n[1] 27\n\n# P(X&lt; cuberoot(27) = P(X &lt; 3)\npnorm(3, 2.66, 0.79)  ##about 66% likely\n\n[1] 0.6665403\n\n# P(X &gt; 3) #about 33% likely\npnorm(3, 2.66, 0.79, lower.tail = FALSE)\n\n[1] 0.3334597\n\n1 - pnorm(3, 2.66, 0.79)\n\n[1] 0.3334597\n\n\n\nWe estimate that about 20% of counties fall in the shaded area, having to travel &lt; 8 miles to nearest facility (8 = 2^3).\n\n\npnorm(2, 2.66, 0.79)\n\n[1] 0.2017342\n\n\n\nWe can use the equation to calculate the z-score for a county where you have to drive 15 miles to a facility.\n\n\n## z = (x-m)/sd since we are in cube root - we multiply x by ^1/3\n(15^(1/3) - 2.66)/0.79\n\n[1] -0.2453012\n\n\n\nThe transformed distance of a facility 15 miles away is .24 standard deviations LOWER than the mean transformed distance.\nNext, we can calculate z for a county with residents who have to travel 50 miles to the nearest facility. In the transformed miles variable, this would be the cube root of 50, or a value of 3.68.\n\n\n(50^(1/3) - 2.66)/0.79  #[1] 1.296242\n\n[1] 1.296242\n\n\n\nThis indicated that the transformed distance to a facility with MAT for this example county was 1.29 standard deviations above the mean transformed distance from a county to a facility with MAT.\n\n\nTransformation Second Example\n\nTaking a second example, let us look at the PHYSHLTH variable from the gender dataset (brfss.csv). We worked with this dataset in an earlier lesson. In doing so, we cleaned the data.\nI copied over that data preparation code in regards to the variable of interest (PHYSHLTH), and tidied it up for one example. To remind ourselves, the question being asked was the following, “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?”\nIf ever you are using the MASS package and dplyr, the select function may have a conflict where R does not know which to use. If you get an error when using select, add dplyr:: in front of the statement to ensure you are using select from dplyr to select variables.\n\n\n#\ngender &lt;- read.csv(\"data/brfss.csv\")\n# Review the data\nsummary(gender)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n# PHYSHLTH example\ngender.clean &lt;- gender %&gt;%\n    dplyr::select(PHYSHLTH) %&gt;%\n    drop_na() %&gt;%\n    # Turn the 77 values to NA, since 77 meant don't know or not sure\n    # from the brss codebook\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA, since 99 meant Refuled from the brss\n    # codebook.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to 0 - since the number 88 meant 0 days of\n    # illness from the brss codebook.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\ntable(gender.clean$PHYSHLTH)\n\n\n     0      1      2      3      4      5      6      7      8      9     10 \n291696  19505  24890  14713   7644  12931   2140   8049   1478    325   9437 \n    11     12     13     14     15     16     17     18     19     20     21 \n   133    908     92   4558   8638    221    153    279     51   5554   1111 \n    22     23     24     25     26     27     28     29     30 \n   132     80     98   2270    149    204    831    390  35701 \n\nsummary(gender.clean)\n\n    PHYSHLTH     \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 0.000  \n Mean   : 4.224  \n 3rd Qu.: 3.000  \n Max.   :30.000  \n NA's   :10299   \n\nqnorm(0.95, mean = 500, sd = 10)\n\n[1] 516.4485\n\n\n\nOnce here, we graph PHYSHLTH.\n\n\n\ngender.clean %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Number of Days Sick\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe determined from the descriptive statistics lesson that this variable had severe skewness (positive). Most people had 0 days of illness.\nNext, we run the 4 calculations by mutating the variable and saving all 4 transformation under new variable names.\n\n\ngenderTransform &lt;- gender.clean %&gt;%\n    mutate(phy.cube.root = PHYSHLTH^(1/3)) %&gt;%\n    mutate(phy.log = log(x = PHYSHLTH)) %&gt;%\n    mutate(phy.inverse = 1/PHYSHLTH) %&gt;%\n    mutate(phy.sqrt = sqrt(x = PHYSHLTH))\n\n\nNext, we create the 4 graphs for each of the 4 transformations labelled above to see if one helps.\n\n\ncuberoot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.cube.root)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Cube root\", y = \"\")\nlogged &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.log)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Log\", y = \"\")\ninversed &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.inverse)) + xlim(0, 1) + geom_histogram(fill = \"#7463AC\",\n    color = \"white\", binwidth = 0.05) + theme_minimal() + labs(x = \"Inverse\",\n    y = \"\")\nsquareroot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.sqrt)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 1) + theme_minimal() + labs(x = \"Square root\", y = \"\")\n\n\nFinally, we plot the graphs using gridExtra so that we can see all 4.\n\n\ngridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)\n\n\n\n\n\n\n\n\n\nIn this example, NOT ONE transformation helped. If this happens, something else would need to occur before correctly using the variable. Examples could be to run a non-linear model, or categorizing the data into bins, especially since there was a large frequency of people that were not ill.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#using-ai",
    "href": "probability.html#using-ai",
    "title": "Probability and Probability Distributions",
    "section": "Using AI",
    "text": "Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about probability and contingency tables.\nWhat are the key characteristics of a probability distribution, and how do you determine whether a given set of values represents a valid probability distribution?\nWhat is the difference between a discrete and a continuous random variable, and how do the probability distributions differ for each type?\nHow do you calculate the expected value and variance for a discrete random variable, and why are these summary measures important in understanding probability distributions?\nWhat are the properties of a binomial distribution, and how is it used to calculate the probability of a certain number of successes in a fixed number of trials?\nHow do you use the dbinom() and pbinom() functions in R to calculate the probability of exact or cumulative successes in a binomial experiment?\nWhat is a normal distribution, and how do you calculate z-scores to determine how far an observation is from the mean of a normally distributed variable?\nHow do you interpret and calculate cumulative probabilities for both discrete and continuous variables using the cumulative distribution function (CDF)”\nWhat is the Central Limit Theorem (CLT), and why is it important when working with large samples and understanding the distribution of sample means?\nWhen data is not normally distributed, what transformations can you apply to make the data more normally distributed, and how do you determine which transformation is most effective?\nHow do you apply probability functions such as pnorm() and qnorm() in R to solve real-world problems, such as calculating the likelihood of events based on normal distributions?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#summary",
    "href": "probability.html#summary",
    "title": "Probability and Probability Distributions",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson, we learned about the basic rules of probability alongside the binomial distribution and continuous distribution. We learned about the normal distribution and the limitations of using that distribution. We also learned how to transform variables that were not normal.\nIn a normal distribution, we used 3 main formulas with the pnorm() function. Each formula has a different role, but they all provide a way to assess variability relative to an average value, allowing for comparisons and inferences about the data or sample in question.\n\n\\[\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Context} & \\textbf{Formula} & \\textbf{Interpretation} & \\textbf{pnorm Function Usage} \\\\\n\\hline\n\\text{Standardizing individual data points} & \\frac{x - \\text{mean}(x)}{\\text{sd}(x)} & \\text{Z-score for individual values} & \\text{pnorm}(x, \\text{mean}, \\text{sd}) \\\\\n\\hline\n\\text{Standardizing the sample mean} & \\frac{x - \\text{mean}(x)}{\\text{sd}(x) / \\sqrt{n}} & \\text{Used for hypothesis tests of means} & \\text{pnorm}(x, \\text{mean}, \\text{se}) \\\\\n\\hline\n\\text{Standardizing a sample proportion} & \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1 - p)}{n}}} & \\text{Used for hypothesis tests of proportions} & \\text{pnorm}(\\hat{p}, p, \\sqrt{\\frac{p(1 - p)}{n}}) \\\\\n\\hline\n\\end{array}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  }
]