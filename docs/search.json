[
  {
    "objectID": "dataprep.html",
    "href": "dataprep.html",
    "title": "4  Data Preparation",
    "section": "",
    "text": "4.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#at-a-glance",
    "href": "dataprep.html#at-a-glance",
    "title": "4  Data Preparation",
    "section": "",
    "text": "In order to succeed in this lesson, we need to be able to evaluate variables and understand how to clean and prepare data to make variables easier to use and in the correct form. This sometimes includes subsetting and filtering data alongside other techniques.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#lesson-objectives",
    "href": "dataprep.html#lesson-objectives",
    "title": "4  Data Preparation",
    "section": "4.2 Lesson Objectives",
    "text": "4.2 Lesson Objectives\n\nCreate variables and identify and change data types.\nLearn how to clean data via dplyr.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#consider-while-reading",
    "href": "dataprep.html#consider-while-reading",
    "title": "4  Data Preparation",
    "section": "4.3 Consider While Reading",
    "text": "4.3 Consider While Reading\n\nWe often spend a considerable amount of time inspecting and preparing the data for the subsequent analysis. This includes the following:\n\nEvaluating Data Types\nSorting Data\nSelecting Variables\nFiltering Data\nCounting Data\nHandling Missing Values\nSummarizing\nGrouping Data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#nominal-example-with-dataset",
    "href": "dataprep.html#nominal-example-with-dataset",
    "title": "4  Data Preparation",
    "section": "5.1 Nominal Example with Dataset",
    "text": "5.1 Nominal Example with Dataset\n\nlibrary(tidyverse)\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\n\n\n# Examine the variable types with summary and class functions.\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nclass(gss.2016$grass)  #Check the data type.\n\n[1] \"character\"\n\ngss.2016$grass &lt;- as.factor(gss.2016$grass)  #Turn to a factor.\nclass(gss.2016$grass)  #Confirming it is now correct.\n\n[1] \"factor\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#numerical-example-with-dataset",
    "href": "dataprep.html#numerical-example-with-dataset",
    "title": "4  Data Preparation",
    "section": "5.2 Numerical Example with Dataset",
    "text": "5.2 Numerical Example with Dataset\n\nWe need to ensure data can be coded as numeric before using the as.numeric() command. For example, to handle the variable age, it seems like numerical values except one value of “89 OR OLDER”. If as.numeric() command was used on this variable, it would put all the 89 and older observations as NAs. To force it to be a numerical variable, and keep that the sample participants were the oldest value, we need to recode it and then use the as.numeric() command to coerce it into a number.\nRecoding the 89 and older to 89 does cause the data to lack integrity in its current form because it will treat the people over 89 years old as 89. But, we are limited here because this needs to be a numerical variable for us to proceed. We will learn a step later on in this section to transform the age variable into categories so that we bring back our data integrity.\n\n\nclass(gss.2016$age)\n\n[1] \"character\"\n\n# Recode '89 OR OLDER' into just '89'\ngss.2016$age &lt;- recode(gss.2016$age, `89 OR OLDER` = \"89\")\n# Convert to numeric data type\ngss.2016$age &lt;- as.numeric(gss.2016$age)\nsummary(gss.2016)  #Conduct final check confirming correct data types\n\n       grass           age       \n DK       : 110   Min.   :18.00  \n IAP      : 911   1st Qu.:34.00  \n LEGAL    :1126   Median :49.00  \n NOT LEGAL: 717   Mean   :49.16  \n NA's     :   3   3rd Qu.:62.00  \n                  Max.   :89.00  \n                  NA's   :10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#limitations-of-using-a-missing-data-technique",
    "href": "dataprep.html#limitations-of-using-a-missing-data-technique",
    "title": "4  Data Preparation",
    "section": "11.1 Limitations of Using a Missing Data Technique",
    "text": "11.1 Limitations of Using a Missing Data Technique\n\nRecommended Closer Evaluation of Missing Data\nThere are limitations of both techniques listed above (omission and imputation).\n\nIf a large number of values are missing, mean imputation will likely distort the relationships among variables, leading to biased results.\nRemoving missing values could also significantly reduce your data set size.\nMissing data needs to be closely evaluated and verified within each variable whether the data is truly blank, has no answer, or is marked with a character value such as the text N/A.\nIf the variable that has many missing values is deemed unimportant or can be represented using a proxy variable that does not have missing values, the variable may be excluded from the analysis.\n\nMissing data needs to be closely evaluated to see if the missing value is meaningful or not.\n\nFor instance, getting data on how many pregnancies would only be applicable to people born of women gender, and blank value for people born of male gender, who are unable to have children, would be expected. In taking this example further, if variable 1 targeted the question, “how many pregnancies have you have had,” we would expect missing data or NAs for all the men. If comparing that variable to a second variable “Incubated from COVID-19: Yes/No” we would not want to omit all the blanks in the dataset because then we would eliminate analysis of an entire gender. Thus a different technique should be chosen besides omitting the blanks to be able to evaluate more concisely.\n\n\nIf a value is not blank and is considered missing, data needs to be mutated to be consistent with the technique of coding true missing values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#the-na.rm-parameter",
    "href": "dataprep.html#the-na.rm-parameter",
    "title": "4  Data Preparation",
    "section": "11.2 The na.rm Parameter",
    "text": "11.2 The na.rm Parameter\n\ny &lt;- c(1, 2, NA, 3, 4, NA)\n# These lines runs, but do not give you anything useful.\nsum(y)\n\n[1] NA\n\nmean(y)\n\n[1] NA\n\n\n\nMany functions in R include parameters that will ignore NAs for you.\n\nsum() and mean() are examples of this, and most summary statistics like median() and var() also use the na.rm parameter to ignore the NAs. Always check the help to determine if na.rm is a parameter.\n\n\nsum(y, na.rm = TRUE)\n\n[1] 10\n\nmean(y, na.rm = TRUE)\n\n[1] 2.5\n\n# na.omit removes the NAs from the data set.\ny &lt;- na.omit(y)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#is.na",
    "href": "dataprep.html#is.na",
    "title": "4  Data Preparation",
    "section": "11.3 is.na()",
    "text": "11.3 is.na()\n\nIn R, the is.na() function is used to check for missing (NA) values in objects like vectors, data frames, or arrays. It returns a logical vector of the same length as the input object, where TRUE indicates a missing value and FALSE indicates a non-missing value.\n\n\n# Gives the observation number of the observations that include NA\n# values\nwhich(is.na(gig$Industry))\n\n [1]  24 139 361 378 441 446 479 500 531 565\n\n# Produces a dataset with observations that have NA values in the\n# Industry field.\nShowBlankObservations &lt;- gig %&gt;%\n    filter(is.na(Industry))\nShowBlankObservations\n\n   EmployeeID  Wage Industry        Job\n1          24 42.58     &lt;NA&gt;  Sales Rep\n2         139 42.18     &lt;NA&gt;   Engineer\n3         361 31.33     &lt;NA&gt;      Other\n4         378 48.09     &lt;NA&gt;      Other\n5         441 32.35     &lt;NA&gt; Accountant\n6         446 30.76     &lt;NA&gt; Accountant\n7         479 42.85     &lt;NA&gt; Consultant\n8         500 43.13     &lt;NA&gt;  Sales Rep\n9         531 43.13     &lt;NA&gt;   Engineer\n10        565 38.98     &lt;NA&gt; Accountant\n\n# Counts the number of observations that have NA values in the\n# Industry field.\nCountBlanks &lt;- gig %&gt;%\n    filter(is.na(Industry)) %&gt;%\n    count(Industry)\nCountBlanks\n\n  Industry  n\n1     &lt;NA&gt; 10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#using-na_if",
    "href": "dataprep.html#using-na_if",
    "title": "4  Data Preparation",
    "section": "11.4 Using na_if()",
    "text": "11.4 Using na_if()\n\nThe na_if() function in tidyr is used to replace specific values in a column with NA (missing) values. This function can be particularly useful when you want to standardize missing values across a dataset or when you want to replace certain values with NA for further data processing\n\n\nTurnNA &lt;- gig %&gt;%\n    mutate(Job = na_if(Job, \"Other\"))\nhead(TurnNA)\n\n  EmployeeID  Wage     Industry        Job\n1          1 32.81 Construction    Analyst\n2          2 46.00   Automotive   Engineer\n3          3 43.13 Construction  Sales Rep\n4          4 48.09   Automotive       &lt;NA&gt;\n5          5 43.62   Automotive Accountant\n6          6 46.98 Construction   Engineer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#na.omit-vs.-drop_na",
    "href": "dataprep.html#na.omit-vs.-drop_na",
    "title": "4  Data Preparation",
    "section": "11.5 na.omit() vs. drop_na()",
    "text": "11.5 na.omit() vs. drop_na()\n\nBoth functions return a new object with the rows containing missing values removed.\nna.omit() is a base R function, so it doesn’t require any additional package installation where drop_na() requires loading the tidyr package, which is part of the tidyverse ecosystem.\ndrop_na() fits well into tidyverse pipelines, making it easy to integrate with other tidyverse functions where na.omit() can also be used in pipelines but might require additional steps to fit seamlessly.\n\n\n# install.packages('Amelia')\nlibrary(Amelia)\ndata(\"africa\")\nsummary(africa)\n\n      year              country       gdp_pc            infl        \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.400  \n 1st Qu.:1977   Burundi     :20   1st Qu.: 513.8   1st Qu.:  4.760  \n Median :1982   Cameroon    :20   Median :1035.5   Median :  8.725  \n Mean   :1982   Congo       :20   Mean   :1058.4   Mean   : 12.753  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1244.8   3rd Qu.: 13.560  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.890  \n                                  NA's   :2                         \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4332190  \n Median : 59.59   Median :0.1667   Median : 5853565  \n Mean   : 62.60   Mean   :0.2889   Mean   : 5765594  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7355000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n NA's   :5                                           \n\nsummary(africa$gdp_pc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  376.0   513.8  1035.5  1058.4  1244.8  2723.0       2 \n\nsummary(africa$trade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.35   38.52   59.59   62.60   81.16  134.11       5 \n\nafrica1 &lt;- na.omit(africa)\nsummary(africa1)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n## to drop all at once.\nafrica2 &lt;- africa %&gt;%\n    drop_na()\nsummary(africa2)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n\n\nYou try to load the airquality dataset from base R and look at a summary of the dataset.\n\nSum the number of NAs in airquality.\nOmit all the NAs from airquality and save it in a new data object called airqual and take a new summary of it.\n\n\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n\n\n\n[1] 44\n\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#brfss-data-cleaning",
    "href": "dataprep.html#brfss-data-cleaning",
    "title": "4  Data Preparation",
    "section": "15.1 brfss Data Cleaning",
    "text": "15.1 brfss Data Cleaning\n\nThe full codebook where this screenshot is taken is brfss_2014_codebook.pdf.\n\n\n\n\nEvaluate CodeBook Before Making Decisions\n\n\n\nbrfss &lt;- read.csv(\"data/brfss.csv\")\nsummary(brfss)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n\n\n15.1.1 Qualitative Variable\n\nTo look at an example, the one below seeks to understand the healthcare issue in reporting gender based on different definitions. The dataset is part of the Behavioral Risk Factor Surveillance System (brfss) dataset (2014), which includes lots of other variables besides reported gender.\n\n\n# Load the data\nbrfss &lt;- read.csv(\"data/brfss.csv\")\n# Summarize the TRNSGNDR variable\nsummary(object = brfss$TRNSGNDR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    4.00    4.00    4.06    4.00    9.00  310602 \n\n# Find frequencies\ntable(brfss$TRNSGNDR)\n\n\n     1      2      3      4      7      9 \n   363    212    116 150765   1138   1468 \n\n\n\nSince this table is not very informative, we need to do some edits.\nCheck the class of the variable to see the issue with analyzing it as a categorical variable.\n\n\nclass(brfss$TRNSGNDR)\n\n[1] \"integer\"\n\n\n\nFirst, we need to change the TRNSGNDR variable to a factor using as.factor().\n\n\n# Change variable from numeric to factor\nbrfss$TRNSGNDR &lt;- as.factor(brfss$TRNSGNDR)\n# Check data type again to ensure factor\nclass(brfss$TRNSGNDR)\n\n[1] \"factor\"\n\n\n\nThen, we need to do some data cleaning on the TRNSGNDR Variable.\n\n\nbrfss.cleaned &lt;- brfss %&gt;% \n  mutate(TRNSGNDR = recode_factor(TRNSGNDR,\n      '1' = 'Male to female',\n      '2' = 'Female to male',\n      '3' = 'Gender non-conforming',\n      '4' = 'Not transgender',\n      '7' = 'Not sure',\n      '9' = 'Refused'))\n\n\nWe can use the levels() command to show the factor levels made with the mutate() command above.\n\n\nlevels(brfss.cleaned$TRNSGNDR)\n\n[1] \"Male to female\"        \"Female to male\"        \"Gender non-conforming\"\n[4] \"Not transgender\"       \"Not sure\"              \"Refused\"              \n\n\n\nCheck the summary.\n\n\nsummary(brfss.cleaned$TRNSGNDR)\n\n       Male to female        Female to male Gender non-conforming \n                  363                   212                   116 \n      Not transgender              Not sure               Refused \n               150765                  1138                  1468 \n                 NA's \n               310602 \n\n\n\nTake a good look at the table to interpret the frequencies in the output above. The highest percentage was the “NA’s” category, followed by “Not transgender”. Removing the NA’s moved the “Not transgender” category to over 97% of observations.\n\n\n\n15.1.2 Quantitative Variable\n\nLet’s use the cleaned dataset to make more changes to the continuous variable PHYSHLTH. In the codebook, it looks like the data is most applicable to the first 2 categories. The 1-30 days coding and the 88 coding, which means 0 days of physical illness and injury.\n\nUsing cleaned data, we need to prep the variable a little more before getting an accurate plot.\nSpecifically, we need to null out the 77 and 99 values and make sure the 88 coding is set to be 0 for 0 days of illness and injury.\n\n\n\nbrfss.cleaned &lt;- brfss %&gt;%\n    mutate(TRNSGNDR = recode_factor(TRNSGNDR, `1` = \"Male to female\", `2` = \"Female to male\",\n        `3` = \"Gender non-conforming\", `4` = \"Not transgender\", `7` = \"Not sure\",\n        `9` = \"Refused\")) %&gt;%\n    # Turn the 77 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to be numeric value of 0.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\n\n\nThe histogram showed most people have between 0 and 10 unhealthy days per 30 days.\nNext, evaluate mean, median, and mode for the PHYSHLTH variable after ignoring the blanks.\n\n\nmean(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 4.224106\n\nmedian(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 0\n\nnames(x = sort(x = table(brfss.cleaned$PHYSHLTH), decreasing = TRUE))[1]\n\n[1] \"0\"\n\n\n\nWhile the mean is higher at 4.22, the median and most common number is 0.\n\n\n## Spread to Report with the Mean\nvar(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 77.00419\n\nsd(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 8.775203\n\n## Spread to Report with Median\nsummary(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.000   0.000   4.224   3.000  30.000   10303 \n\nrange(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1]  0 30\n\nmax(brfss.cleaned$PHYSHLTH, na.rm = TRUE) - min(brfss.cleaned$PHYSHLTH,\n    na.rm = TRUE)\n\n[1] 30\n\nIQR(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 3\n\n\n\nlibrary(semTools)\n# Plot the data\nbrfss.cleaned %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram()\n\n\n\n\n\n\n\n# Calculate Skewness and Kurtosis\nskew(brfss.cleaned$PHYSHLTH)\n\n   skew (g1)           se            z            p \n2.209078e+00 3.633918e-03 6.079054e+02 0.000000e+00 \n\nkurtosis(brfss.cleaned$PHYSHLTH)\n\nExcess Kur (g2)              se               z               p \n   3.474487e+00    7.267836e-03    4.780634e+02    0.000000e+00 \n\n\n\nThe skew results provide a z of 607.905 (6.079054e+02) which is much higher than 7 (for large datasets). This indicates a clear right skew which means the data is not normally distributed.\nThe kurtosis results are also very leptokurtic with a score of 478.063.\n\n\n\n15.1.3 Using Filters Example\n\nBelow takes an example of the brfss data to filter by certain variable statuses.\n\nThe first filter() chose observations that were any one of the three categories of transgender included in the data. Used the | “or” operator for this filter().\nThe second filter chose people in an age category above category 4 but below category 12, in the age categories 5 through 11.\nThe last filter used the !is.na to choose observations where HADMAM variable was not NA.\n\nNext, we reduce data set to contain only variables used to create table by using the select() command.\nNext, we change all the remaining variables in data set to factors using mutate_all() command. This not only changes the strings to factors, but also changes the numerical variables to factors.\nFinally, we use mutate() commands to change the variable category to something meaningful(from the codebook).\n\nNotice the backslash before the apostrophe in Don’t in the X_INCOMG recode. This is to prevent the .R file from ending the quotations. You could use double quotes around the statement to bypass this, or add the backslash like I did here.\n\n\nbrfss_small &lt;- brfss.cleaned %&gt;%\n    filter(TRNSGNDR == \"Male to female\" | TRNSGNDR == \"Female to male\" |\n        TRNSGNDR == \"Gender non-conforming\") %&gt;%\n    filter(X_AGEG5YR &gt; 4 & X_AGEG5YR &lt; 12) %&gt;%\n    filter(!is.na(HADMAM)) %&gt;%\n    select(TRNSGNDR, X_AGEG5YR, X_RACE, X_INCOMG, X_EDUCAG, HLTHPLN1, HADMAM) %&gt;%\n    mutate_all(as.factor) %&gt;%\n    # The next few mutates add labels to categorical variables based\n    # on the codebook.\nmutate(X_AGEG5YR = recode_factor(X_AGEG5YR, `5` = \"40-44\", `6` = \"45-49\",\n    `7` = \"50-54\", `8` = \"55-59\", `9` = \"60-64\", `10` = \"65-69\", `11` = \"70-74\")) %&gt;%\n    mutate(X_INCOMG = recode_factor(X_INCOMG, `1` = \"Less than 15,000\",\n        `2` = \"15,000 to less than 25,000\", `3` = \"25,000 to less than 35,000\",\n        `4` = \"35,000 to less than 50,000\", `5` = \"50,000 or more\", `9` = \"Don't know/not sure/missing\")) %&gt;%\n    mutate(X_EDUCAG = recode_factor(X_EDUCAG, `1` = \"Did not graduate high school\",\n        `2` = \"Graduated high school\", `3` = \"Attended college/technical school\",\n        `4` = \"Graduated from college/technical school\", `9` = NA_character_)) %&gt;%\n    mutate(HLTHPLN1 = recode_factor(HLTHPLN1, `1` = \"Yes\", `2` = \"No\",\n        `7` = \"Don't know/not sure/missing\", `9` = \"Refused\")) %&gt;%\n    mutate(X_RACE = recode_factor(X_RACE, `1` = \"White\", `2` = \"Black\",\n        `3` = \"Native American\", `4` = \"Asian/Pacific Islander\", `5` = \"Other\",\n        `6` = \"Other\", `7` = \"Other\", `8` = \"Other\", `9` = \"Other\"))\n# print a summary\nsummary(brfss_small)\n\n                  TRNSGNDR   X_AGEG5YR                     X_RACE   \n Male to female       : 77   40-44:27   White                 :152  \n Female to male       :113   45-49:27   Black                 : 31  \n Gender non-conforming: 32   50-54:32   Native American       :  4  \n Not transgender      :  0   55-59:44   Asian/Pacific Islander:  6  \n Not sure             :  0   60-64:44   Other                 : 29  \n Refused              :  0   65-69:24                               \n                             70-74:24                               \n                        X_INCOMG                                     X_EDUCAG \n Less than 15,000           :46   Did not graduate high school           :24  \n 15,000 to less than 25,000 :44   Graduated high school                  :86  \n 25,000 to less than 35,000 :19   Attended college/technical school      :68  \n 35,000 to less than 50,000 :26   Graduated from college/technical school:44  \n 50,000 or more             :65                                               \n Don't know/not sure/missing:22                                               \n\n HLTHPLN1  HADMAM \n Yes:198   1:198  \n No : 24   2: 22  \n           9:  2  \n\n\n\n\n\n\nThis data set full of categorical variables is now fully cleaned and ready to be analyzed!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "introR.html",
    "href": "introR.html",
    "title": "2  Introduction to R and RStudio",
    "section": "",
    "text": "2.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#at-a-glance",
    "href": "introR.html#at-a-glance",
    "title": "2  Introduction to R and RStudio",
    "section": "",
    "text": "In order to succeed in this lesson, you will need to start by having both R and RStudio downloaded. Then, the only way to learn R is to use it in various ways and to practice as much as you can.\nIt is also important to note that this is a statistics class and that R is a statistical computing software. Because of that, we need to not only pay attention to what we are typing in, but understand why we are typing it in the ways suggested, and also how we could do it differently to get similar if not the same results. We also need to understand what important information we can pull from what we have done. Finally, to succeed in this section, we need to begin to understand how to clean data, which is a very important step for data analysts. Specifically, we learn how to count, sort, and handle missing data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#lesson-objectives",
    "href": "introR.html#lesson-objectives",
    "title": "2  Introduction to R and RStudio",
    "section": "2.2 Lesson Objectives",
    "text": "2.2 Lesson Objectives\n\nBe Introduced to R and R Studio.\nSet up R and R Studio.\nUse basic Built-in Functions in R.\nEnter and load data into R.\nIdentify and treat missing values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#consider-while-reading",
    "href": "introR.html#consider-while-reading",
    "title": "2  Introduction to R and RStudio",
    "section": "2.3 Consider While Reading",
    "text": "2.3 Consider While Reading\n\nThis material is so important because it is likely the start of your R journey and will provide the groundwork for learning a modern approach to calculating statistics. R has a fairly steep learning curve, but, once you are over it, it becomes fairly easy to figure out new things. It is important to make connections to what we are doing and why we are doing it a certain way. There are rules to learning R, and you will get better with constant practice. Try to avoid just typing in code. Instead, determine why the code was typed the way it was and try to figure out all the other ways the code could also be typed to get the same answer. Practice, practice, practice!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#two-main-branches-of-statistics",
    "href": "introR.html#two-main-branches-of-statistics",
    "title": "2  Introduction to R and RStudio",
    "section": "3.1 Two Main Branches of Statistics",
    "text": "3.1 Two Main Branches of Statistics\n\nDescriptive Statistics - collecting, organizing, and presenting the data.\nInferential Statistics - drawing conclusions about a population based on sample data from that population.\n\nA population consists of all items of interest.\nA sample is a subset of the population.\nA sample statistic is calculated from the sample data and is used to make inferences about the population parameter.\n\nReasons for sampling from the population:\n\nToo expensive to gather information on the entire population.\nOften impossible to gather information on the entire population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#why-r",
    "href": "introR.html#why-r",
    "title": "2  Introduction to R and RStudio",
    "section": "4.1 Why R?",
    "text": "4.1 Why R?\n\nR is a very sophisticated statistical software that allows you to enter commands one-at-a-time, or write scripts using the R language.\nEasily installed, state-of-the-art, and it is free and open source and supported by a well-established R Community.\nR can be used with RStudio, which is a graphical user interface that allows you to do the following:\n\nwrite, edit, and execute code;\ngenerate, view, and store plots;\nmanage files, objects and data frames;\nintegrate with version control systems.\n\nR comes with community that helps in the development of R resources.\n\nA package is developed by R users to do one specific thing or a set of related things that could store a collection of functions, data, and code.\nA library is the place where the package is located on your computer.\n\nA repository is a central location where many developed packages are located and available for download. There are 3 big repositories, but we use Comprehensive R Archive Network, or CRAN, which is R’s main repository with over 18,000 packages available.\nR’s community is vast, and you can always seek information from the community to try to help you with a R related issue.\nR is Called a Dynamically Typed Language\nIn R, a variable itself is not declared of any data type.\nRather it receives the data type of the R object that is assigned to it.\nWe can change a variable’s data type if we want, but it will inherit one based on the object assigned to it. We will learn some common ways to do this in the data preparation section.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#r-script-files",
    "href": "introR.html#r-script-files",
    "title": "2  Introduction to R and RStudio",
    "section": "5.1 R Script Files",
    "text": "5.1 R Script Files\n\nUsing R Script Files:\n\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nUsing the Console\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you must re-enter them at the command line. Nothing saves for later.\n\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise.R script files help to solve this issue.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#create-a-new-r-script-file-chapter1.r",
    "href": "introR.html#create-a-new-r-script-file-chapter1.r",
    "title": "2  Introduction to R and RStudio",
    "section": "5.2 Create a New R Script File: Chapter1.R",
    "text": "5.2 Create a New R Script File: Chapter1.R\n\nTo save your notes from today’s lecture, create a .R file named Chapter1.R and save it to your project file you made in the last class.\nThere are a couple of parts to this chapter, and we can add code from today’s chapter in one file so that our code is stacked nicely together.\n\nFor each new chapter, start a new file and save it to your project folder.\n\n\n\n\nScreenshot of R Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#using-a-prolog",
    "href": "introR.html#using-a-prolog",
    "title": "2  Introduction to R and RStudio",
    "section": "5.3 Using a Prolog",
    "text": "5.3 Using a Prolog\n\nWe should include a prolog to introduce each R Script File during the course.\nA prolog is a set of comments at the top of a code file that provides information about what is in the file.\nIncluding a prolog is considered coding best practice.\nIt also names the files and resources used that facilitates identification.\nAn informal prolog is below:\n\n\n####################################\n# Project name: \n# Project purpose: \n# Code author name: \n# Date last edited: \n# Data used: \n# Libraries used: \n####################################\n\n\nOn your R Script File, add your own prolog following the template as shown.\nI like to add a line for Data used and Libraries used so we know what all we used in the script.\n\n\n####################################\n# Project name: Chapter 1\n# Project purpose: To create an R script file to learn about R. \n# Code author name: Pamela Schlosser\n# Date last edited: [Enter Date Here]\n# Data used: NA\n# Libraries used: NA\n####################################\n\n\nThen, as we work through our .R script and add data files or libraries to our code, we go back and edit the prolog.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#r-handles-text-in-multiple-ways",
    "href": "introR.html#r-handles-text-in-multiple-ways",
    "title": "2  Introduction to R and RStudio",
    "section": "5.4 R Handles Text in Multiple Ways",
    "text": "5.4 R Handles Text in Multiple Ways\n\nR can generally use single quotes or double quotes when marking text. However, if you use a single quote to start, use a single quote to end. The same for double quotes - ensure the pairing is the same quote type.\nYou sometimes need to be careful with nested quotes, but generally it does not matter which you use.\n\n\n\"This is a string\"\n\n[1] \"This is a string\"\n\n\"This is also a string\"\n\n[1] \"This is also a string\"\n\n\n\nWe can also add comments to our code to document our work and add notes to our self or to others.\n\n\n# This is a comment for documentation or annotation\n\n\nAdd the code above to your R file and run each line using Ctl + Enter or select all lines and click Run.\nTake note that nothing prints in the console after running a comment.\n\n\n\n\nChapter 1 R File",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#using-comments",
    "href": "introR.html#using-comments",
    "title": "2  Introduction to R and RStudio",
    "section": "5.5 Using Comments",
    "text": "5.5 Using Comments\n\nWe use comments to organize and explain code in our R Script file.\nBe sure to write clear code that does not need a lot of comments.\nInclude useful comments where needed so that anyone (including yourself in the future) can run and understand your code.\nIf something does not work, don’t delete it yet. Instead, comment it out while you troubleshoot it or to try different alternatives.\nNotice the prolog above is in comments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#note-on-r-markdown",
    "href": "introR.html#note-on-r-markdown",
    "title": "2  Introduction to R and RStudio",
    "section": "5.6 Note on R Markdown",
    "text": "5.6 Note on R Markdown\n\nThese files were formatted with RMarkdown. RMarkdown is a simple formatting syntax for authoring documents of a variety of types, including PowerPoint and html files.\nOn the document, RMarkdown prints the command and then follows the command with the output after 2 hashtags.\nIn your R Script File, you only need to type in the command and then run your code to get the same output as presented here.\n\n\n\n\nReading our HTML file",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#r-is-an-interactive-calculator",
    "href": "introR.html#r-is-an-interactive-calculator",
    "title": "2  Introduction to R and RStudio",
    "section": "5.7 R is an Interactive Calculator",
    "text": "5.7 R is an Interactive Calculator\n\nAn important facet of R is that it should serve as your sole calculator.\nTry these commands in your .R file by typing them in and clicking Ctr + Enter on each line.\n\n\n3 + 4\n\n[1] 7\n\n3 * 4\n\n[1] 12\n\n3/4\n\n[1] 0.75\n\n3 + 4 * 100^2\n\n[1] 40003\n\n\n\nTake note that order of operations holds in R: PEMDAS\n\nParentheses ()\nExponents ^ and \\(**\\)\nDivision \\(/\\), Multiplication \\(*\\), modulo, and integer division\nAddition + and Subtraction -\n\nNote that modulo and integer division have the same priority level as multiplication and division, where modulo is just the remainder.\n\n\nprint(2 + 3 * 5 - 7^2%%4 + (5/2))\n\n[1] 18.5\n\n5/2  #parentheses: = 2.5\n\n[1] 2.5\n\n7^2  #exponent:= 49\n\n[1] 49\n\n3 * 5  #multiplication: = 15\n\n[1] 15\n\n17%%4  #modulo: = 1\n\n[1] 1\n\n17%/%4  #integer division: = 4\n\n[1] 4\n\n2 + 15  #addition: = 17\n\n[1] 17\n\n17 - 1  #subtraction: = 16\n\n[1] 16\n\n16 + 2.5  #addition: = 18.5\n\n[1] 18.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#running-commands",
    "href": "introR.html#running-commands",
    "title": "2  Introduction to R and RStudio",
    "section": "5.8 Running Commands",
    "text": "5.8 Running Commands\n\nThere are a few ways to run commands via your .R file.\n\nYou can click Ctr + Enter on each line.\nYou can select all the lines you want to run and select Ctr + Enter.\nYou can select all the lines you want to run and select the run button as shown in the Figure.\n\n\n\n\n\nRun Code\n\n\n\nNow that I have asked you to add a couple lines of code, after this point, when R code is shown on this file, you should add it to your .R script file along with any notes you want. I won’t explicitly say - “add this code.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#the-assignment-operator-in-creating-objects",
    "href": "introR.html#the-assignment-operator-in-creating-objects",
    "title": "2  Introduction to R and RStudio",
    "section": "6.1 The Assignment Operator in Creating Objects",
    "text": "6.1 The Assignment Operator in Creating Objects\n\nEntering and Storing Variables in R requires you to make an assignment.\n\nWe use the assignment operator ‘&lt;-’ to assign a value or expression to a variable.\nWe typically do not use the = sign in R even though it works because it also means other things in R.\n\nSome examples are below to add to your .R file.\n\n\nstates &lt;- 29\nA &lt;- \"Apple\"\n# Equivalent statement to above - again = is less used in R.\nA = \"Apple\"\nprint(A)\n\n[1] \"Apple\"\n\n# Equivalent statement to above\nA\n\n[1] \"Apple\"\n\nB &lt;- 3 + 4 * 12\nB\n\n[1] 51\n\n\n\n\n\nThe Assignment Operator",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#naming-objects",
    "href": "introR.html#naming-objects",
    "title": "2  Introduction to R and RStudio",
    "section": "6.2 Naming Objects",
    "text": "6.2 Naming Objects\n\nLine length limit: 80\nAlways use a consistent way of annotating code.\nCamel case is capitalizing the first letter of each word in the object name, with the exception of the first word.\nDot case puts a dot between words in a variable name while camel case capitalizes each word in the variable name.\nObject names appear on the left of assignment operator. We say an object receives or is assigned the value of the expression on the right.\n\n\nNaming Constants: A Constant contains a single numeric value.\n\n\nThe recommended format for constants is starting with a “k” and then using camel case. (e.g., kStates).\n\n\nNaming Functions: Functions are objects that perform a series of R commands to do something in particular.\n\n\nThe recommended format for Functions is to use Camel case with the first letter capitalized. (e.g., MultiplyByTwo).\n\n\nNaming Variables: A Variable is a measured characteristic of some entity.\n\n\nThe recommended format for variables is to use either the dot case or camel case. e.g., filled.script.month or filledScriptMonth.\nA valid variable name consists of letters, numbers, along with the dot or underline characters.\nA variable name must start with a letter, or the dot when not followed by a number.\nA variable cannot contain spaces.\nVariable names are case sensitive: x is different from X just as Age is different from AGE.\nThe value on the right must be a number, string, an expression, or another variable.\nSome Examples Using Variable Rules:\n\n\nAB.1 &lt;- \"Allowed?\"\n# Does not follow rules - not allowed Try the statement below with no\n# hashtag to see the error message .123 &lt;- 'Allowed?'\nA.A123 &lt;- \"Allowed?\"\nG123AB &lt;- \"Allowed?\"\n# Recommended format for constants\nkStates &lt;- 29\n\n\nDifferent R coders have different preferences, but consistency is key in making sure your code is easy to follow and for others to read. In this course, we will generally use the recommendation in the text which are listed above.\nWe tend to use one letter variable names (i.e., x) for placeholders or for simple functions (like naming a vector).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#built-in-functions-setting-an-argument",
    "href": "introR.html#built-in-functions-setting-an-argument",
    "title": "2  Introduction to R and RStudio",
    "section": "7.1 Built-in Functions: Setting an Argument",
    "text": "7.1 Built-in Functions: Setting an Argument\n\nThe standard format to a built-in function is functionName(argument)\n\nFor example, the square root function structure is listed as sqrt(x), where x is a numeric or complex vector or array.\n\n\n\n# Here, we are setting a required argument x to a value of 100. When\n# a value is set, it turns it to a parameter of the function.\nsqrt(x = 100)\n\n[1] 10\n\n# Because there is only one argument and it is required, we can\n# eliminate its name x= from our function call. This is discussed\n# below.\nsqrt(100)\n\n[1] 10\n\n\n\nThere is a little variety in how we can write functions to get the same results.\nA parameter is what a function can take as input. It is a placeholder and hence does not have a concrete value. An argument is a value passed during function invocation.\nThere are some default values set up in R in which arguments have already been set.\nThere are a few functions with no parameters like Sys.time() which produces the date and time. If you are not sure how many parameters a function has, you should look it up in the help.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#default-values",
    "href": "introR.html#default-values",
    "title": "2  Introduction to R and RStudio",
    "section": "7.2 Default Values",
    "text": "7.2 Default Values\n\nThere are many default values set up in R in which arguments have already been set to a particular value or field.\nDefault values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\nWhen only one argument is required, the argument is usually not set to have a default value.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#built-in-functions-using-more-than-one-argument",
    "href": "introR.html#built-in-functions-using-more-than-one-argument",
    "title": "2  Introduction to R and RStudio",
    "section": "7.3 Built-in Functions: Using More than One Argument",
    "text": "7.3 Built-in Functions: Using More than One Argument\n\nFor functions with more than one parameter, we must determine what arguments we want to include, and whether a default value was set and if we want to change it. Default values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\n\nFor example, the default S3 method for the seq() function is listed as the following: seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),length.out = NULL, along.with = NULL, …)\nDefault values have been set on each parameter, but we can change some of them to get a meaningful result.\nFor example, we set the from, to, and by parameter to get a sequence from 0 to 30 in increments of 5.\n\n\n\n# We can use the following code.\nseq(from = 0, to = 30, by = 5)\n\n[1]  0  5 10 15 20 25 30\n\n\n\nWe can simplify this function call even further:\n\nIf we use the same order of parameters as the instructions, we can eliminate the argument= from the function.\nSince we do list the values to the arguments in same order as the function is defined, we can eliminate the from=, to=, and by= to simplify the statement.\n\n\n# Equivalent statement as above\nseq(0, 30, 5)\n\n[1]  0  5 10 15 20 25 30\n\n\nIf you leave off the by parameter, it defaults at 1.\n\n\n# Leaving by= to default value of 1\nseq(0, 30)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\n\n\nThere can be a little hurdle deciding when you need the argument value in the function call. The general rule is that if you don’t know, include it. If it makes more sense to you to include it, include it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#tips-on-arguments",
    "href": "introR.html#tips-on-arguments",
    "title": "2  Introduction to R and RStudio",
    "section": "7.4 Tips on Arguments",
    "text": "7.4 Tips on Arguments\n\nAlways look up a built-in function to see the arguments you can use.\nArguments are always named when you define a function.\nWhen you call a function, you do not have to specify the name of the argument.\nArguments have default values, which is used if you do not specify a value for that argument yourself.\nAn argument list comprises of comma-separated values that contain the various formal arguments.\nDefault arguments are specified as follows: parameter = expression\n\n\ny &lt;- 10:20\nsort(y)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\nsort(y, decreasing = FALSE)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#saving",
    "href": "introR.html#saving",
    "title": "2  Introduction to R and RStudio",
    "section": "2.6 Saving",
    "text": "2.6 Saving\n\nYou can save your work in the file menu or the save shortcut using Ctrl + S or Cmd+S depending on your Operating System.\nYou will routinely be asked to save your workspace image, and you don’t need to save this unless specifically asked. It saves the output we have generated so far.\nYou can stop this from happening by setting the Tools &gt; Global Options &gt; Under Workspace changing this to Never.\nBe careful with this option because it won’t save what you don’t run.\n\n\n\n\nEvalulating Your Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#calling-a-library",
    "href": "introR.html#calling-a-library",
    "title": "2  Introduction to R and RStudio",
    "section": "2.7 Calling a Library",
    "text": "2.7 Calling a Library\n\nIn R, a package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library.\nLibraries need to be activated one time in each new R session.\nYou can access a function from a library one time using library::function()\n\n\n# Use to activate library in an R session.\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\nYou can access a function from a library one time only using library::function()\n\nUseful if only using one function from the library.\n\nWe will return to this in data prep.\n\n\n## Below is an example that would use dplyr for one select function\n## to select variable1 from the oldData and save it as a new object\n## NewData. Since we don’t have datasets yet, we will revisit this.\n## NewData &lt;- dplyr::select(oldData, variable1)\n\n\nSome libraries are part of other global libraries:\n\ndplyr is part of tidyverse, there is actually no need to activate it if tidyverse is active, however, sometimes it helps when conflicts are present\nAn example of a conflict is the use of a select function which shows up in both the dplyr and MASS package. If both libraries are active, R does not know which to use.\ntidyverse has many libraries included in it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#the-environment",
    "href": "introR.html#the-environment",
    "title": "2  Introduction to R and RStudio",
    "section": "2.8 The Environment",
    "text": "2.8 The Environment\n\nYou can evaluate your Environment Tab to see your Variables we have defined in R Studio.\nUse the following functions to view and remove defined variables in your Global Environment\n\n\nls()  #Lists all variables in Global Environment \n\n [1] \"A\"       \"A.A123\"  \"AB.1\"    \"B\"       \"G123AB\"  \"income\"  \"kStates\"\n [8] \"states\"  \"vote\"    \"voted\"   \"x\"       \"y\"      \n\nrm(states)  #Removes variable named states\nrm(list = ls())  #Clears all variables from Global Environment\n\n\n\n\nEvalulating Your RStudio Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#creating-a-vector",
    "href": "introR.html#creating-a-vector",
    "title": "2  Introduction to R and RStudio",
    "section": "8.1 Creating a Vector",
    "text": "8.1 Creating a Vector\n\nA vector is the simplest type of data structure in R.\n\nA vector is a set of data elements that are saved together as the same type.\nWe have many ways to create vectors with some examples below.\n\nUse c() function, which is a generic function which combines its arguments into a vector or list.\n\n\nc(1, 2, 3, 4, 5)  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nIf numbers are aligned, can use the “:“ symbol to include numbers and all in between. This is considered an array.\n\n\n1:5  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nUse seq() function to make a vector given a sequence.\n\n\nseq(from = 0, to = 30, by = 5)  #Creates a sequence vector from 0 to 30 in increments on 5 \n\n[1]  0  5 10 15 20 25 30\n\n\n\nUse rep() function to repeat the elements of a vector.\n\n\nrep(x = 1:3, times = 4)  #Repeat the elements of the vector 4 times\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nrep(x = 1:3, each = 3)  #Repeat the elements of a vector 3 times each\n\n[1] 1 1 1 2 2 2 3 3 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#creating-a-matrix",
    "href": "introR.html#creating-a-matrix",
    "title": "2  Introduction to R and RStudio",
    "section": "8.2 Creating a Matrix",
    "text": "8.2 Creating a Matrix\n\nA matrix is another type of object like a vector or a list.\n\nA matrix has a rectangular format with rows and columns.\nA matrix uses matrix() function\nYou can include the byrow = argument to tell the function whether to fill across or down first.\nYou can also include the dimnames() function in addition to the matrix() to assign names to rows and columns.\n\nUsing matrix() function, we can create a matrix with 3 rows and 3 columns as shown below.\n\nTake note how the matrix fills in the new data.\n\n\n# Creating a Variable X that has 9 Values.\nx &lt;- 1:9\n# Setting the matrix.\nmatrix(x, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Note – we do not need to name the arguments because we go in the\n# correct order.  The function below simplifies the statement and\n# provides the same answer as above.\nmatrix(x, 3, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n8.2.1 Setting More Arguments in a Matrix\n\nThe byrow argument fills the Matrix across the row\nBelow, we can use the byrow statement and assign it to a variable m.\n\n\nm &lt;- matrix(1:9, 3, 3, byrow = TRUE)  #Fills the Matrix Across the Row and assigns it to variable m\nm  #Printing the matrix in the console\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\nThe dimnames() function adds labels to either the row and the column. In this case below both are added to our matrix m.\n\n\ndimnames(x = m) &lt;- list(c(\"2020\", \"2021\", \"2022\"), c(\"low\", \"medium\", \"high\"))\nm  #Printing the matrix in the console\n\n     low medium high\n2020   1      2    3\n2021   4      5    6\n2022   7      8    9\n\n\n\nYou try to make a matrix of 25 items, or a 5 by 5, and fill the matrix across the row and assign the matrix to the name m2.\nYou should get the answer below.\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n[5,]   21   22   23   24   25\n\n\n\n\n8.2.2 Differences between Data Frames and Matrices\n\nIn a data frame the columns contain different types of data, but in a matrix all the elements are the same type of data. A matrix is usually numbers.\nA matrix can be looked at as a vector with additional methods or dimensions, while a data frame is a list.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#creating-a-data-frame",
    "href": "introR.html#creating-a-data-frame",
    "title": "2  Introduction to R and RStudio",
    "section": "8.3 Creating a Data Frame",
    "text": "8.3 Creating a Data Frame\n\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. In a data frame the rows are observations and columns are variables.\n\nData frames are generic data objects to store tabular data.\nThe column names should be non-empty.\nThe row names should be unique.\nThe data stored in a data frame can be of numeric, factor or character type.\nEach column should contain same number of data items.\nCombing vectors into a data frame using the data.frame() function\n\nBelow, we can create vectors for state, year enacted, personal oz limit medical marijuana.\n\n\nstate &lt;- c(\"Alaska\", \"Arizona\", \"Arkansas\")\nyear.legal &lt;- c(1998, 2010, 2016)\nounce.lim &lt;- c(1, 2.5, 3)\n\n\nThen, we can combine the 3 vectors into a data frame and name the data frame pot.legal.\n\n\npot.legal &lt;- data.frame(state, year.legal, ounce.lim)\n\n\nNext, check your global environment to confirm data frame was created.\n\n\n\n\nGlobal Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#importing-a-data-frame-into-r",
    "href": "introR.html#importing-a-data-frame-into-r",
    "title": "2  Introduction to R and RStudio",
    "section": "8.4 Importing a Data Frame into R",
    "text": "8.4 Importing a Data Frame into R\n\nWhen importing data from outside sources, you can do the following:\n\n\nYou can import data from an R package using data() function.\nYou can also link directly to a file on the web.\nYou can import data through from your computer through common file extensions:\n\n.csv: comma separated values;\n.txt: text file;\n.xls or .xlsx: Excel file;\n.sav: SPSS file;\n.sasb7dat: SAS file;\n.xpt: SAS transfer file;\n.dta: Stata file.\n\n\n\nEach different file type requires a unique function to read in the file. With all the variety in file types, it is best to look it up in the R Community to help.\n\n\n8.4.1 Use data() function\n\nAll we need is the data() function to read in a data set that is part of R. R has many built in libraries now, so there are many data sets we can use for testing and learning statistics in R.\n\n\n# The mtcars data set is part of R, so no new package needs to be\n# downloaded.\ndata(\"mtcars\")\n\n\nLoad a data frame from a unique package in R.\n\nThere are also a lot of packages that house data sets. It is fairly easy to make a package that contains data and load it into CRAN. These packages need to be installed into your R one time. Then, each time you open R, you need to reload the library using the library() function.\nWhen your run the install.packages() function, do not include the # symbol. Then, after you run it one time, comment it out. There is no need to run this code a second time unless something happens to your RStudio.\n\n\n# install.packages('MASS') #only need to install package one time in\n# R\nlibrary(MASS)\n\n\n\ndata(\"Insurance\")\nhead(Insurance)\n\n  District  Group   Age Holders Claims\n1        1    &lt;1l   &lt;25     197     38\n2        1    &lt;1l 25-29     264     35\n3        1    &lt;1l 30-35     246     20\n4        1    &lt;1l   &gt;35    1680    156\n5        1 1-1.5l   &lt;25     284     63\n6        1 1-1.5l 25-29     536     84\n\n\n\n\n8.4.2 Accessing Variables\n\nYou can directly access a variable from a dataset using the $ symbol followed by the variable name.\nThe $ symbol facilitates data manipulation operations by allowing easy access to variables for calculations, transformations, or other analyses. For example:\n\n\nhead(Insurance$Claims)  #lists the first 6 Claims in the Insurance dataset.\n\n[1]  38  35  20 156  63  84\n\nsd(Insurance$Claims)  #provides the standard deviation of all Claims in the Insurance dataset.\n\n[1] 71.1624\n\n\n\n\n8.4.3 Setting up a Working Directory\n\nYou should have the data files from our LMS in a data folder on your computer. Your project folder would contain that data folder.\nBefore importing and manipulating data, you must find and edit your working directory to directly connect to your project folder!\nThese functions are good to put at the top of your R files if you have many projects going at the same time.\n\n\ngetwd()  #Alerts you to what folder you are currently set to as your working directory\n# For example, my working directory is set to the following:\n# setwd('C:/Users/Desktop/ProbStat') #Allows you to reset the working\n# directory to something of your choice.\n\n\nIn R, when using the setwd() function, notice the forward slashes instead of backslashes.\nYou can also go to Tools &gt; Global Options &gt; General and reset your default working directory when not in a project. This will pre-select your working directory when you start R.\nOr if in a project, like we should be, you can click the More tab as shown in the Figure below, and set your project folder as your working directory.\n\n\n\n\nSetting Your Working Directory\n\n\n\n\n8.4.4 Reading in Data from .csv\n\nReading in a .csv file is extremely popular way to read in data.\nThere are a few functions to read in .csv files. And these functions would change based on the file type you are importing.\n\n\n8.4.4.1 read.csv() function\n + Extremely popular way to read in data.\n + read.csv() is a base R function that comes built-in with R: No library necessary\n\nAll your datasets should be in a data folder in your working directory so that you and I have the same working directory.\nStructure of function datasetName &lt;- read.csv(“data/dataset.csv”)\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n# or equivalently\ngss.2016 &lt;- read.csv(\"data/gss2016.csv\")\n# Examine the contents of the file\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n# Or equivalently, we can shorten this to the following code\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n\n\n8.4.5 Using tidyverse to load data\n\n8.4.5.1 read_csv function\n\nread_csv() is a function from the readr package, which is part of the tidyverse ecosystem.\nread_csv() is generally faster than read.csv() as it’s optimized for speed, making it more efficient, particularly for large datasets.\n\n\n# install.packages(tidyverse) ## Only need to install one time on\n# your computer. #install.packages links have been commented out\n# during processing of RMarkdown.  Activate the library, which you\n# need to access each time you open R and RStudio\nlibrary(tidyverse)\n\n\n# Now open the data file to evaluate with tidyverse\ngss.2016b &lt;- read_csv(file = \"data/gss2016.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#summarize-data",
    "href": "introR.html#summarize-data",
    "title": "2  Introduction to R and RStudio",
    "section": "8.5 Summarize Data",
    "text": "8.5 Summarize Data\n\nUse the summary() function to examine the contents of the file.\nThe summary() function is part of base R and does not require a package.\n\n\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\nAgain, we can eliminate the object = because it is the first argument and is required.\n\n\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n8.5.1 Explicit Use of Libraries\n\nYou can activate a library one time using library::function() format\nFor example, we can use the summarize() function from dplyr which is part of tidyverse installed earlier.\nSince dplyr is part of tidyverse, there is actually no need to activate it when we have already activated tidyverse in this session, however, it does help when conflicts are present. More on that later.\n\nThe line below says to take the the gss.2016 data object and summarize the length of age using the dplyr library.\n\n\ndplyr::summarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\nIn the line of code above, we see package::function(). If we initiate the library like below, we do not need the beginning of the statement. The code below provides the same answer as the way written above.\n\n\nlibrary(dplyr)\nsummarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\n\nYou try to access the ChickWeight dataset from the MASS package and summarize it generally using summary() and str() functions.\nEarlier, we looked up the tapply() function in the help bar and found that the format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean. Use the tapply() function to take the mean weight of Chicks based on Diet. See the answer below.\nWhile there are a few ways to get group means in R. I like the tapply() function which is used to apply a function over subsets of a vector. The function splits the data into subsets based on a given factor or factors, applies a specified function to each subset, and then returns the results in a convenient form. Here, we are applying the mean function.\nTo access a variable, use dataset$variable or you can also attach a dataset to your code so you can just use the variable name later on. So use ChickWeight$weight and ChickWeight$Diet appropriately in the function alongside the FUN mean.\n\n\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n       1        2        3        4 \n102.6455 122.6167 142.9500 135.2627",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "3  Descriptive Statistics",
    "section": "",
    "text": "3.0.1 Lesson Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#lesson-objectives",
    "href": "descriptives.html#lesson-objectives",
    "title": "3  Descriptive Statistics",
    "section": "",
    "text": "Choose and conduct descriptive analyses for categorical (factor) variables.\nChoose and conduct descriptive analyses for continuous (numeric) variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#consider-while-reading",
    "href": "descriptives.html#consider-while-reading",
    "title": "3  Descriptive Statistics",
    "section": "3.2 Consider While Reading",
    "text": "3.2 Consider While Reading\n\nIn any analysis, it is critically important to understand your data set by evaluating descriptive statistics. For qualitative data, you should know how to calculate frequencies and proportions and make a user-friendly display of results. For quantitative data, you should know what a histogram is and how it is used to describe quantitative data. We also should know how to describe variables by their center and spread of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#defining-and-calculating-central-tendency",
    "href": "descriptives.html#defining-and-calculating-central-tendency",
    "title": "3  Descriptive Statistics",
    "section": "5.1 Defining and Calculating Central Tendency",
    "text": "5.1 Defining and Calculating Central Tendency\n\nThe term central location refers to how numerical data tend to cluster around some middle or central value.\nMeasures of central location attempt to find a typical or central value that describes a variable.\nWhy frequency distributions do not work for numeric variables:\n\nNumeric variables measured on a continuum.\nInstead, we calculate descriptive statistics including central tendency and spread of the values for a numeric variable.\n\nWe will examine the three mostly widely used measures of central location: mean, median and mode.\nThen we discuss a percentile: a measure of relative position.\n\n\n5.1.1 Using the Mean\n\nThe arithmetic mean or simply the mean is a primary measure of central location. It is often referred to as the average. Simply add up all the observations and divide by the number of observations.\nThe numerator (top of the fraction) is the sum (sigma) of all the values of x from the first value (i = 1) to the last value (n) divided by the number of values (n).\n\\(m_x = (\\sum_{i=1}^{n} x_{i})/n\\)\nConsider the salaries of employees at a company: \nWe can use the mean() command to calculate the mean in R.\n\n\n# Create Vector of Salaries\nsalaries &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000)\n# Calculate the mean using the mean() command\nmean(salaries)\n\n[1] 154285.7\n\n\n\nNote that due to at least one outlier this mean does not reflect the typical salary - more on that later.\nIf we edit our vector to include NAs, we have to account for this. This is a common way to handle NAs in functions that do not allow for them.\n\n\nsalaries2 &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000, NA,\n    NA)\n# Calculate the mean using the mean() command Notice that it does not\n# work\nmean(salaries2)\n\n[1] NA\n\n# Add in na.rm parameter to get it to produce the mean with no NAs.\nmean(salaries2, na.rm = TRUE)\n\n[1] 154285.7\n\n\n\nNote that there are other types of means like the weighted mean or the geometric mean.\n\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\), where are the weights associated to the values.\nAn example is below.\n\n\nvalues &lt;- c(4, 7, 10, 5, 6)\nweights &lt;- c(1, 2, 3, 4, 5)\nweighted_mean &lt;- weighted.mean(values, weights)\nweighted_mean\n\n[1] 6.533333\n\n\n\n\n5.1.2 Using the Median\n\nThe median is another measure of central location that is not affected by outliers.\nWhen the data are arranged in ascending order, the median is:\n\nThe middle value if the number of observations is odd, or\nThe average of the two middle values if the number of observations is even.\n\nConsider the sorted salaries of employees presented earlier which contains an odd number of observations.\n\nOn the same salaries vector created above, use median() command to calculate the median in R.\n\n\n# Calculate the median using the median() command\nmedian(salaries)\n\n[1] 90000\n\n\n\nNow compare to the mean and note the large difference in numbers signifying that at least one outlier is most likely present.\nSpecifically, if the mean and median are different, it is likely the variable is skewed and contains outliers.\n\n\nmean(salaries)\n\n[1] 154285.7\n\n\n\nFor another example, consider the sorted data below that contains an even number of values.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\n\n\nWhen data contains an even number of values, the median is the average of the 2 sorted middle numbers (12.56 and 13.47).\n\n\nmedian(GrowthFund)\n\n[1] 13.015\n\n(12.56 + 13.47)/2\n\n[1] 13.015\n\n# The mean is still the average\nmean(GrowthFund)\n\n[1] 10.088\n\n\n\n\n5.1.3 Using the Mode\n\nThe mode is another measure of central location.\nThe mode is the most frequently occurring value in a data set.\nThe mode is useful in summarizing categorical data.\nA data set can have no mode, one mode (unimodal), two modes (bimodal) or many modes (multimodal).\nThe mode is less useful when there are more than three modes.\nThe mode is useful summary for a categorical variable.\n\n\n5.1.3.1 Example of Function with Salary Variable\n\nConsider the salary of employees presented earlier. The mode is $40,000 since this value appears most often.\n\n\n# Try this command with and without it.\nnames(sort(x = table(salaries), decreasing = TRUE))\n\n[1] \"40000\"  \"65000\"  \"90000\"  \"145000\" \"150000\" \"550000\"\n\n\n\n40,000 appears 2 times and is the mode because that occurs most often.\n\n\n\n5.1.3.2 Finding No Mode\n\nLook at the sort(table()) commands with the GrowthFund Vector we made earlier.\nI added a [1:3] at the end of the statement to produce the 3 highest frequencies found in the vector.\n\n\nsort(table(GrowthFund), decreasing = TRUE)[1:3]\n\nGrowthFund\n-38.32   1.71   3.17 \n     1      1      1 \n\n\n\nEven if you use this command, you still need to evaluate the data more systematically to verify the mode. If the highest frequency of the sorted table is 1, then there is no mode.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#spread-to-report-with-the-mean",
    "href": "descriptives.html#spread-to-report-with-the-mean",
    "title": "3  Descriptive Statistics",
    "section": "6.1 Spread to Report with the Mean",
    "text": "6.1 Spread to Report with the Mean\n\n6.1.1 Evaluating Skewness\n\nSkewness is a measure of the extent to which a distribution is skewed.\nCan evaluate skewness visually with histogram.\n\nA histogram is a visual representation of a frequency or a relative frequency distribution.\nBar height represents the respective class frequency (or relative frequency).\nBar width represents the class width.\n\n\n\n\n\nEvaluating Skewness Visually\n\n\n\n\n6.1.2 Skewed Distributions: Median Not Same as Mean\n\nSometimes, a histogram is difficult to tell if skewness is present or if the data is relatively normal or symmetric.\nIf Mean is less than Median and Mode, then the variable is Left-Skewed.\nIf the Mean is greater than the Median and Mode, then the variable is Right-Skewed.\nIf the Mean is about equal to the Median and Mode, then the variable has a symmetric distribution.\nIn R, we can easily look at mean and median with the summary() command.\n\n\n\n\nEvaluating Skewness Using Mean and Median\n\n\n\nMean is great when data are normally distributed (data is not skewed).\nMean is not a good representation of skewed data where outliers are present.\n\nAdding together a set of values that includes a few very large or very small values like those on the far left of a left-skewed distribution or the far right of the right-skewed distribution will result in a large or small total value in the numerator of Equation and therefore the mean will be a large or small value relative to the actual middle of the data.\n\n\n\n\n6.1.3 Using skew() Command in R\n\nThe skew() command is from the semTools package. The install.packages() command is commented out below, but install it one time on your R before commenting it out.\n\n\n# install the semTools package if necessary.\n# install.packages('semTools') Activate the library\nlibrary(semTools)\n\n\nAfter the package is installed and loaded, run the skew() command on the salaries vector made above.\n\n\nskew(salaries)\n\n skew (g1)         se          z          p \n2.31126775 0.92582010 2.49645450 0.01254418 \n\n\n\n\n6.1.4 Interpreting the skew() Command Results\n\nse = standard error\nz = skew/se\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nSalary: Our sample size was small, &lt;50, so the z value of 2.496 in regards to the salary vector indicates there is a problem with skewness.\nGrowthFund: We can check the skew of GrowthFund.\n\n\nskew(GrowthFund)\n\n  skew (g1)          se           z           p \n-1.38071963  0.77459667 -1.78250137  0.07466751 \n\n\n\nGrowthFund was also considered a small sample size, so the same -2/2 thresholds are used. Here, our z value is -1.78250137, which is in normal range. This indicates there is no problem with skewness.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#histograms",
    "href": "descriptives.html#histograms",
    "title": "3  Descriptive Statistics",
    "section": "6.2 Histograms",
    "text": "6.2 Histograms\n\nA histogram is a graphical representation of the distribution of numerical data.\nIt consists of a series of contiguous rectangles, or bars, where the area of each bar corresponds to the frequency of observations within a particular range or bin of values.\nThe x-axis typically represents the range of values being measured, while the y-axis represents the frequency or count of observations falling within each range.\nHistograms are commonly used in statistics and data analysis to visualize the distribution of a dataset and identify patterns or trends.\nThey are particularly useful for understanding the central tendency, variability, and shape of the data distribution - this includes our observation of skewness.\nWorks much better with larger datsets.\n\n\n6.2.1 Commands to Make a Histogram\n\nhist() command in base R.\ngeom_histogram() command in ggplot2 package.\na hist using the GrowthFund dataset does not look that great because its sample size is so small.\n\n\nhist(GrowthFund)\n\n\n\n\n\n\n\n\n\n\n6.2.2 hist vs geom_histogram\n\nIn R, hist() and geom_histogram() are both used to create histograms, but they belong to different packages and have slightly different functionalities.\n\n\n# Making an appropriate data.frame to use the hist() command\nHousePrice &lt;- c(430, 520, 460, 475, 670, 521, 670, 417, 533, 525, 538,\n    370, 530, 525, 430, 330, 575, 555, 521, 350, 399, 560, 440, 425, 669,\n    660, 702, 540, 460, 588, 445, 412, 735, 537, 630, 430)\nHousePrice &lt;- data.frame(HousePrice)\n\n\nhist(): This function is from the base R graphics package and is used to create histograms. It provides a simple way to visualize the distribution of a single variable.\n\n\n# Using base R to create the histogram.\nhist(HousePrice$HousePrice, breaks = 5, main = \"A Histogram\", xlab = \"House Prices (in $1,000s)\",\n    col = \"yellow\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\ngeom_histogram(): This function is from the ggplot2 package, which is part of the tidyverse. It is used to create histograms as part of a more flexible and powerful plotting system.\n\n\n# Using geom_histogram() command to create the histogram.\nggplot(HousePrice, aes(x = HousePrice)) + geom_histogram(binwidth = 100,\n    boundary = 300, color = \"black\", fill = \"yellow\") + labs(title = \"A Histogram\",\n    x = \"House Prices (in $1,000s)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe could add more parameters here to make the 2 histograms look identical, but this configuration of parameters is very close. Take note that there are a lot more parameters you can add to the geom_histogram() command than you can with base R to make it look more professional. Be sure to look them up and also check with the notes in the book, which focuses on geom_histogram instead of hist().\nVariance is a measure of spread for numeric variables that is essentially the average of the squared differences between each observation value on some variable and the mean for that variable with population variance. \\[Population Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2}/N\\] \\[Sample Var(x) = s^2 = \\sum{(x_i-\\bar{x})^2}/(n-1)\\]\nStandard deviation is the square root of the variance.\n\nUse var() command and sd() command to calculate sample variance and sample standard deviation.\n\n\n\n## Calculated from Small Sample\nx &lt;- c(1, 2, 3, 4, 5)\nsum((x - mean(x))^2/(5 - 1))\n\n[1] 2.5\n\nvar(x)\n\n[1] 2.5\n\nsqrt(var(x))\n\n[1] 1.581139\n\nsd(x)\n\n[1] 1.581139\n\nsd(HousePrice$HousePrice)\n\n[1] 102.6059\n\nvar(HousePrice$HousePrice)\n\n[1] 10527.97\n\n\n\nLooking at Spread for a Larger Dataset\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\nsummary(customers$Spending, na.rm = TRUE)  #mean and median\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\n### Spread to Report with the Mean\nsd(customers$Spending, na.rm = TRUE)\n\n[1] 350.2876\n\nvar(customers$Spending, na.rm = TRUE)\n\n[1] 122701.4\n\n\n\n\n6.2.3 Kurtosis in Evaluating Mean Spread\n\nKurtosis is the sharpness of the peak of a frequency-distribution curve or more formally a measure of how many observations are in the tails of a distribution.\nA normal distribution will have a kurtosis value of three;\n\nDistributions with kurtosis around 3 are described as mesokurtic.\nIf kurtosis is significantly above or below 3, there is excess kurtosis.\n\nValues of kurtosis significantly above 3 indicate the distribution is leptokurtic, with fewer observations in the tails than a normal distribution (the fewer observations in the tails often give a distribution a pointy look).\nValues of kurtosis significantly below 3 indicate the distribution is platykurtic, with more observations in the tails than a normal distribution would have given the mean, standard deviation, and sample size.\n\nUses kurtosis() command from the semTools package.\n\n\n\n\n\nEvaluate Kurtosis\n\n\n\nThe kurtosis() command subtracts 3 from the kurtosis, so positive values will be indicative to a leptokurtic distribution and negative will indicate a platykurtic distribution. To see if kurtosis (leptokurtic or platykurtic) is significant, we confirm them by first evaluating the z-score to see if the variable is normal or not. The same cutoff values from skew also apply for the z for small, medium, and large sample sizes in kurtosis. These are the same basic rules for the rules in judging skewness.\n\n\n# z-value is 3.0398, which is &gt; 2 indicating leptokurtic Small sample\n# size: range is -2 to 2\nkurtosis(salaries)\n\nExcess Kur (g2)              se               z               p \n    5.628711065     1.851640200     3.039851407     0.002366949 \n\n# z-value is 2.20528007, which is &gt; 2 indicating leptokurtic Small\n# sample size: range is -2 to 2\nkurtosis(GrowthFund)\n\nExcess Kur (g2)              se               z               p \n     3.41640519      1.54919334      2.20528007      0.02743445 \n\n# Small sample size: range is -2 to 2 Skewness and kurtosis are both\n# in range.\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n0.3173182 0.4082483 0.7772676 0.4370009 \n\nkurtosis(HousePrice$HousePrice)  #normal\n\nExcess Kur (g2)              se               z               p \n     -0.5399982       0.8164966      -0.6613601       0.5083814 \n\n\n\nThe rules of determining problematic distributions with regards to kurtosis are below.\n\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nIf kurtosis is found, then evaluate the excess kur score to see if it is positive or negative to determine whether it is leptokurtic or platykurtic.\n\nLet’s do a few more examples using the customers dataset.\n\n\n# Noted sample size at 200 observations or a medium sample size.\n# Using threshold –3.29 to 3.29 to assess normality.\n\n#-3.4245446445 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$Spending)\n\nExcess Kur (g2)              se               z               p \n  -1.1862970634    0.3464101615   -3.4245446445    0.0006158307 \n\n# Normal: 2.977622119 is in between -3.29 and 3.29\nkurtosis(customers$Income)\n\nExcess Kur (g2)              se               z               p \n    1.031478559     0.346410162     2.977622119     0.002904939 \n\n#-3.7251961028 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$HHSize)\n\nExcess Kur (g2)              se               z               p \n  -1.2904457837    0.3464101615   -3.7251961028    0.0001951634 \n\n# Normal: -0.20056607 is in between -3.29 and 3.29\nkurtosis(customers$Orders)\n\nExcess Kur (g2)              se               z               p \n    -0.06947812      0.34641016     -0.20056607      0.84103789",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#spread-to-report-with-the-median",
    "href": "descriptives.html#spread-to-report-with-the-median",
    "title": "3  Descriptive Statistics",
    "section": "6.3 Spread to Report with the Median",
    "text": "6.3 Spread to Report with the Median\n\nRange = Maximum Value – Minimum Value.\n\nSimplest measure.\nFocuses on Extreme values.\nUse commands diff(range()) or max() – min().\n\nIQR: Difference between the first and third quartiles.\n\nUse IQR() command or quantile() command.\n\n\n\nsummary(customers$Spending, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\ndiff(range(customers$Spending, na.rm = TRUE))\n\n[1] 1200\n\nmax(customers$Spending, na.rm = TRUE) - min(customers$Spending, na.rm = TRUE)\n\n[1] 1200\n\nIQR(customers$Spending, na.rm = TRUE)\n\n[1] 578.5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#spread-to-report-with-the-mode",
    "href": "descriptives.html#spread-to-report-with-the-mode",
    "title": "3  Descriptive Statistics",
    "section": "6.4 Spread to Report with the Mode",
    "text": "6.4 Spread to Report with the Mode\n\nWhile there is no great function to test for spread, you can look at the data and see if it is concentrated around 1 or 2 frequencies. If it is, then the spread is distorted towards those high frequency values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#at-a-glance",
    "href": "dataviz.html#at-a-glance",
    "title": "5  Data Visualization",
    "section": "",
    "text": "In order to succeed in this lesson, we need to learn how to visualize data, noting that there are many ways to do this in R. This lesson presents common graphs used in R, and focuses on using ggplot2, a very common graphics package that is part of the tidyverse.\nBecause packages evolve fairly quickly, parameters within the commands shown in this section could evolve and adjust. The best way to figure out how to fine-tune your new graph is to explore options and parameter settings, while also using the open-source community.\nBy the end of this lesson, you should be able to create the following:\n\n\nCommon Graphs for a Single Categorical Variable\nCommon Graphs for a Single Continuous Variable\nCommon Graphs for Two Variables at Once\n\n\nThis lesson does not provide an exclusive list of what you can do in R, but is a good starting point presenting some of the top graphs used in statistics with some common parameters added in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#lesson-objectives",
    "href": "dataviz.html#lesson-objectives",
    "title": "5  Data Visualization",
    "section": "5.2 Lesson Objectives",
    "text": "5.2 Lesson Objectives\n\nChoose and create graphs for a single categorical variable.\nChoose and create graphs for a single continuous variable.\nChoose and create graphs for two variables at once.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#bar-graph",
    "href": "dataviz.html#bar-graph",
    "title": "5  Data Visualization",
    "section": "7.1 Bar Graph",
    "text": "7.1 Bar Graph\n\nA bar graph depicts the frequency or the relative frequency for each category of the qualitative data as a bar rising vertically from the horizontal axis.\nThis is also known as a bar chart in which your book refers to as a visual display of data often used to examine similarities and differences across categories of things; bars can represent frequencies, percentages, means, or other statistics.\nWe can learn a lot from a bar graph, like the marital status group with the highest and lowest frequencies according to the census.gov.\n\n\n\n\n\n\n\n\n\n\n\n7.1.1 geom_bar()\n\nCreate a Bar Graph using ggplot() Command\nLike histograms, ggplot has many more parameters available over base R to construct bar graphs.\nFirst, we load tidyerse to access ggplot() command and others. You can always do this at the start of all your code to keep all the libraries together that are being used.\n\n\nlibrary(\"tidyverse\")\n\n\nggplot() works in layers, so you will routinely see the + symbol to kick off a new layer with added functionality.\nUsing the ggplot, we always include the aes command first inside the ggplot() command. The aes() command is a quoting function that describes the variables being used. From there, it depends on the plot.\n\nFirst layer: ggplot() and aes() which calls the dataset and variables used.\nSecond layer: Graph type: Bar graph: geom_bar().\nAdditional layers: labs - for labels including titles; themes; and geom_text. Recreate the example below adding one layer at a time to see how the visualization changes.\n\n\n\n## inputting probabilities calculated from a 2023 multiple choice\n## question.  From what you learned about R so far, how do you expect\n## its market share to change?\nGoUp &lt;- 0.54285\nGoDown &lt;- 0.03809\nRemainStable &lt;- 0.34285\nNoOpinion &lt;- 0.07619\n# designing the data frame\ndata_frame &lt;- data.frame(Category = c(\"Go Up\", \"Go Down\", \"Remain Stable\",\n    \"No Opinion\"), Percentage = c(GoUp, GoDown, RemainStable, NoOpinion))\n# Making the graph\nMarketShare &lt;- ggplot(data_frame, aes(x = Category, y = Percentage, fill = Category)) +\n    geom_bar(stat = \"identity\") + labs(title = \"How do you expect R's market share to change?\",\n    x = \"Opinion Category\", y = \"Percentage (%)\") + theme_minimal() + geom_text(aes(label = Percentage),\n    vjust = -0.5, size = 4)\nMarketShare",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#bar-graph-with-data-wrangling",
    "href": "dataviz.html#bar-graph-with-data-wrangling",
    "title": "5  Data Visualization",
    "section": "7.2 Bar Graph with Data Wrangling",
    "text": "7.2 Bar Graph with Data Wrangling\n\nWe can also use a dataset to make a bar graph.\n\nLoad the Gun related data from the nhanes survey.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n\n\nNext, we need to check the import.\n\n\n# Results hidden to save space, but gives you the first 6 records in\n# the data set.\nhead(nhanes)\n\n\nWe can also check the summary of data of the variable of interest, AUQ300, to get a sense of what we are evaluating.\n\n\nsummary(nhanes$AUQ300)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   2.000   1.656   2.000   7.000    4689 \n\n\n\nThe AUQ300 variable represents gun use. A screenshot of the nhanes_auditory_2012_codebook available to you via the book resources is copied below so that we can see what AUQ300 really refers to. This is always a necessary step because variable names can be convoluted and not representative of the variable definition.\n\n\n\n\nAUQ300\n\n\n\n7.2.0.1 Recode Variable if Needed\n\nLook to see if the AUQ300 needs recoding after looking at the codebook and making sense of the variable.\nAUQ300 needs to be a factor variable with 1 equaling a Yes and 2 equaling a No. We can use recode_factor to accomplish 2 things at once with the mutate function.\nrecode_factor() transforms the levels of a categorical variable (factor) into a new set of levels and is specific to categorical variables.\nrecode() is generic and can apply to numerical, categorical, or textual data, but still transforms data from one format or code to another.\n\n\nnhanes.clean &lt;- nhanes %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\"))\n\n\nThen, we need to check the recode for accuracy. You should see the No’s and Yes’s alongside the rest being coded as NA’s.\n\n\nsummary(nhanes.clean$AUQ300)\n\n Yes   No NA's \n1613 3061 4690 \n\n\n\n\n7.2.0.2 Get Bar Roughly Plotted\n\nStart with the basic plot using the ggplot() and geom_bar() commands.\nBelow writes the statement with and without the piping operator.\n\nSince we are also going to use data preparation techniques, the piping operator is recommended.\n\n\n\n# Without piping operator\nggplot(nhanes.clean, aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n# With piping operator\nnhanes.clean %&gt;%\n    ggplot(aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n7.2.0.3 Add Functions to Clean Chart\n\nOmit the NA category from AUQ300 variable representing gun use and plot it again.\nAdd an axis labels under labs(x = …, y=…).\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(x = AUQ300)) + geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\")\n\n\n\n\n\n\n\n##Here, we really benefit from the piping operator because we are doing more than one thing.\n\n\nFrom the bar graph, we can see that almost double the amount of people have not fired a firearm for any reason than those that fired one.\n\n\n\n7.2.0.4 Adding Color\n\nThere are many ways to add color to a bar graph. Below, the color is filled in directly in the aes() command by choosing it to give a different color to each categorical value of AUQ300.\n\nWhen fill is mapped to a variable, the fill color of the geom will vary based on the values of that variable. This is useful for distinguishing different groups or categories within the data.\n\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(AUQ300, fill=AUQ300)) +\n  geom_bar(aes(AUQ300)) +\n  labs(x = \"Gun use\", y = \"Number of participants\", \n       subtitle = \"Filled inside the aes()\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#data-prep-and-then-visualized",
    "href": "dataviz.html#data-prep-and-then-visualized",
    "title": "5  Data Visualization",
    "section": "7.3 Data Prep and then Visualized",
    "text": "7.3 Data Prep and then Visualized\n\nIn the command below, we create a gss.2016.cleaned object to make a barplot. In doing so, we do the following:\nCreate a bar graph using the ggplot() command, which requires an aes() quoting function. This function says that we want to use the grass variable in our bar graph.\nDrop all NAs from the grass variable so that legal and not legal are the only categories.\nWe then create the bars and fill them with 2 colors, red and purple. Many color codes can be used here, and will be discussed in a later chapter.\nWe then add labels to our graph on both x and y axis.\nFinally, we print the new graph, which is saved under the legalize.bar object.\nBelow, I brought back over the code from the last part in Data Preparation. You should still have this in your Chapter1.R file. We are going to use that file to create a graphics in R.\n\n\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(x = grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    mutate(age = as.numeric(x = age)) %&gt;%\n    mutate(age.cat = cut(x = age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n        \"30 - 59\", \"60 - 74\", \"75+\")))\n\n\nOnce the data is prepped, we can graph the variable or variables.\n\n\n# Make a Bar Graph for Grass Variable\nlegalize.bar &lt;- gss.2016.cleaned %&gt;%\n    drop_na(grass) %&gt;%\n    ggplot(aes(x = grass)) + geom_bar(fill = c(\"red\", \"purple\")) + labs(x = \"Should marijuana be legal?\",\n    y = \"Percent of responses\")\nlegalize.bar",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#edit-the-graphic",
    "href": "dataviz.html#edit-the-graphic",
    "title": "5  Data Visualization",
    "section": "7.4 Edit The Graphic",
    "text": "7.4 Edit The Graphic\n\nNext, we can edit these commands to include the age variable. The aes() quoting function has expanded to have the bars filled color using the grass variable, the age category has replaced the grass variable on the x axis, and there is a new a formula on the y axis to sum and count.\nWe also gave this a theme and updated the labels.\n\n\nlegalize.bar &lt;- gss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, y = 100 * (after_stat(count))/sum(after_stat(count)),\n        fill = grass)) + geom_bar(position = \"dodge\") + theme_minimal() +\n    labs(x = \"Age Category\", y = \"Percent of responses\")\n\nlegalize.bar\n\n\n\n\n\n\n\n\n\nEvaluate these 2 graphs and see what information you can get from them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#pie-chart",
    "href": "dataviz.html#pie-chart",
    "title": "5  Data Visualization",
    "section": "7.5 Pie Chart",
    "text": "7.5 Pie Chart\n\nA pie chart is a segmented circle whose segments portray the relative frequencies of the categories of a qualitative variable.\nSlices of pie in different colors represent the parts.\nIn this example, the firearm is divided by type to show parts of a whole, where the total of the proportions must add to 1.0 and the total of the percentages must add to 100%.\n\n\n# Importing data from working directory\nfbi.deaths &lt;- read.csv(\"data/fbi_deaths.csv\", stringsAsFactors = TRUE)\n# Selecting rows of interest for pie chart\nfbi.deaths.small &lt;- fbi.deaths[c(3, 4, 5, 6, 7), ]\nfbi.deaths.small &lt;- fbi.deaths.small %&gt;%\n    rename(Weapon = X)\n# Checking summary of fbi deaths\nsummary(fbi.deaths.small)\n\n                       Weapon      X2012          X2013          X2014     \n Firearms, type not stated:1   Min.   : 116   Min.   : 123   Min.   :  93  \n Handguns                 :1   1st Qu.: 298   1st Qu.: 285   1st Qu.: 258  \n Other guns               :1   Median : 310   Median : 308   Median : 264  \n Rifles                   :1   Mean   :1779   Mean   :1691   Mean   :1662  \n Shotguns                 :1   3rd Qu.:1769   3rd Qu.:1956   3rd Qu.:2024  \n Asphyxiation             :0   Max.   :6404   Max.   :5782   Max.   :5673  \n (Other)                  :0                                               \n     X2015          X2016     \n Min.   : 177   Min.   : 186  \n 1st Qu.: 258   1st Qu.: 262  \n Median : 272   Median : 374  \n Mean   :1956   Mean   :2201  \n 3rd Qu.:2502   3rd Qu.:3077  \n Max.   :6569   Max.   :7105  \n                              \n\n\n\nAgain, ggplot works in layers, so in order to make a pie, you need a few layers and have a few optional ones.\n\nThe aes() command specifies the variable to create the pie, in this case x2016.\ngeom_col() sets the borders of the pie and makes it visible.\ncoord_polar() command makes the pie circular.\ntheme_void() command is optional and adjusts the theme of the pie. to remove axis, background, etc.\n\n\n\nggplot(fbi.deaths.small, aes(x=\"\", y=X2016, fill=Weapon)) +\n  geom_col() + \n  coord_polar(\"y\", start=0) + \n  theme_void() \n\n\n\n\n\n\n\n\n\nFrom the pie, we can see that the majority of weapons that caused fbi gun related deaths are handguns followed by a type of firearm that is not stated.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#comparison-of-charts",
    "href": "dataviz.html#comparison-of-charts",
    "title": "5  Data Visualization",
    "section": "7.6 Comparison of Charts",
    "text": "7.6 Comparison of Charts\n\nRecommended graphs for single categorical or factor type variable:\n\nBar graph, for showing relative group sizes.\nPie charts are available in R but are not recommended because they tend to be less clear for comparing group sizes.\n\nPie charts are difficult to read since the relative size of pie pieces is often hard to determine.\nPie charts take up a lot of space to convey little information.\nPeople often use fancy formatting like 3D, which takes up more space and makes understanding relative size of pie pieces even more difficult.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#histograms",
    "href": "dataviz.html#histograms",
    "title": "5  Data Visualization",
    "section": "8.1 Histograms",
    "text": "8.1 Histograms\n\nA histogram is a useful plot to determine central tendency and spread.\nWe went over histograms in Lesson 2, so refer back for information on how to create a histogram using base R and ggplot.\nRemember that you can tell the distribution from a histogram, and that distribution can be normal or skewed (Right or Left).\n\nWith each chart based on quantitative data, you should be able to get a sense of the distribution.\nThe histogram below looks right skewed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#density-plots",
    "href": "dataviz.html#density-plots",
    "title": "5  Data Visualization",
    "section": "8.2 Density Plots",
    "text": "8.2 Density Plots\n\nA density plot is similar to a histogram but more fluid in appearance because it does not have the separate bins.\n\nProbability density is not very useful for interpreting what is happening at any given value of the variable on the x-axis, but it is useful in computing the percentage of values that are within a range along the x-axis.\nThe area under the curve in a density plot could be interpreted as the probability of a single observation or a range of observations.\nWe can use random normal data to create the density plot like shown below with a sample of 1000, a mean of 10 and a standard deviation of 2. To do this, we need to make the vector and assign it to a data frame.\nIn R, set.seed() is a function used to set the seed for random number generation. By setting a seed using set.seed(), you ensure reproducibility of your code. If you run the same code with the same seed, you’ll get the same sequence of random numbers every time. This is particularly useful for debugging, testing, or when you want to ensure that your results are reproducible.\nWe use set.seed before any function with a random normal generator to ensure reproducibility.\nIf a dataset is provided, then you do not need to generate your own random data as shown in the step below.\n\n\nset.seed(1)\nx &lt;- rnorm(1000, mean = 10, sd = 2)\ndf &lt;- data.frame(x)\n\n\nNext, we can make the density plot using the ggplot2 package under tidyverse.\n\nLayer 2 includes the geom_density() command in addition to the standard Layer 1 ggplot() command to create the density plot.\n\n\n\nggplot(df, aes(x)) + geom_density()\n\n\n\n\n\n\n\n\n\nThere are a lot of arguments you can change. I selected a couple below. Be sure to look at the help file on the geom_density() layer to get the variety on what you can do.\n\ncolor = sets a line color\nlwd = makes the line thicker. Increase this number for thicker line.\nfill= colors the area under the curve.\nalpha= sets the transparency to the area under the curve.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", lwd = 3)\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x)) +\n  geom_density(color = \"darkblue\", \n    #fill = Fills color under the curve\n    fill = \"lightblue\",\n    #alpha = Sets a transparency to the area under the curve. \n    #.5 for 50% transparency. \n    alpha = .5) \n\n\n\n\n\n\n\n\n\nWe can even add a mean line, which we know in this case is 10 because we used random normal data with that mean set as a parameter.\ngeom_vline() is a function used to add vertical lines to a plot created with ggplot. This function is useful for visually indicating specific points or ranges on the x-axis.\nYou can do a line break in your R code after a comma (\\(,\\)) or after a plus sign (\\(+\\)). I find things easier to read on less lines, but it is personal preference how many lines you use given still following the rules in R.\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", fill = \"lightblue\",\n    alpha = 0.5) + geom_vline(aes(xintercept = mean(x)), color = \"red\",\n    linetype = \"dashed\", lwd = 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#boxplot",
    "href": "dataviz.html#boxplot",
    "title": "5  Data Visualization",
    "section": "8.3 Boxplot",
    "text": "8.3 Boxplot\n\nA boxplot is a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups.\nBoxplots are often used to compare the distribution of a continuous variable across several groups.\nA box plot allows you to:\n\nGraphically display the distribution of a data set.\nCompare two or more distributions.\nIdentify outliers in a data set.\n\n\n\n\n\nA Boxplot with Outliers on Left\n\n\n\nBoxplots include the following information:\n\nA line representing the median value.\nA box containing the middle 50% of values.\nWhiskers extending to 1.5 times the IQR.\nOutliers more than 1.5 times the IQR away from the median.\n\n\n\n\n\nA Boxplot with No Outliers\n\n\n\nThis boxplot above displays 5 summary values:\n\nS = smallest value.\nL = largest value.\nQ1 = first quantile = 25th percentile.\nQ2 = median = second quantile = 50th percentile.\nQ3 = third quantile = 75th percentile.\n\nFor example, use the GrowthFund Vector from the last lesson. It is executed again below.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\nGrowthFund &lt;- as.data.frame(GrowthFund)\n\n\nThe quantile() function returns the five point summary when no arguments are specified.\n\n\nQuanData &lt;- quantile(GrowthFund$GrowthFund)\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#detecting-outliers",
    "href": "dataviz.html#detecting-outliers",
    "title": "5  Data Visualization",
    "section": "8.4 Detecting Outliers",
    "text": "8.4 Detecting Outliers\n\nWe see an outlier visually, but without the tool available, we can detect them through the statistics. First we calculate the IQR, which is just quarter 3 minus quarter 1.\n\n\nsummary(GrowthFund$GrowthFund)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-38.320   3.875  13.015  10.088  16.942  36.290 \n\nIQRvalue &lt;- 16.9425 - 3.875\nIQRvalue\n\n[1] 13.0675\n\nIQRvalue &lt;- IQR(GrowthFund$GrowthFund)\nIQRvalue\n\n[1] 13.0675\n\n\n\nThen, multiply the IQR by 1.5.\n\n\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 19.60125\n\n\n\nFinally conduct 2 checks to determine if outliers are past the low whisker and/or high whisker.\n\nA TRUE value indicates that at least one outlier is present at the small end of the distribution.\nA FALSE value indicates that no outliers are at the high end of the distribution.\n\n\n\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n 25% \nTRUE \n\n# True indicating an outlier to the left\n3.875 - -38.32  #42.195\n\n[1] 42.195\n\n42.195 &gt; 19.60125  #TRUE\n\n[1] TRUE\n\nQuanData[4] - QuanData[5] &gt; OutlierValue\n\n  75% \nFALSE \n\n# False indicating no outlier to the right.\n16.9425 - 36.29  #-19.3475\n\n[1] -19.3475\n\n-19.3475 &gt; 19.60125  #FALSE \n\n[1] FALSE\n\n\n\n8.4.1 Boxplot\n\nWe can use ggplot to retrieve our graph and associated numbers.\nThe outlier is visually depicted on the graph as -38.32.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe can add a little color to the plot with the fill parameter, but then we also need to be sure to turn off the legends in the geom_boxplot.\n\n\nggplot(GrowthFund, aes(GrowthFund, fill = \"red\")) + geom_boxplot(show.legend = FALSE)\n\n\n\n\n\n\n\n\n\nYou can add parameters to make this visualization more professional, but this gets you started. Be sure to look at some examples in the R community or on ChatGPT.\n\n\n\n8.4.2 mtcars Example\n\nWe can also view mpg from the mtcars data set as a boxplot.\nIn this example, there is one outlier to the right.\n\n\ndata(\"mtcars\")\nggplot(mtcars, aes(mpg)) + geom_boxplot(color=\"#AAAAAA\", fill=\"#AAA3F3\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nLet’s get some summary statistics to check for outliers, skewness, and kurtosis to see how our visual aids help us in understanding those results.\n\n\nIQRvalue &lt;- IQR(mtcars$mpg)\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 11.0625\n\nQuanData &lt;- quantile(mtcars$mpg)\nQuanData\n\n    0%    25%    50%    75%   100% \n10.400 15.425 19.200 22.800 33.900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n  25% \nFALSE \n\n# False indicating no outlier to the left\nQuanData[4] - QuanData[5] &gt; OutlierValue\n\n  75% \nFALSE \n\n# False indicating no outlier to the right.\nsemTools::skew(mtcars$mpg)\n\nskew (g1)        se         z         p \n0.6723771 0.4330127 1.5527885 0.1204737 \n\nsemTools::kurtosis(mtcars$mpg)\n\nExcess Kur (g2)              se               z               p \n    -0.02200629      0.86602540     -0.02541068      0.97972740 \n\n\n\nLooks like the mpg variable is quite normal with no signs of outliers, skewness, or kurtosis. In the boxplot, it looks like there might be an outlier to the right, but it is not problematic to the dataset as indicated by the manual calculations.\nWe can also get a histogram of mpg and are able to make the same claims towards normality. You can see a slight pull to the right, but it is seemingly normal.\n\n\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 5, color = \"black\",\n    fill = \"green\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#bar-graphs-for-two-categorical-variables",
    "href": "dataviz.html#bar-graphs-for-two-categorical-variables",
    "title": "5  Data Visualization",
    "section": "9.1 Bar Graphs for Two Categorical Variables",
    "text": "9.1 Bar Graphs for Two Categorical Variables\n\nThere are two formats available for bar charts:\n\nGrouped\nStacked\n\n\n\n\n# A tibble: 6 × 3\n# Groups:   vs, gear [6]\n     vs  gear     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     0     3    12\n2     0     4     2\n3     0     5     4\n4     1     3     3\n5     1     4    10\n6     1     5     1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.1 Grouped Bar Graph\n\nGrouped bar graph allow comparison of multiple sets of data items, with a single color used to denote a specific series across all sets.\nFor example, we can look at both the vs and gear variables in the ggplot command.\n\nWe did a little grouping and counting before we began to generate a new table with frequencies based on vs and gear variables.\nOnce the dataset was made, we can make the graph with the geom_bar layer.\nSince there are two variables, we need to set the position to dodge.\n(stat = “identity”) tells ggplot that the y values are already computed and should be used as-is for the heights of the bars. In this case, they are frequencies calculated in the countsDF dataset.\n\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(vs = as.factor(vs)) %&gt;%\n    mutate(gear = as.factor(gear))\n\n\ncountsDF &lt;- mtcars %&gt;%\n    group_by(vs, gear) %&gt;%\n    count()\n\nsummary(countsDF)\n\n vs    gear        n         \n 0:3   3:2   Min.   : 1.000  \n 1:3   4:2   1st Qu.: 2.250  \n       5:2   Median : 3.500  \n             Mean   : 5.333  \n             3rd Qu.: 8.500  \n             Max.   :12.000  \n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\") + labs(title = \"Grouped Car Distribution by Gears and VS\",\n    x = \"Number of Gears\", y = \"Count\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n9.1.2 Stacked Bar Graph\n\nA Stacked bar graph extends the standard bar graph from looking at numeric values across one categorical variable to two. Each bar in a standard bar graph is divided into a number of sub-bars stacked end to end, each one corresponding to a level of the second categorical variable.\nUsing ggplot, we can also stack these charts by removing the position = dodge statement.\n\n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Stacked Car Distribution\",\n       x = \"Number of Gears\",\n       y = \"Count\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#bar-graph-for-continuous-across-groups",
    "href": "dataviz.html#bar-graph-for-continuous-across-groups",
    "title": "5  Data Visualization",
    "section": "9.2 Bar Graph for Continuous Across Groups",
    "text": "9.2 Bar Graph for Continuous Across Groups\n\nIn comparison to a bar graph for a single categorical variable, a bar chart for a continuous variable across groups includes both a x and y axis. The continuous variable is put on the y axis, and the categorical (factor) variable is placed on the x axis showing the groups.\nTherefore, instead of counting data based on group, we can see another continuous variable based on group data.\nThe frequency data (i.e., counts) can be replaced with another numerical variable like mean.\nIn the below example, instead of counting observations per group, here, we took the average mpg (a continuous variable) based on groups of gear and vs and summarized the data into a variable avg_mpg. We then used that variable in a ggplot() command to create a unique chart to that above.\n\n\navg_mpg &lt;- mtcars %&gt;%\n    group_by(gear, vs) %&gt;%\n    summarise(mpg = mean(mpg, na.rm = TRUE))\n\n\nggplot(avg_mpg, aes(gear,\n  mpg, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  ggtitle(\"Average MPG by VS and Gear\")\n\n\n\n\n\n\n\n\n\nWe can add color using the scale_fill_manual. Since there are two colors, we use the c() command to combine them together inside the layer.\n\nscale_fill_manual() is a function used to manually set the colors for filled areas (like bars, areas in geom_area, etc.) in a plot. Here, values argument takes our custom color palette of yellow and brown.\n\n\n\nggplot(avg_mpg, aes(gear, mpg, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\", color = \"black\") + ggtitle(\"Average MPG by VS and Gear\") +\n    scale_fill_manual(values = c(\"yellow\", \"brown\"))\n\n\n\n\n\n\n\n\n\n9.2.1 Boxplot for Continuous Across Groups\n\nA boxplot requires one continuous variable (like we did above). When we include an additional grouping variable, we get multiple boxplots, one for each group. This allows us to directly compare distributions.\nThe categorical variable should correctly be a factor data type before you begin.\nIn the example below, mpg is the continuous variable, and gear is the categorical variable.\nWe see 3 boxplots for three values of gear (3, 4, 5). ggplot() and geom_boxplot() are required components of the command. The scale_fill_manual() and theme_minimal() layers are optional ways to change the style and color.\n\n\nmtcars %&gt;%\n  ggplot(aes(x = gear, y = mpg)) +\n  geom_boxplot(aes(fill = gear)) +\n  scale_fill_manual(values = c(\"gray\", \"red\", \"blue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLets alter the functions above to depict mpg based on vs with categorical states 0 and 1.\n\n\nmtcars %&gt;%\n    ggplot(aes(x = vs, y = mpg)) + geom_boxplot(aes(fill = vs), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"gray\", \"red\")) + theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-two-continuous-variables",
    "href": "dataviz.html#graphs-for-two-continuous-variables",
    "title": "5  Data Visualization",
    "section": "9.3 Graphs for Two Continuous Variables",
    "text": "9.3 Graphs for Two Continuous Variables\n\n9.3.1 Scatterplots\n\nA scatterplot is used to determine if two continuous variables are related.\n\nEach point is a pairing: \\((x_1, y_1),(x_2, y_2),\\) etc.\n\nOur goal with a scatterplot is to characterize the relationship by visual inspection. This includes determining if the relationship looks positive, negative, or not existent.\n\n\n\n\nScatterPlot Results\n\n\n\nSometimes, it is really clear how to characterize the relationship. Other times, additional statistical tests are needed to confirm the relationship (which we will go over in later lessons). This is true especially with big data, where the plot window can look like a giant blog of observations.\nLet’s work a clean example examining the relationship between income and the years of education one has had.\nThis plot has a clear positive trend, meaning that as one has more years of education, we see higher income. And similarly, as we see higher income, we also see more years of education. This means that a scatter can help characterize the relationship, and does not state that one variable is causing another to occur.\n\n\nEdu &lt;- read.csv(\"data/education.csv\")\nplot(Edu$Income ~ Edu$Education, ylab = \"Income\", xlab = \"Education\")\n\n\n\n\n\n\n\n\n\nWorking with ggplot instead of base R, we would use the following code.\n\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs (for labels) as shown below.\n\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\")\n\n\n\n\n\n\n\n\n\ngeom_point() has some parameters you can change like shape = where you can add depth to your chart. Some common shapes of geom_point are as follows.\n\n\n# shape = 0, square shape = 1, circle shape = 2, triangle point up\n# shape = 3, plus shape = 4, cross shape = 5, diamond shape = 6,\n# triangle point down shape = 7, square cross shape = 8, star shape =\n# 9, diamond plus shape = 10, circle plus shape = 11, triangles up\n# and down shape = 12, square plus shape = 13, circle cross shape =\n# 14, square and triangle down shape = 15, filled square shape = 16,\n# filled circle shape = 17, filled triangle point-up shape = 18,\n# filled diamond\n\n\nFor instance, the code below changes the color, shape, and size of the geom_point().\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 10) + labs(y = \"Income\", x = \"Education\")\n\n\n\n\n\n\n\n\n\nggplot allows us to add a geom_line, which is helpful in drawing a line through the data. Here, I am also resetting the color off of the default value. You see this a lot on time series models like stock charts.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_line(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nAnother way to do this is to add a stat_smooth line, which is considered a trendline to help us visualize the relationship between the variables.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + stat_smooth(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nWe can change the type of trendline. The most common is the to develop the trendline using the lm method, which stands for the linear method that we are going to learn in Chapter 9. For now, lets insert _method=“lm” into our stat_smooth later to see the change.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\") + stat_smooth(method = \"lm\", color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nLet’s look at a few more examples and see if the relationship is considered positive, negative, or not existent.\nBelow, we see a negative trend.\n\n\nggplot(mtcars, aes(x = disp, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to hp, below, we see another negative trend.\n\n\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to qsec, below, we see a weak positive trend. This relationship would need to verified later on.\n\n\nggplot(mtcars, aes(x = qsec, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nThe plot() command also works when you do not have 2 continuous variables, and instead have one categorical variable paired with one continuous variable. However, the plot is not as adequate as others are in inferring the relationship from the variables.\nFor example, the plot below would be better served as a boxplot.\n\n\nplot(mtcars$mpg ~ mtcars$vs)\nboxplot(mtcars$mpg ~ mtcars$vs)\n\n\n\n\n\n\n\n\n\nUsing ggplot, we would have the same issue. Since vs is categorical, it does not look right when we use the geom_point() later.\n\n\nggplot(mtcars, aes(x = vs, y = mpg)) + geom_point() + stat_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nInstead, we would want the geom_boxplot layer like shown below. The stat_smooth() is also not an applicable layer to a boxplot and would need to be removed.\n\n\nggplot(mtcars, aes(x = vs, y = mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n9.3.2 Try to Recreate\n\nYou try examples of scatterplots using the UScrime data set that is part of the MASS package to examine a few relationships using ggplot2. A few examples are below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what information you can take away from the scatterplots above and create some more to practice.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "6  Probability and Probability Distributions",
    "section": "",
    "text": "6.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#at-a-glance",
    "href": "probability.html#at-a-glance",
    "title": "6  Probability and Probability Distributions",
    "section": "",
    "text": "In order to succeed in this lesson, you will need to learn the basic rules behind random events including how to calculate probability based on distribution. We focus on learning these skills with regards to the binomial distribution and the normal distribution, and in doing so move from learning about descriptive statistic to learning about inferential statistics. We will also learn the limitations of having a variable that is not normal, and how to transform it to be normal so that we can calculate probability using the same algorithms. In your homework assignment for this chapter, you will put these skills into practice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#lesson-objectives",
    "href": "probability.html#lesson-objectives",
    "title": "6  Probability and Probability Distributions",
    "section": "6.2 Lesson Objectives",
    "text": "6.2 Lesson Objectives\n\nDefine and use probability distributions to infer from a sample.\nCompute and interpret z-scores to compare observations to groups.\nEstimate population means from sample means using the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#consider-while-reading",
    "href": "probability.html#consider-while-reading",
    "title": "6  Probability and Probability Distributions",
    "section": "6.3 Consider While Reading",
    "text": "6.3 Consider While Reading\n\nAs you engage with the lesson, pay attention to the key concepts and how those concepts are applied. There are rules for calculating and combining probabilities that are important for you to know to solve the problem sets and to solve real-life problems. Take note of the applicability of the normal distribution, which is a major cornerstone for statistical analysis.\nIn this lesson, we move from simply describing data to making inferences. There are so many ways to calculate probability. The assigned readings focus on helping you learn and understand two of the most common distributions: the binomial distribution and the normal distribution. In doing so, we learn about random variables, sampling, and the importance of setting the seed. We also find z-scores, which will turn out to be very important for our future studies. Next, we focus on the utility of the transformation section, which allows us to use the rules and practices regarding a normal distribution, assuming that at least one transformation was successful into reshaping the quantitative variable into a normally-shaped distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "6  Probability and Probability Distributions",
    "section": "8.1 Random Variables",
    "text": "8.1 Random Variables\n\nA Random Variable is a function that assigns numerical values to the outcomes of a random experiment.\nDenoted by uppercase letters (e.g., \\(X\\) ).\nCorresponding values of the random variable: \\(x_1,x_2, x_3,...\\)\nRandom variables may be classified as:\n\nDiscrete - The random variable assumes a countable number of distinct values.\n\n\nDiscrete probability distributions show probabilities for variables that can only have certain values, which includes categorical variables and variables that must be measured in whole numbers like number of people texting during class.\nThe Binomial Distribution is a discrete distribution that evaluates the probability of a “yes” or “no” outcome occurring over a given number of trials\n\n\nContinuous - The random variable is characterized by (infinitely) uncountable values within any interval.\n\n\nContinuous probability distributions show probabilities for values, or ranges of values, of a continuous variable that can take any value in some range.\nThe Normal Distribution is a continuous distribution and is the most important of all probability distributions. Its graph is bell-shaped and this bell-shaped curve is used in almost all disciplines.\n\nFor example, consider an experiment in which two shirts are selected from the production line and each is either defective (D) or non-defective (N).\n\nSince only 2 shirts are selected, here is the sample space, which are all the available options: \\({(D,D), (D,N), (N,D), (N,N)}\\)\nThe random variable X is the number of defective shirts.\nThe possible number of defective shirts is the set \\(X={0,1,2}\\),\nSince these are the only countable number of possible outcomes, this is a discrete random variable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#useful-commands-for-random-variables",
    "href": "probability.html#useful-commands-for-random-variables",
    "title": "6  Probability and Probability Distributions",
    "section": "8.2 Useful Commands for Random Variables",
    "text": "8.2 Useful Commands for Random Variables\n\nset.seed() command is useful when conducting random sampling since it will result in the same sample to be taken each time the code is run, which makes sampling more reproducible.\n\nWe briefly looked at this when making our density plot with random normal data using the rnorm() command.\n\nsample_n() command can be used to take a sample. The arguments for sample_n() are size = which is where to put the size of the sample to take and replace = which is where you choose whether or not you want R to sample with replacement (replacing each value into the population after selection, so that it could be selected again) or without replacement (leaving a value out of the sampling after selection).\nLet’s look at an example using the pdmp_2017.csv file.\n\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\nBelow, I am using the read.csv() command to read in the data set and use strings as factors = TRUE to ensure character variables are coerced as factors. This helps bypass the need to coerce later on. We do need to note that it will change all string variables to factors with the TRUE parameter.\n\n\nopioidpolicy  &lt;- read.csv(\"data/pdmp_2017.csv\", stringsAsFactors = TRUE) \n# Set a starting value for sampling\nset.seed(3)\n# Sample 25 states and save as sample and check summary\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nYou should have the same answers as I do above (8 No’s, and 17 Yes’s) because we set the same seed. If you don’t, I mention the reason below at the end of this short sampling experiment.\n\n\n# Sample another 25 states and check summary.\n# Note the different answer than above. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 14  11 \n\n\n\n# Sample another 25 states and check summary\n# Again, note the differences in numbers each time. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 12  13 \n\n\n\n# Sample another 25 states and check summary using same set seed as our first run (3). \nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nAgain, you should have the same numbers as I do above, and these numbers should be equivalent to our first run (8 No’s, and 17 Yes’s). If you don’t have the same numbers as me is possible that your random number generator is on a different setting. Post R version 3.6.0 or later, we should be on Rejection sample.kind. The next line sets the RNGkind().\n\n\nRNGkind(sample.kind = \"Rejection\")\n# Run the same code again as above for replication results\nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#summary-measures-for-a-random-variable",
    "href": "probability.html#summary-measures-for-a-random-variable",
    "title": "6  Probability and Probability Distributions",
    "section": "8.3 Summary Measures for a Random Variable",
    "text": "8.3 Summary Measures for a Random Variable\n\n8.3.1 Expected Value\n\nWe can calculate the expected value, or value we think is going to occur based on the type of distribution.\n\nExpected value is also known as the population mean \\(\\mu\\), and is the weighted average of all possible values of \\(X\\).\nMore specifically, \\(E(X)\\) is the long-run average value of the random variable over infinitely many independent repetitions of an experiment.\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the expected value of \\(X\\) is the probability weighted average of the values. In the case of one random variable, that means: \\(E(X) = \\mu = \\sum{x_iP(X=x_i)}\\)\n\n\n\n\n8.3.2 Variance\n\nVariance of a random variable is the average of the squared differences from the mean.\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the variance is defined as: \\(Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2*P(X=x_i)}\\)\n\n\n\n\n8.3.3 Standard Deviation\n\nStandard deviation is consistently the square root of the variance. \\(SD(X) = \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#summary-measures-for-a-binomial-random-variable",
    "href": "probability.html#summary-measures-for-a-binomial-random-variable",
    "title": "6  Probability and Probability Distributions",
    "section": "10.1 Summary Measures for a Binomial Random Variable",
    "text": "10.1 Summary Measures for a Binomial Random Variable\n\nExpected value, variance, and standard deviation were introduced and defined in section 2. We can take derivatives of the formulas to simplify our calculations given knowing a \\(n\\) and \\(p\\).\nThe formula for the expected value of a binomial random variable expands from \\(\\sum{x_iP(X=x_i)}\\) to \\(= n*p\\).\nThe variance of a binomial random variable expands from \\(\\sum{(x_i-\\mu)^2*P(X=x_i)}\\) to \\(= n*p*(1-p)\\)\nThe standard deviation a binomial random variable expands from \\(\\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\) to \\(= \\sqrt{np*(1-p)}\\)\nExample summary statistics of a binomial random variable\n\nA current WM student has a career free-throw percentage of 89.4%. Suppose he shoots six free throws in tonight’s game. What is the expected number of free throws that he will make?\n\n\n\nex &lt;- 6 * 0.894\nex\n\n[1] 5.364\n\nvarx &lt;- 6 * 0.894 * (1 - 0.894)\nvarx\n\n[1] 0.568584\n\nsdx &lt;- sqrt(varx)\nsdx\n\n[1] 0.7540451\n\n\n\nIf the student shoots 6 free throws and typically makes 89.4% of them, we can multiply those two values together for the expected value, 5.364. We also find that this data has a variance of .568584 and standard deviation of .754051.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-probability-mass-function",
    "href": "probability.html#the-probability-mass-function",
    "title": "6  Probability and Probability Distributions",
    "section": "10.2 The Probability Mass Function",
    "text": "10.2 The Probability Mass Function\n\nThe Probability Mass Function for a discrete random variable X is a list of the values of X with the associated probabilities, that is, the list of all possible pairs: \\((X, P(X=x))\\)\n\nA probability mass function computes the probability that an exact number of successes happens for a discrete random variable, given \\(n\\) and \\(p\\) defined above.\nDistribution of probabilities of different numbers of successes.\nA probability mass function is used to describe discrete random variables in a binomial distribution.\nEvery random variable is associated with a probability distribution that describes the variable completely.\nUses dbinom() command to calculate in R.\n\nExample using the probability mass function: Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that none of the workers is afraid that they will never be able to retire?\nAgain, we can use the dbinom() command to calculate this in R given x = none or 0, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n# P(X=0)\ndbinom(0, 10, 0.2)\n\n[1] 0.1073742\n\n\n\nThe answer suggests that there is a .107 or 10.737% chance that no workers think they won’t be able to retire.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#cumulative-distribution-function",
    "href": "probability.html#cumulative-distribution-function",
    "title": "6  Probability and Probability Distributions",
    "section": "10.3 Cumulative Distribution Function",
    "text": "10.3 Cumulative Distribution Function\n\nAnother way to look at a probability distribution is to examine its cumulative probability distribution. Here, you can determine the probability of getting some range of values, which is often more useful than finding the probability of one specific number of successes.\nA cumulative distribution function may be used to describe either discrete or continuous random variables.\n\nThe cumulative distribution function for X is defined as: \\(P(X&lt;=x)\\)\nThe less than and equal to sign is the standard way to look at the cumulative distribution function. You can calculate &gt;, &gt;=, &lt; from \\(P(X&lt;=x)\\) given the two rules of probability discussed above.\n\n\\(0 &lt;= P(X=x) &lt;= 1\\)\nThe sum of the probabilities of all possible values of a variable is 1\n\nUses pbinom() command to calculate in R with the default value of lower.tail = TRUE is for n or fewer successes\nCan change lower.tail parameter to lower.tail = FALSE is computing higher than n rather than n or higher.\n\nExample using the cumulative distribution function, Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that less than 3 of the workers are afraid that they will never be able to retire?\nWe can use the pbinom() command to calculate this in R given q = less than 3 or &lt;=2, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n# P(X&lt;3) or P(X&lt;=2)\npbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\nOr likewise, we could use multiple dbinom() commands to get individual probabilities and add them up. This statement is much longer, but does give you the same answer. Examine the figure below to see why.\n\n\ndbinom(0, 10, 0.2) + dbinom(1, 10, 0.2) + dbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\n\n\nVisual of Binomial Distribution Example",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#variations-in-binom-command",
    "href": "probability.html#variations-in-binom-command",
    "title": "6  Probability and Probability Distributions",
    "section": "10.4 Variations in binom() Command",
    "text": "10.4 Variations in binom() Command\n\nIn order to find \\(P(X = 70)\\) given 100 trials and .68 probability of success, we enter:\n\n\n# P(X = 70)\ndbinom(70, 100, 0.68)\n\n[1] 0.07907911\n\n\n\nIn order to find \\(P(X &lt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt;= 70)\npbinom(70, 100, 0.68)\n\n[1] 0.7006736\n\n\n\nIn order to find \\(P(X &lt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt; 70)\npbinom(69, 100, 0.68)\n\n[1] 0.6215945\n\n\n\nIn order to find \\(P(X &gt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt; 70)\npbinom(70, 100, 0.68, lower.tail = FALSE)\n\n[1] 0.2993264\n\n\n\nIn order to find \\(P(X &gt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt;= 70)\npbinom(69, 100, 0.68, lower.tail = FALSE)  #Or\n\n[1] 0.3784055\n\n1 - pbinom(69, 100, 0.68)\n\n[1] 0.3784055\n\n\n\nExamples of binomial distribution calculations in a word problems: A current WM student has a career free-throw percentage of 90.3%. Suppose he shoots five free throws in tonight’s game. What is the probability that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903)\n\n[1] 0.6003973\n\n\n\nWhat is the percentage that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903) * 100  #60.04%\n\n[1] 60.03973\n\n\n\nA current WM student has a career free-throw percentage of 80.5%. Suppose he shoots six free throws in tonight’s game. What is the probability that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE)\n\n[1] 0.6676464\n\n# Or\ndbinom(5, 6, 0.805) + dbinom(6, 6, 0.805)\n\n[1] 0.6676464\n\n\n\nWhat is the percentage that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE) * 100  # 66.76%\n\n[1] 66.76464\n\n\n\nThirty five percent of consumers with credit cards carry balances from month to month. Six consumers with credit cards are randomly selected. What is the probability that fewer than two consumers carry a credit card balance?\n\n\n# P(X&lt;=2)\ndbinom(0, 6, 0.35) + dbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n# Or\npbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n\n\nWhat is the percentage that fewer than two consumers carry a credit card balance?\n\n\npbinom(1, 6, 0.35) * 100  #31.9%\n\n[1] 31.90799",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#density-functions-for-continuous-distributions",
    "href": "probability.html#density-functions-for-continuous-distributions",
    "title": "6  Probability and Probability Distributions",
    "section": "11.1 Density Functions for Continuous Distributions",
    "text": "11.1 Density Functions for Continuous Distributions\n\n11.1.1 Probability Density Function\n\nThe Probability Density Function is used to describe continuous random variables.\nProbability Density Function \\(f(x)\\) of a continuous random variable X describes the relative likelihood that \\(X\\) assumes a value within a given interval (e.g., \\(P(a&lt;=X&lt;=b)\\), where \\(f(x)&gt;=0\\) for all possible values of \\(X\\) and the area under \\(f(x)\\) over all values of \\(x\\) equals one.\n\n\n\n11.1.2 Cumulative Density Function\n\nThe Cumulative Density Function \\(F(x)\\) of a continuous random variable X suggests that for any value x of the random variable X, the cumulative distribution function \\(F(x)\\) is computed as, \\(F(x) = P(X &lt;= x)\\) as a result, \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nThe goal of cumulative distributions is to find the area under the curve.\n\nUses the pnorm() command.\nThree arguments of the command:\n\n\nq is the value of interest;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.\nWe can also work backwards to find a value given a probability.\n\nUses the qnorm() command.\nThree arguments of the command:\n\n\np is the cumulative probability;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-normal-distribution",
    "href": "probability.html#the-normal-distribution",
    "title": "6  Probability and Probability Distributions",
    "section": "11.2 The Normal Distribution",
    "text": "11.2 The Normal Distribution\n\nThe normal distribution serves as the cornerstone of statistical inference.\nData is symmetric about its mean.\nMean=Median=Mode.\nThe distribution is bell-shaped.\nThe distribution is asymptotic, which means that the tails get closer and closer to the horizontal axis, but never touch it.\nClosely approximates the probability distribution of a wide range of random variables, such as the following:\n\nHeights and weights of newborn babies.\nScores on SAT.\nCumulative debt of college graduates.\n\nThe normal distribution is completely described by two parameters: population mean \\(\\mu\\), which describes the central location of the distribution,and population variance \\(\\sigma^2\\), which describes the dispersion of the distribution.\n\n\n\n\nNormal Distribution\n\n\n\nA special case of the normal distribution:\n\nMean is equal to zero (E(Z) = 0).\nStandard deviation is equal to one (SD(Z) = 1).\n\n\n\n# Solving the figure above using the pnorm() command in R.  P(X&lt;=0)\npnorm(q = 0, mean = 0, sd = 1)\n\n[1] 0.5\n\n# Or the following because the default value of the mean and sd are 0\n# and 1.\npnorm(0)\n\n[1] 0.5\n\n\n\nIf we assume a mean of 0 and a standard deviation of 1, and we are looking for the probability when the mean is 0, we will get a .5 probability or 50 percent (.5*100). This is because the curve is normal with identical sides.\nWe can also solve this backwards. Below is the qnorm() command, which solves the figure above looking at probability instead of values.\n\n\n# P(Z &gt;= z) = .5\nqnorm(0.5, 0, 1)\n\n[1] 0\n\n# Or\nqnorm(0.5)\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#empirical-rule",
    "href": "probability.html#empirical-rule",
    "title": "6  Probability and Probability Distributions",
    "section": "11.3 Empirical Rule",
    "text": "11.3 Empirical Rule\n\nChevyshev’s Theorem states that at least \\(1 - 1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.\nWith a normal distribution, we can assume approximately the following under the empirical rule:\n\n68% of values within one SD of the mean;\n95% of values within two SD of the mean;\n99.7% of values within three SD of the mean.\n\n\n\n\n\nEmpirical Rule",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#calculating-z-scores",
    "href": "probability.html#calculating-z-scores",
    "title": "6  Probability and Probability Distributions",
    "section": "11.4 Calculating z-scores",
    "text": "11.4 Calculating z-scores\n\nA z-score allows description and comparison of where an observation falls compared to the other observations for a normally distributed variable.\nA z-score is calculated as the number of standard deviations an observation is away from the mean.\nA normally distributed variable can be used to create z-scores.\nThe \\(x_i\\) represents the value of variable \\(x\\) for a single observation, \\(\\mu_x\\) is the mean of the \\(x\\) variable, \\(\\sigma_x\\) is the standard deviation of the \\(x\\) variable. So, \\(z_i\\) is the difference between the observation value and the mean value for a variable and is converted by the denominator into standard deviations. The final z-score for an observation is the number of standard deviations it is from the mean. \\[z_i = (x_i - \\mu_x)/\\sigma_x\\].\nA z score or z value specifies by how many standard deviations the corresponding x value falls above (z &gt; 0) or below (z &lt; 0) the mean.\n\nA positive z indicates by how many standard deviations the corresponding x lies above mean.\nA zero z indicates that the corresponding x equals mean.\nA negative z indicates by how many standard deviations the corresponding x lies below mean.\n\nExample of a z-score calculation: Scores on a management aptitude exam are normally distributed with a mean of 72 and a standard deviation of 8.\n\nWhat is the probability that a randomly selected manager will score above 60?\nFirst, we could transform the random variable X to Z score using the transformation formula:\n\n\n\n(60 - 72)/8\n\n[1] -1.5\n\n\n\nThen, you can calculate the probability using the standard normal distribution, which has a mean of 0 and a standard deviation of 1.\n\n\n# P(Z &gt; -1.5)\npnorm(-1.5, 0, 1, lower.tail = FALSE)\n\n[1] 0.9331928\n\n# Or similarly, we could use the line below because of the default\n# values associated with the pnorm() command\npnorm(-1.5, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nAlso, because R handles the standard normal transformation on our behalf with its inclusion of parameters, we can use the pnorm() command with the mean and standard deviation provided above to calculate the probability in less steps.\n\n\n# Note the same answer as above.  P(X &gt; 60)\npnorm(60, 72, 8, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nTo answer the question, there is a 0.933 probability or a 93.3% chance that a randomly selected manager will score above a 60 on the managerial aptitude exam.\nIn order to get the percentage in R, we simply multiply the answer by 100.\n\n\npnorm(60, 72, 8, lower.tail = FALSE) * 100\n\n[1] 93.31928",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#finding-utility-in-calculating-probability",
    "href": "probability.html#finding-utility-in-calculating-probability",
    "title": "6  Probability and Probability Distributions",
    "section": "11.5 Finding Utility in Calculating Probability",
    "text": "11.5 Finding Utility in Calculating Probability\n\nExample using pnorm() with Word Problems: Suppose the life of a particular brand of laptop battery is normally distributed with a mean of 6 hours and a standard deviation of 0.9 hours. Use R to calculate the probability that the battery will last more than 8 hours before running out of power and document that probability below.\n\n\n# P(X &gt; 8)\npnorm(8, 6, 0.9, lower.tail = FALSE)\n\n[1] 0.01313415\n\n\n\nThe time for a professor to grade a student’s homework in business statistics is normally distributed with a mean of 15 minutes and a standard deviation of 3.5 minutes. What is the probability that randomly selected homework will require less than 16 minutes to grade?\n\n\n# P(X &lt; 16)\npnorm(16, 15, 3.5)\n\n[1] 0.6124515\n\n\n\nWhat percentage of randomly selected homeworks will require less than 16 minutes to grade?\n\n\npnorm(16, 15, 3.5) * 100\n\n[1] 61.24515",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#finding-probability-between-two-values",
    "href": "probability.html#finding-probability-between-two-values",
    "title": "6  Probability and Probability Distributions",
    "section": "11.6 Finding Probability Between Two Values",
    "text": "11.6 Finding Probability Between Two Values\n\nWe mentioned above that in order to find probability between 2 values a and b, we could use the following equation: \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nTo use this formula, find P(−1.52 &lt;= Z &lt;= 1.96). This would equal P(Z&lt;=1.96)−P(Z&lt;=−1.52) given a standard normal random variable Z we get the commands below.\n\n\n# P(Z &lt;= 1.96) - P(Z &lt;= -1.52)\npnorm(1.96, 0, 1) - pnorm(-1.52, 0, 1)\n\n[1] 0.9107466",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#finding-value-given-a-probability",
    "href": "probability.html#finding-value-given-a-probability",
    "title": "6  Probability and Probability Distributions",
    "section": "11.7 Finding Value Given a Probability",
    "text": "11.7 Finding Value Given a Probability\n\nWe use R’s pnorm() and qnorm() commands to solve problems associated with the normal distribution.\nIf we want to find a particular x value for a given cumulative probability (p), then we enter “qnorm(p, μ, σ)”.\n\n\n# P(X &gt; x) = 0.10\nqnorm(0.9, 7.49, 6.41)\n\n[1] 15.70475\n\n# or\nqnorm(0.1, 7.49, 6.41, lower.tail = FALSE)\n\n[1] 15.70475",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#finding-utility-in-calculating-values-from-probability",
    "href": "probability.html#finding-utility-in-calculating-values-from-probability",
    "title": "6  Probability and Probability Distributions",
    "section": "11.8 Finding Utility in Calculating Values from Probability",
    "text": "11.8 Finding Utility in Calculating Values from Probability\n\nExample of qnorm() with Word Problems: The stock price of a particular asset has a mean and standard deviation of $58.50 and $8.25, respectively. What is the 95th percentile of this stock price?\n\n\n# P(X&lt;=x) =.95\nqnorm(0.95, 58.5, 8.25)\n\n[1] 72.07004\n\n\n\nThe salary of teachers in a particular school district is normally distributed with a mean of $50,000 and a standard deviation of $2,500. Due to budget limitations, it has been decided that the teachers who are in the top 2.5% of the salaries would not get a raise. What is the salary level that divides the teachers into one group that gets a raise and one that does not?\n\n\n# P(X&gt;=x) =.025\nqnorm(0.025, 50000, 2500, lower.tail = FALSE)\n\n[1] 54899.91\n\n\n\nYou are planning a May camping trip to a National Park in Alaska and want to make sure your sleeping bag is warm enough. The average low temperature in the park for May follows a normal distribution with a mean of 32°F and a standard deviation of 8°F. Above what temperature must the sleeping bag be suited such that the temperature will be too cold only 5% of the time?\n\n\n# P(X&lt;=x) =.05\nqnorm(0.05, 32, 8)\n\n[1] 18.84117",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#examples-using-proportions",
    "href": "probability.html#examples-using-proportions",
    "title": "6  Probability and Probability Distributions",
    "section": "14.1 Examples using Proportions",
    "text": "14.1 Examples using Proportions\n\nAnne wants to determine if the marketing campaign has had a lingering effect on the proportion of customers who are women and teenage girls. Before the campaign, p = 0.43 for women and p = 0.21 for teenage girls. Based on 50 women and 50 teenage girls sampled after the campaign, p = 0.46 and p = 0.34, respectively.\nTo calculate the probability that the marketing campaign had a lingering effect on women, we use \\(P(\\hat{P} &gt;= .46)\\).\n\n\npnorm(0.46, 0.43, sqrt(0.43 * (1 - 0.43)/50), lower.tail = FALSE)\n\n[1] 0.3341494\n\n\n\nTo calculate the probability that the marketing campaign had a lingering effect on teenage girls, we use \\(P(\\hat{P} &gt;= .34)\\).\n\n\npnorm(0.34, 0.21, sqrt(0.21 * (1 - 0.21)/50), lower.tail = FALSE)\n\n[1] 0.01200832\n\n\n\nThe marketing campaign had a much stronger effect on women (.334), with basically zero effect on teenage girls (.012).\nThe labor force participation rate is the number of people in the labor force divided by the number of people in the country who are of working age and not institutionalized. The BLS reported in February 2012 that the labor force participation rate in the United States was 63.7%. A marketing company asks 120 working-age people if they either have a job or are looking for a job, or, in other words, whether they are in the labor force. What is the probability that between 60% and 62.5% of those surveyed are members of the labor force?\nHere we find \\(P(\\hat{P} &lt;= .625)-P(\\hat{P} &lt;= .6)\\).\n\n\npnorm(0.625, 0.637, sqrt(0.637 * (1 - 0.637)/120)) - pnorm(0.6, 0.637,\n    sqrt(0.637 * (1 - 0.637)/120))\n\n[1] 0.192639\n\n\n\nThere is a 19.26% chance that 60% to 62.5% of those surveyed are members of the labor force.\nSometimes it is helpful to assign variables so that you can use the consistent functions. The example below does that.\n\n\n## Between .625 and .6 - given sample size of 120 and a p of .637\np &lt;- 0.637\nn &lt;- 120\nphat1 &lt;- 0.625\nphat2 &lt;- 0.6\nQ1 &lt;- pnorm(phat1, p, sqrt(p * (1 - p)/n)) - pnorm(phat2, p, sqrt(p * (1 -\n    p)/n))\nQ1  #0.192639\n\n[1] 0.192639",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#transformation-second-example",
    "href": "probability.html#transformation-second-example",
    "title": "6  Probability and Probability Distributions",
    "section": "15.1 Transformation Second Example",
    "text": "15.1 Transformation Second Example\n\nTaking a second example, let us look at the PHYSHLTH variable from the gender dataset (brfss.csv). We worked with this dataset in an earlier lesson. In doing so, we cleaned the data.\nI copied over that data preparation code in regards to the variable of interest (PHYSHLTH), and tidied it up for one example. To remind ourselves, the question being asked was the following, “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?”\nIf ever you are using the MASS package and dplyr, the select function may have a conflict where R does not know which to use. If you get an error when using select, add dplyr:: in front of the statement to ensure you are using select from dplyr to select variables.\n\n\n#\ngender &lt;- read.csv(\"data/brfss.csv\")\n# Review the data\nsummary(gender)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n# PHYSHLTH example\ngender.clean &lt;- gender %&gt;%\n    dplyr::select(PHYSHLTH) %&gt;%\n    drop_na() %&gt;%\n    # Turn the 77 values to NA, since 77 meant don't know or not sure\n    # from the brss codebook\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA, since 99 meant Refuled from the brss\n    # codebook.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to 0 - since the number 88 meant 0 days of\n    # illness from the brss codebook.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\ntable(gender.clean$PHYSHLTH)\n\n\n     0      1      2      3      4      5      6      7      8      9     10 \n291696  19505  24890  14713   7644  12931   2140   8049   1478    325   9437 \n    11     12     13     14     15     16     17     18     19     20     21 \n   133    908     92   4558   8638    221    153    279     51   5554   1111 \n    22     23     24     25     26     27     28     29     30 \n   132     80     98   2270    149    204    831    390  35701 \n\nsummary(gender.clean)\n\n    PHYSHLTH     \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 0.000  \n Mean   : 4.224  \n 3rd Qu.: 3.000  \n Max.   :30.000  \n NA's   :10299   \n\n\n\nOnce here, we graph PHYSHLTH.\n\n\ngender.clean %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Number of Days Sick\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe determined from Chapter 2 that this variable had severe skewness (positive). Most people had 0 days of illness.\nNext, we run the 4 calculations by mutating the variable and saving all 4 transformation under new variable names.\n\n\ngenderTransform &lt;- gender.clean %&gt;%\n    mutate(phy.cube.root = PHYSHLTH^(1/3)) %&gt;%\n    mutate(phy.log = log(x = PHYSHLTH)) %&gt;%\n    mutate(phy.inverse = 1/PHYSHLTH) %&gt;%\n    mutate(phy.sqrt = sqrt(x = PHYSHLTH))\n\n\nNext, we create the 4 graphs for each of the 4 transformations labelled above to see if one helps.\n\n\ncuberoot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.cube.root)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Cube root\", y = \"\")\nlogged &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.log)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Log\", y = \"\")\ninversed &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.inverse)) + xlim(0, 1) + geom_histogram(fill = \"#7463AC\",\n    color = \"white\", binwidth = 0.05) + theme_minimal() + labs(x = \"Inverse\",\n    y = \"\")\nsquareroot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.sqrt)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 1) + theme_minimal() + labs(x = \"Square root\", y = \"\")\n\n\nFinally, we plot the graphs using gridExtra so that we can see all 4.\n\n\ngridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)\n\n\n\n\n\n\n\n\n\nIn this example, NOT ONE transformation helped. If this happens, something else would need to occur before correctly using the variable. Examples could be to run a non-linear model, or categorizing the data into bins, especially since there was a large frequency of people that were not ill.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "contingency.html",
    "href": "contingency.html",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "",
    "text": "7.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#at-a-glance",
    "href": "contingency.html#at-a-glance",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "",
    "text": "In order to succeed in this section, you will need to learn how to create and calculate probability inside a contingency table, when we have two categorical variables, and conduct our first hypothesis test, the chi-squared test of independence. In your homework assignment for this chapter, you will put these skills into practice.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#lesson-objectives",
    "href": "contingency.html#lesson-objectives",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.2 Lesson Objectives",
    "text": "7.2 Lesson Objectives\n\nCompute probability regarding two categorical variables inside contingencies tables.\nCalculate and interpret the chi-squared statistic for the test of independence.\nCalculate and interpret the chi-squared statistic for the goodness of fit test.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#consider-while-reading",
    "href": "contingency.html#consider-while-reading",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.3 Consider While Reading",
    "text": "7.3 Consider While Reading\n\nIn this lesson, we move from simply describing data to making inferences. We learn about contingency tables in regards to calculating probability and are able to calculate and interpret the Chi-squared statistic for the test of independence.\nAs you read and listen to the materials for this lesson, pay attention to the concepts, and pay attention to how those concepts are applied. There are rules for calculating and combining probabilities that are important for you to know to solve the problem sets and to solve real-life problems. Take note of the applicability of the normal distribution, which is a major cornerstone for statistical analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#probability-rules",
    "href": "contingency.html#probability-rules",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "8.1 Probability Rules",
    "text": "8.1 Probability Rules\n\nThe addition rule refers to the probability that event A or B occurs, or that at least one of these events occurs, is the following:\n\\[P(A \\cup B) = (P(A) + P(B) - P(A \\cap B)\\]\nThe complement rule refers to the probability of the complement of an event, \\(P(A^c)\\) is equal to one minus the probability of the event.\n\\[P(A^c) = 1-P(A)\\]\nConditional probability refers to the probability of an event given that another event has already occurred.\n\nGiven two events A and B, each with a positive probability of occurring, the probability that A occurs given that B has occurred (A conditioned on B) is equals to \\(P(A|B) = (P(A \\cap B)/P(B)\\). Similarly, the probability that B occurs given that A has occurred (B conditioned on A) is equal to \\(P(B|A) = (P(B \\cap A)/P(A)\\).\n\nMarginal probability refers to the probability of an event occurring \\((P(A))\\), it may be thought of as an unconditional probability. It is not conditioned on another event.\nThe joint probability rule is determined by dividing each cell frequency by the grand total. Joint probability refers to the statistical measure that calculates the likelihood of two events occurring together and at the same point in time.\n\\[P(A \\cap B)\\]\n\nFor example, the probability that a randomly selected person is under 35 years of age and makes an Under Armour purchase is \\(P(U35 \\cap UA)=174/600= .29\\)\n\nFor example, this Venn Diagram illustrates the sample space for events A and B.\nThe union of two events (\\(A \\cup B\\)) is the event consisting of all simple events in A or B.\n\n The intersection of two events (\\(A \\cap B\\)) consists of all simple events in both A and B.\n\n\n\nVenn Diagram: Intersection and Union\n\n\n\nThe complement of event A (i.e., \\(A^c\\)) is the event consisting of all simple events in the sample space S that are not in A.\n\n\n\n\nVenn Diagram: Complement Rule",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#summary-measures-for-a-random-variable",
    "href": "contingency.html#summary-measures-for-a-random-variable",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "8.2 Summary Measures for a Random Variable",
    "text": "8.2 Summary Measures for a Random Variable\n\nThe Summary Measures of a Random Variable are consistent with our discussion of Probability in the last lesson. They are copied back here for ease of reference.\n\n\n8.2.1 Expected Value\n\nWe can calculate the expected value, or value we think is going to occur based on the type of distribution.\n\nExpected value is also known as the population mean \\(\\mu\\), and is the weighted average of all possible values of \\(X\\).\nMore specifically, \\(E(X)\\) is the long-run average value of the random variable over infinitely many independent repetitions of an experiment.\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the expected value of \\(X\\) is the probability weighted average of the values. In the case of one random variable, that means: \\[E(X) = \\mu = \\sum{x_iP(X=x_i)} = n*p\\]\n\n\n\n\n8.2.2 Variance\n\nVariance of a random variable is the average of the squared differences from the mean.\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the variance is defined as: \\[Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2*P(X=x_i)} = n*p*(1-p)\\]\n\n\n\n\n8.2.3 Standard Deviation\n\nStandard deviation is consistently the square root of the variance. \\[SD(X) = \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}= \\sqrt{np*(1-p)}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#calculating-probability",
    "href": "contingency.html#calculating-probability",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "8.3 Calculating Probability",
    "text": "8.3 Calculating Probability\n\nWe could calculate the statistics in this contingency table a couple of ways. First, we could manually calculate it like below.\n\n\n## Calculate Totals\nTotalUnder35 &lt;- 174 + 132 + 90\nTotalUnder35  ##Total under 35 age group\n\n[1] 396\n\nTotal35up &lt;- 54 + 72 + 78\nTotal35up  ## Total 35 and older\n\n[1] 204\n\nTotalUA &lt;- 174 + 54\nTotalUA  ## Total Under Armour\n\n[1] 228\n\nTotalNike &lt;- 132 + 72\nTotalNike  ## Total Nike\n\n[1] 204\n\nTotalAdidas &lt;- 90 + 78\nTotalAdidas  ## Total Adidas\n\n[1] 168\n\nTotal &lt;- 174 + 132 + 90 + 54 + 72 + 78\nTotal  ##Grand Total \n\n[1] 600\n\n\n\nAlternatively, we could make this a matrix and use the matrix command to help us calculate the answer. This gives us all the statistics with a couple of functions and allows us to interpret from there after selecting the right information from the output.\n\n\n# Put observed values in a vector by row.\nx &lt;- c(174, 132, 90, 54, 72, 78)\n# Turn the vector into a matrix. In this case, we had 2 rows, 3\n# columns, and entered the data by row.\no &lt;- matrix(x, 2, 3, byrow = TRUE)\no\n\n     [,1] [,2] [,3]\n[1,]  174  132   90\n[2,]   54   72   78\n\n# Next, we could add the names of the columns and rows values using\n# the dimnames() command.\ndimnames(o) &lt;- list(c(\"Under 35\", \"35 and Older\"), c(\"Under Armour\", \"Nike\",\n    \"Adidas\"))\no\n\n             Under Armour Nike Adidas\nUnder 35              174  132     90\n35 and Older           54   72     78\n\n# We can easily calculate the total from here.\ntotal &lt;- sum(o)\ntotal\n\n[1] 600\n\nrowsums &lt;- margin.table(o, 1)\nrowsums\n\n    Under 35 35 and Older \n         396          204 \n\ncolsums &lt;- margin.table(o, 2)\ncolsums\n\nUnder Armour         Nike       Adidas \n         228          204          168 \n\nprob &lt;- prop.table(o)\nprob  # each cell represents a joint probability\n\n             Under Armour Nike Adidas\nUnder 35             0.29 0.22   0.15\n35 and Older         0.09 0.12   0.13\n\nrowprob &lt;- margin.table(prob, 1)\nrowprob  #each value represents marginal probability\n\n    Under 35 35 and Older \n        0.66         0.34 \n\ncolprob &lt;- margin.table(prob, 2)\ncolprob  #each value represents marginal probability\n\nUnder Armour         Nike       Adidas \n        0.38         0.34         0.28 \n\n\n\nTake note that when subsetting from the matrix we created above, the dimnames do not serve as a column or row. so the first number is in row 1, column 1. This is distinct from data.frames, which typically have the 1st column designated for the labels.\n\n\nprob[1, 1]  #corresponds to the joint probability value associated with Row 1 (Under 35), Col 1 (Under Armour)\n\n[1] 0.29\n\n\n\nOnce we do our calculations our totals, we can ask probability questions.\nWhen looking up an answer to the question, it makes it easier to reference the tables to ensure you are subsetting correctly.\n\n\no\n\n             Under Armour Nike Adidas\nUnder 35              174  132     90\n35 and Older           54   72     78\n\nprob\n\n             Under Armour Nike Adidas\nUnder 35             0.29 0.22   0.15\n35 and Older         0.09 0.12   0.13\n\n\n\nIf a person is under 35, what is the probability they will select Under Armour? \\[P(UA|under35) = (P(UA \\cap under35)/P(under35))\\]\n\n\n# This is a conditional probability question.\n174/396  #or\n\n[1] 0.4393939\n\nprob[1, 1]/rowprob[1]\n\n Under 35 \n0.4393939 \n\n\n\nWhat is the probability of a person 35 or older randomly selecting Nike? \\[P(N|older35) = (P(N \\cap older35)/P(older35))\\]\n\n\n# This is a conditional probability question.\n72/204  # or\n\n[1] 0.3529412\n\nprob[2, 2]/rowprob[2]\n\n35 and Older \n   0.3529412 \n\n\n\nIf the chance the preference was the same regardless of condition, how many people would you expect to select Nike? \\[Ex(Nike)\\]\n\n\n# This is a expected value question.\nTotalNike  # or\n\n[1] 204\n\ncolsums[2]\n\nNike \n 204 \n\n\n\nWe can calculate summary measures based on a variable category.\n\n\n# E(X) of 35 Years or older (formula n*p)\nn &lt;- total\nn  #600\n\n[1] 600\n\np &lt;- rowprob[2]\np  #.34\n\n35 and Older \n        0.34 \n\nn * p  #Or simply the frequency for the column 35 and older\n\n35 and Older \n         204 \n\n# Var(X) (formula n*p*(1-p))\nvarx &lt;- n * p * (1 - p)\nvarx  #134.64\n\n35 and Older \n      134.64 \n\n# SD(X) (formula sqrt of variance)\nsdx &lt;- sqrt(varx)\nsdx  #11.60345 \n\n35 and Older \n    11.60345",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#larger-contingency-table",
    "href": "contingency.html#larger-contingency-table",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "8.4 Larger Contingency Table",
    "text": "8.4 Larger Contingency Table\n\nThe survey question asked participants in multiple groups the following question, “How likely are you to participate in the event in the future?”\nIn order to answer this question, first, lets bring in some data and calculate totals.\n\n\n\n\nContingency Table Numbers\n\n\n\nx &lt;- c(197, 388, 230, 103, 137, 98, 20, 18, 18, 13, 58, 45)\no &lt;- matrix(x, 4, 3, byrow = TRUE)\no\n\n     [,1] [,2] [,3]\n[1,]  197  388  230\n[2,]  103  137   98\n[3,]   20   18   18\n[4,]   13   58   45\n\ndimnames(o) &lt;- list(c(\"Students\", \"Faculty/Staff\", \"Alumni\", \"Town Residents\"),\n    c(\"Unlikely\", \"Moderately Likely\", \"Very Likely\"))\no\n\n               Unlikely Moderately Likely Very Likely\nStudents            197               388         230\nFaculty/Staff       103               137          98\nAlumni               20                18          18\nTown Residents       13                58          45\n\ntotal &lt;- sum(o)\ntotal\n\n[1] 1325\n\nrowsums &lt;- margin.table(o, 1)\nrowsums\n\n      Students  Faculty/Staff         Alumni Town Residents \n           815            338             56            116 \n\ncolsums &lt;- margin.table(o, 2)\ncolsums\n\n         Unlikely Moderately Likely       Very Likely \n              333               601               391 \n\n# Lets add the grand total to the colsums\ncolsums &lt;- c(colsums, total)\ncolsums\n\n         Unlikely Moderately Likely       Very Likely                   \n              333               601               391              1325 \n\n\n\nIn this example, we can bind the data together so that it is a little easier to see. Examine the final table and compare that to the figure above.\n\n\ntotals &lt;- cbind(o, rowsums)\ntotals\n\n               Unlikely Moderately Likely Very Likely rowsums\nStudents            197               388         230     815\nFaculty/Staff       103               137          98     338\nAlumni               20                18          18      56\nTown Residents       13                58          45     116\n\ntotals &lt;- rbind(totals, colsums)\ntotals\n\n               Unlikely Moderately Likely Very Likely rowsums\nStudents            197               388         230     815\nFaculty/Staff       103               137          98     338\nAlumni               20                18          18      56\nTown Residents       13                58          45     116\ncolsums             333               601         391    1325\n\n\n\n\n\nContingency Table Proportions\n\n\n\nNext, let’s calculate probabilities.\n\n\nprob &lt;- prop.table(o)\nprob\n\n                  Unlikely Moderately Likely Very Likely\nStudents       0.148679245        0.29283019  0.17358491\nFaculty/Staff  0.077735849        0.10339623  0.07396226\nAlumni         0.015094340        0.01358491  0.01358491\nTown Residents 0.009811321        0.04377358  0.03396226\n\nTotalRowProb &lt;- margin.table(prob, 1)\nTotalRowProb\n\n      Students  Faculty/Staff         Alumni Town Residents \n    0.61509434     0.25509434     0.04226415     0.08754717 \n\nTotalColProb &lt;- margin.table(prob, 2)\nTotalColProb\n\n         Unlikely Moderately Likely       Very Likely \n        0.2513208         0.4535849         0.2950943 \n\nTotalColProb &lt;- c(TotalColProb, sum(TotalColProb))\nTotalColProb\n\n         Unlikely Moderately Likely       Very Likely                   \n        0.2513208         0.4535849         0.2950943         1.0000000 \n\n\n\nIn this example, we can also bind the data together so that it is a little easier to see. Again, we are trying to get our R code in the best shape to be able to select the right information to calculate probability from it.\n\n\nproportions &lt;- cbind(prob, TotalRowProb)\nproportions\n\n                  Unlikely Moderately Likely Very Likely TotalRowProb\nStudents       0.148679245        0.29283019  0.17358491   0.61509434\nFaculty/Staff  0.077735849        0.10339623  0.07396226   0.25509434\nAlumni         0.015094340        0.01358491  0.01358491   0.04226415\nTown Residents 0.009811321        0.04377358  0.03396226   0.08754717\n\nproportions &lt;- rbind(proportions, TotalColProb)\nproportions\n\n                  Unlikely Moderately Likely Very Likely TotalRowProb\nStudents       0.148679245        0.29283019  0.17358491   0.61509434\nFaculty/Staff  0.077735849        0.10339623  0.07396226   0.25509434\nAlumni         0.015094340        0.01358491  0.01358491   0.04226415\nTown Residents 0.009811321        0.04377358  0.03396226   0.08754717\nTotalColProb   0.251320755        0.45358491  0.29509434   1.00000000\n\n\n\nNow that our tables have been tabulated, we can answer some probability questions.\nWhat proportion of Students participated in the survey?\n\\(P_S= 815/1325\\) or \\(0.61509434\\)\nWhat proportion of Faculty/Staff remarked that they were Unlikely to participate?\n\\(P(U|FS) = P(U \\cap FS) / P(FS) = 103/338 = 0.077735849/0.25509434 = 0.3047337\\)\nIf a person is a Town Resident, what is the probability that they will select Very Likely?\n\\(P(VL|TR) = P(VL \\cap TR) / P(TR) = 45/116 = 0.03396226/0.08754717 = 0.387931\\)\nWhat proportion of people would you expect to select Moderately Likely?\n\\(P_ML = 601/1325\\) or \\(0.4535849\\). What is the expected number? \\(E(ML) = 601\\).\nWhat is the probability that a randomly selected person is a Faculty/Staff or selects Very Likely?\n\\(P_FS + P_VL - P_FS \\cup P_VL = 0.25509434 + 0.2950943 − 0.07396226 = 0.4762264\\)\n\n\n# E(X) of Moderately Likely (formula n*p)\nn = total\nn  #1325\n\n[1] 1325\n\np = TotalColProb[2]\np  #.45358\n\nModerately Likely \n        0.4535849 \n\nn * p  #Or simply the frequency for the column Moderately Likely\n\nModerately Likely \n              601 \n\n# Var(X) (formula n*p*(1-p))\nvarx &lt;- n * p * (1 - p)\nvarx  #328.3955\n\nModerately Likely \n         328.3955 \n\n# SD(X) (formula sqrt of variance)\nsdx &lt;- sqrt(varx)\nsdx  #18.12\n\nModerately Likely \n         18.12169",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#steps-to-conduct-chi-squared-test-for-independence",
    "href": "contingency.html#steps-to-conduct-chi-squared-test-for-independence",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "9.1 Steps to conduct Chi-Squared Test for Independence",
    "text": "9.1 Steps to conduct Chi-Squared Test for Independence\n\nStep 1: Write the null and alternate hypotheses.\nStep 2: Compute the test statistic.\nStep 3: Calculate the probability.\nStep 4: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis OR If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#assumptions-of-the-chi-squared-test-of-independence",
    "href": "contingency.html#assumptions-of-the-chi-squared-test-of-independence",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "9.2 Assumptions of the Chi-Squared Test of Independence",
    "text": "9.2 Assumptions of the Chi-Squared Test of Independence\n\nAssumption 1: The variables must be nominal or ordinal.\nAssumption 2: The expected values should be 5 or higher in at least 80% of groups.\nAssumption 3: The observations must be independent.\nNote that there are a couple of ways observations can be non-independent.\n\nOne way to violate this assumption would be if the data included the same set of people before and after some intervention or treatment.\nAnother way to violate this assumption would be for the data to include siblings or parents and children or spouses or other people who are somehow linked to one another.\nSince people who are linked to each other often have similar characteristics, statistical tests need to be able to account for this and the chi-squared test does not.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#example-of-a-test-for-independence",
    "href": "contingency.html#example-of-a-test-for-independence",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "9.3 Example of a Test for Independence",
    "text": "9.3 Example of a Test for Independence\n\n\n\nContingency Table Example\n\n\n\nStep 1: Set up the Hypothesis\n\n\\(H_0\\): Age group and Brand Name are independent.\n\\(H_A\\): Age group and Brand Name are dependent.\n\nFirst, we bring in the data manually from the table by column. We had 3 columns, so we need to make three vectors as shown below.\n\n\nUA &lt;- c(174, 54)\nN &lt;- c(132, 72)\nA &lt;- c(90, 78)\n\n\nThen we convert these columns to a data frame. If you already have a data frame, you don’t have to do this step.\n\n\nM &lt;- data.frame(UA, N, A)\nM\n\n   UA   N  A\n1 174 132 90\n2  54  72 78\n\n\n\nAlways make sure your data frame looks like the table.\nSteps 2 and 3: Compute the test statistic and calculate the probability.\nHere, we run the chisq.test() command, which only includes the columns of interest.\n\n\nchisq.test(M)\n\n\n    Pearson's Chi-squared test\n\ndata:  M\nX-squared = 22.529, df = 2, p-value = 1.282e-05\n\n\n\nStep 4: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nHere, our p-value is 1.282e-05, so we reject the null, \\(H_0\\), and conclude that Age group and Brand Names are dependent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#example-using-an-imported-dataset",
    "href": "contingency.html#example-using-an-imported-dataset",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "9.4 Example Using an Imported Dataset",
    "text": "9.4 Example Using an Imported Dataset\n\nImport the STEM data into a data frame (table) into R.\nStep 1: Set up the Hypothesis:\n\n\\(H_0\\): Stem field and Gender are independent.\n\\(H_A\\): Stem field and Gender are dependent.\n\n\n\nSTEM &lt;- read.csv(\"data/stem.csv\")\nSTEM\n\n   STEM.field Female Male\n1    Sciences    120  100\n2  Technology     15   75\n3 Engineering     30  125\n4        Math     15   20\n\n\n\nSteps 2 and 3: Compute the test statistic and calculate the probability.\nAgain, we use Rs chisq.test() command to calculate the value of the test statistic, as well as the p-value. Within the chisq.test() function, we indicate that the relevant data are in columns 2 and 3 of the data frame.\n\n\nchisq.test(STEM[, 2:3])\n\n\n    Pearson's Chi-squared test\n\ndata:  STEM[, 2:3]\nX-squared = 66.795, df = 3, p-value = 2.072e-14\n\n\n\nStep 4: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nSince the p-value is less than 0.05, we can reject the null hypothesis. At the 5% significance level, we can conclude that one’s sex does influence field choice within the STEM major.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#example-using-raw-numbers",
    "href": "contingency.html#example-using-raw-numbers",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "10.1 Example Using Raw Numbers",
    "text": "10.1 Example Using Raw Numbers\n\nLast year the management at a restaurant surveyed its patrons to rate the quality of its food. The results were as follows: 15% Excellent; 30% Good; 45% Fair; and 10% Poor.\nBased on this and other survey results, management made changes to the menu.\nThis year, the management surveyed 250 patrons, asking the same questions about food quality. Here are the results: 46 Excellent; 83 Good; 105 Fair; and 16 Poor.\nWe want to know if the current results agree with those from last year, or if there has been a significant change.\nTo do this, we compute an expected frequency for each category and compare it to what we actually observe. Then, we compute the difference between what was observed and expected for each category. If the results this year are consistent with last year, these differences will be relatively small.\nThe steps are same as the Chi-Squared Test of Independence.\nStep 1: Set up the Hypothesis:\n\n\\(𝐻_0\\): Each proportion equals a hypothesized value.\n\\(𝐻_A\\): Not all proportions equal their hypothesized values.\n\nSteps 2 and3: Compute the test statistic and calculate the probability.\nWe can use R’s chisq.test() function to calculate the value of the test statistic and corresponding p-value. Within this function, we use the option p to indicate the location of the hypothesized proportions.\n\n\n# Chi Squared Test Goodness of Fit\no &lt;- c(46, 83, 105, 16)\np &lt;- c(0.15, 0.3, 0.45, 0.1)\nchisq.test(o, p = p)\n\n\n    Chi-squared test for given probabilities\n\ndata:  o\nX-squared = 6.52, df = 3, p-value = 0.08888\n\n\n\nNote that p= is required here because it is not the second argument in the chisq.test() command.\nStep 4: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nAt alpha = .05, we do not reject \\(H_0\\) and cannot conclude that the proportions differ from the ones a year ago.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#example-using-an-imported-dataset-1",
    "href": "contingency.html#example-using-an-imported-dataset-1",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "10.2 Example Using an Imported Dataset",
    "text": "10.2 Example Using an Imported Dataset\n\nThe steps are same as the Chi-Squared Test of Independence.\nStep 1: Set up the Hypothesis:\n\n\\(𝐻_0\\): Each proportion equals a hypothesized value.\n\\(𝐻_A\\): Not all proportions equal their hypothesized values.\n\nSteps 2 and 3: Compute the test statistic and calculate the probability.\nFirst, we import the stemGOF.csv data into a data frame (table) in R.\nThen, again, we use Rs chisq.test function to calculate the value of the test statistic and the p-value. Within the chisq.test function, we use the option p to indicate the location of the hypothesized proportions.\n\n\nSTEMGOF &lt;- read.csv(\"data/stemGOF.csv\")\nSTEMGOF\n\n            Major X2010Proportions RecentNumbers\n1        Business             0.19            80\n2       Education             0.09            35\n3      Healthcare             0.12            85\n4 Social Sciences             0.22           105\n5            STEM             0.08            55\n6           Other             0.30           140\n\nchisq.test(STEMGOF$RecentNumbers, p = STEMGOF$X2010Proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  STEMGOF$RecentNumbers\nX-squared = 21.526, df = 5, p-value = 0.0006441\n\n\n\nStep 4: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nSince our p-value is 0.0006441, we reject \\(H_0\\) and conclude that the proportions differ at the 5% significance level. In this case, that means that the recent stem numbers differ from the 2010 values.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "ttests.html",
    "href": "ttests.html",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "",
    "text": "8.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#at-a-glance",
    "href": "ttests.html#at-a-glance",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "",
    "text": "In order to succeed in this section, you will need to understand how to set up and test a hypothesis, knowing that we can reuse our logic of hypothesis testing for other types of models later on. Pay close attention to the idea of and the reason for the t-distribution with regards to one-sample, independent samples, and dependent samples hypothesis test. Additionally, note when the test is a two-tailed, a right-tailed, or a left-tailed test, and what that means for the interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#lesson-objectives",
    "href": "ttests.html#lesson-objectives",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.2 Lesson Objectives",
    "text": "8.2 Lesson Objectives\n\nCompare a sample mean to a population mean with a one-sample t-test.\nCompare two unrelated sample means with an independent-samples t-test.\nCompare two related sample means with a dependent-samples t-test.\nCompare and contrast the the three t-tests covered in this module: one-sample, independent samples, and dependent samples.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#consider-while-reading",
    "href": "ttests.html#consider-while-reading",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.3 Consider While Reading",
    "text": "8.3 Consider While Reading\n\nIn this lesson, we learn more about hypothesis testing, which will continue throughout the rest of the term! We are still conducting inference analysis. As an analyst, I like to say that this starts the fun stuff.\nWe cover three important t-tests. There are more examples on the our course management system with regards to setting up a two-tailed, right-tailed, or left-tailed test and adjusting the \\(\\mu\\) of interest.\nMake sure you understand the key differences between all the three t-tests presented in this module and their role in interpreting data and making inferences. All three tests are making comparisons: comparisons to a mean value, comparison to another population, or comparison to paired data like before and after results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-testing-covers-a-variety-of-tests",
    "href": "ttests.html#hypothesis-testing-covers-a-variety-of-tests",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.1 Hypothesis Testing Covers a Variety of Tests",
    "text": "9.1 Hypothesis Testing Covers a Variety of Tests\n\nWe will cover the following:\n\n\nOne-Sample t-test;\nIndependent samples t-test;\nDependent (or Paired) samples t-test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#using-a-t-distribution",
    "href": "ttests.html#using-a-t-distribution",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.2 Using a t-Distribution",
    "text": "9.2 Using a t-Distribution\n\nAll of these hypothesis tests listed above use a t-distribution, which is like a z-distribution (i.e., z-score), but specifically accounts for some specific population information being unknown.\n\nThe z-distribution shows how many sample standard deviations (SD) some value is away from the mean.\nThe t-distribution shows how many standard errors (SE) some value is away from the mean.\n\nAll of these hypothesis tests in the assigned readings use the t.test() command to execute in R with different parameters set.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#approaches-to-hypothesis-testing",
    "href": "ttests.html#approaches-to-hypothesis-testing",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.3 Approaches to Hypothesis Testing",
    "text": "9.3 Approaches to Hypothesis Testing\n\nAll approaches to scientific hypothesis testing enable us to determine whether the sample evidence is inconsistent with what is hypothesized under the null hypothesis, \\(H_0\\).\nBasic principle: First assume that \\(H_0\\) is true and then determine if sample evidence contradicts this assumption.\nThis means that we can never assume our claim (against status quo) is supported without rejecting the null statistically through the scientific process.\nThere are two general approaches to hypothesis testing:\n\nThe Critical Value Approach\nThe p-value Approach\n\n\n\n9.3.1 The Critical Value Approach: Not used here.\n\nIn this approach, we calculate the test statistic, which is also based on the type of test conducted. We then see whether the test statistic is past the critical value either looked up in a table or computed directly from a function in R.\n\n\n\n9.3.2 The p-Value Approach: What we are using here.\n\nIn this approach, we still calculate the test statistic, which is still based on the type of test conducted. However, in the p-value approach, this test statistic corresponds directly to a p-value, or probability value, that we use to make a decision on whether the hypothesis is supported or not.\n\nThe p-value is the likelihood, or probability, of observing a sample mean that is at least as extreme as the one derived from the given sample, under the assumption that the null hypothesis is true.\nThe calculation of the p-value depends on the specification of the alternative hypothesis.\nWe set a decision rule to reject \\(H_0\\) if p-value \\(&lt; \\alpha\\), which is set apriori. Specifically, the p-value approach sets an \\(\\alpha\\) value (e.g., .05, .01, .001) and then determines if the p-value calculated is less than the set alpha value. The smaller the \\(\\alpha\\), the more difficult it is to find significance in your alternative hypothesis, where you reject \\(H_0\\).\n\nWe typically set a \\(\\alpha &lt; .05\\) for a variety of tests.\nWe can go lower (e.g. .01, .001), but we typically do not go higher than .05.\n\n\n\n\n\n9.3.3 Why Use The p-Value Approach\n\nThe critical value approach has variance in the formulas and numbers looked up (based on distribution and sometimes degrees of freedom), while the p-value is used consistently, we reject the null if our p-value is less than an \\(\\alpha\\) set apriori.\n\n\n\n9.3.4 Five Step Procedure Using The p-value Approach\n\nSpecify null and alternative hypothesis.\nCompute the test statistic.\nCalculate the p-value and compare it to a predetermined alpha level.\nInterpret the probability.\nState the conclusion.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#setting-up-hypotheses-tests",
    "href": "ttests.html#setting-up-hypotheses-tests",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.4 Setting up Hypotheses Tests",
    "text": "9.4 Setting up Hypotheses Tests\n\nThere are three types of hypotheses tests that you can analyze:\n\n\nTwo-tailed test: \\(H_0 =\\) vs \\(H_A \\neq\\)\nRight-tailed test: \\(H_0 &lt;=\\) vs \\(H_A &gt;\\)\nLeft-tailed test: \\(H_0 &gt;=\\) vs \\(H_A &lt;\\)\n\n\nIn a two-tailed test, the null is set to the equality (\\(H_0 =\\)) vs an alternate set to the inequality (\\(H_A \\neq\\)). In a right-tailed test, the null is set to less than or equal to (\\(H_0 &lt;=\\)), while the alternative is set to greater than (\\(H_A &gt;\\)). Finally, in a left-tailed test, the null is set to greater than or equal to (\\(H_0 &gt;=\\)), while the alternative is set to less than (\\(H_A &lt;\\)).\nIn the figure below, the green area is the Rejection region where we reject \\(H_0\\) and show support for our alternative hypothesis \\(H_A\\).\n\nThe rest of the area under the curve is the Failure to Reject \\(H_0\\) region in which we cannot show support for our alternative hypothesis \\(H_A\\).\n\n\n\n\n\nHypothesis Test\n\n\n\nIn a hypothesis test, you have to decide if a claim is true or not. Before you can figure out if you have a left-tailed test or right-tailed test, you have to make sure you have a single tail to begin with. A tail in hypothesis testing refers to the tail at either end of a distribution curve.\n\nStep 1: Write your null hypothesis statement and your alternate hypothesis statement.\nStep 2: Draw a normal distribution curve.\nStep 3: Shade in the related area under the normal distribution curve. The area under a curve represents 100%, so shade the area accordingly. The number line goes from left to right, so the first 25% is on the left and the 75% mark would be at the right tail.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-testing-single-population-a-right-tailed-test",
    "href": "ttests.html#hypothesis-testing-single-population-a-right-tailed-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.5 Hypothesis Testing: Single Population: A Right-Tailed Test",
    "text": "9.5 Hypothesis Testing: Single Population: A Right-Tailed Test\n\nA right-tailed test is where your hypothesis statement contains a greater than (&gt;) symbol. In other words, the inequality points to the right.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is greater than the original 90 hours, your hypothesis statements might be:\nNull hypothesis: No change or less than (\\(H_0\\) ≤ 90)\nAlternate hypothesis: Increased change (\\(H_A\\) &gt; 90)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-testing-single-population-a-left-tailed-test",
    "href": "ttests.html#hypothesis-testing-single-population-a-left-tailed-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.6 Hypothesis Testing: Single Population: A Left-Tailed Test",
    "text": "9.6 Hypothesis Testing: Single Population: A Left-Tailed Test\n\nA left-tailed test is where your hypothesis statement contains a less than (&lt;) symbol. In other words, the inequality points to the left.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is less than 70 hours, your hypothesis statements might be:\nNull hypothesis: No change or greater than (\\(H_0\\) ≥ 70)\nAlternate hypothesis: Decreased change (\\(H_A\\) &lt; 70)\n\n\n\n\n\nLeft-Tailed Hypothesis Test",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-testing-single-population-a-two-tailed-test",
    "href": "ttests.html#hypothesis-testing-single-population-a-two-tailed-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.7 Hypothesis Testing: Single Population: A Two-Tailed Test",
    "text": "9.7 Hypothesis Testing: Single Population: A Two-Tailed Test\n\nThe two-tailed test is called a two-tailed test because the rejection region can be in either tail.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is different from the mean value, your hypothesis statements might be:\nNull hypothesis: Equals mean (\\(H_0 = \\mu\\)).\nAlternate hypothesis: Not equal to mean (\\(H_A \\neq \\mu\\)).\n\n\n\n\n\nTwo-Tailed Hypothesis Test",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#forming-a-hypothesis",
    "href": "ttests.html#forming-a-hypothesis",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "9.8 Forming a Hypothesis",
    "text": "9.8 Forming a Hypothesis\n\nWhen forming a hypothesis, we need to do the following first.\n\nIdentify the relevant population parameter of interest (e.g., \\(\\mu\\)).\nDetermine whether it is a one- or a two-tailed test.\nInclude some form of the equality sign in \\(H_0\\) and use \\(H_A\\) to establish a claim.\n\nFor an example in how to set up a hypothesis, we can look at a trade group that predicts that back-to-school spending will average $606.40 per family this year. The group uses this information to set an economic model in making predictions. A different economic model will be needed if the prediction is wrong.\n\nParameter of interest is \\(\\mu\\) since we are interested in the average back-to-school spending.\nSince we want to determine if the population mean differs from 606.4 (i.e, \\(\\neq\\)), it is a two-tail test.\nThe hypothesis test is as follows: \\(H_0: \\mu = 606.4\\) versus \\(H_A: \\mu \\neq 606.4\\)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#t.test-command-for-one-sample-t-test",
    "href": "ttests.html#t.test-command-for-one-sample-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.1 t.test() Command for One-Sample t-Test",
    "text": "11.1 t.test() Command for One-Sample t-Test\n\nIn base R, the t.test() command discussed above is useful for getting the t for a one-sample t-test. The command arguments include the name of the variable and the hypothesized or population value (\\(\\mu\\)) to compare it to. We can also include the alternative statement to signify a right-tailed, left-tailed, or two-tailed test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-1-specify-null-and-alternative-hypothesis",
    "href": "ttests.html#step-1-specify-null-and-alternative-hypothesis",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.2 Step 1: Specify Null and Alternative Hypothesis",
    "text": "11.2 Step 1: Specify Null and Alternative Hypothesis\n\nIn order to set up the appropriate null and alternative hypothesis, we need to determine the type of test: two-tailed, right-tailed, or left-tailed.\n\n\n11.2.1 Two-Tailed Test\n\nReject \\(H_0\\) on either side of the hypothesized value of the population parameter.\n\n\\(H_0: \\mu =\\mu_0\\) versus \\(H_A\\): \\(\\mu \\neq \\mu_0\\).\n\nThe \\(\\neq\\) symbol in \\(H_A\\) indicates that both tail areas in the distribution will be used to make the decision regarding the rejection of \\(H_0\\).\nA not equal to sign in the alternative signifies a two-tailed test.\n\nAn example of a two-tailed problem: A Dean is interested if the hours of study time per week is different than 12 hours.\n\\(H_0\\): Study hours is not different than 12 hours (\\(=\\)).\n\\(H_A\\): Study hours is different than 12 hours. (\\(\\neq\\)).\n\nExample of command in R:\n\ntwo.sided for two-tailed. However, this is the default value so it could be left out of the statement for the same results.\n\\(\\mu\\) of interest is 12.\n\n\n\nStudyHours &lt;- read.csv(\"data/StudyHours.csv\")\nsummary(StudyHours)\n\n     Hours      \n Min.   : 4.00  \n 1st Qu.:11.00  \n Median :16.00  \n Mean   :16.37  \n 3rd Qu.:20.50  \n Max.   :35.00  \n\nt.test(StudyHours$Hours, mu = 12, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = 3.5842, df = 34, p-value = 0.001047\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 13.89281 18.85005\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .01 level as indicated by the\n# p-value of .001047.\n\n\n\n11.2.2 One-Tailed Test\n\nReject \\(H_0\\) only on one side of the hypothesized value of the population parameter.\n\n\\(H_0\\): \\(\\mu &lt;=\\mu_0\\) versus \\(H_A\\): \\(\\mu &gt; \\mu_0\\) (right-tailed test).\n\\(H_0\\): \\(\\mu &gt;=\\mu_0\\) versus \\(H_A\\): \\(\\mu &lt; \\mu_0\\) (left-tailed test).\n\nNote that the inequality in the alternative \\(H_A\\) determines which tail area will be used to make the decision regarding the rejection of \\(H_0\\), right (&gt;) or left (&lt;).\nA greater than sign in the alternative signifies a right-tailed test.\n\nAn example of a right-tailed problem: A Dean is interested if the hours of study time per week is greater than 10 hours.\n\\(H_0\\): Study hours are less than or equal to than 10 hours a week. (\\(&lt;=\\)) \\(H_A\\): Study hours are greater than 10 hours a week. (\\(&gt;\\))\nExample of right-tailed command in R:\n\ngreater for right-tailed.\n\\(\\mu\\) of interest is 10 for 10 hours a week.\n\n\n\n\nt.test(StudyHours$Hours, mu = 10, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = 5.224, df = 34, p-value = 4.398e-06\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 14.3091     Inf\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 4.398e-06.\n\n\nA less than sign in the alternative signifies a left-tailed test.\n\nExample of a left-tailed problem: A Dean is interested if the hours of study time per week is less than 24 hours.\n\\(H_0\\): Study hours are greater than or equal to than 24 hours a week. (\\(&gt;=\\))\n\\(H_A\\): Study hours are less than 24 hours a week. (\\(&lt;\\))\nExample of left-tailed command in R:\n\nless for left-tailed.\n\\(\\mu\\) of interest is 24 for 24 hours a week.\n\n\n\n\nt.test(StudyHours$Hours, mu = 24, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = -6.2547, df = 34, p-value = 2.015e-07\nalternative hypothesis: true mean is less than 24\n95 percent confidence interval:\n     -Inf 18.43376\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 2.015e-07.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-2.-compute-the-test-statistic",
    "href": "ttests.html#step-2.-compute-the-test-statistic",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.3 Step 2. Compute the Test Statistic",
    "text": "11.3 Step 2. Compute the Test Statistic\n\nWhen the population standard deviation \\(\\sigma\\) is unknown, the test statistic for testing the population mean \\(\\mu\\) is assumed to follow the \\(t\\) distribution with a computed degrees of freedom (\\(df\\)) based on whether population variances are assumed to be equal or not.\nFormula for the Test Statistic: \\(t = (m_x-\\mu_x)/(s_x/\\sqrt(n_x))\\)\n\nIn the One-sample t-test, \\(m_x\\) represents the mean of the variable \\(x\\), the variable to be tested, \\(\\mu_x\\) is the population mean or hypothesized value of the variable, \\(s_x\\) is the sample standard deviation of \\(s\\), and \\(n\\) is the sample size.\n\nR will compute the \\(df\\) based on the parameters set in the t.test() command along with the test statistic, so we can rely on it to calculate this for us.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-3.-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level",
    "href": "ttests.html#step-3.-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.4 Step 3. Calculate the p-value and Compare it to a Predetermined Alpha level",
    "text": "11.4 Step 3. Calculate the p-value and Compare it to a Predetermined Alpha level\n\nSet alpha (\\(\\alpha\\)) to a common level, like .05, and compare it to the calculated p-value from your output in R.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-4.-interpret-the-probability",
    "href": "ttests.html#step-4.-interpret-the-probability",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.5 Step 4. Interpret the Probability",
    "text": "11.5 Step 4. Interpret the Probability\n\nReject \\(H_0\\) if p-value is less than \\(\\alpha\\) value. This means we show support for our alternative hypothesis \\(H_A\\), which is against status quo.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-5.-state-the-conclusion",
    "href": "ttests.html#step-5.-state-the-conclusion",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.6 Step 5. State the Conclusion",
    "text": "11.6 Step 5. State the Conclusion\n\nInterpret the results in plain English.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#example-of-a-one-sample-t-test-in-r",
    "href": "ttests.html#example-of-a-one-sample-t-test-in-r",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.7 Example of a One-Sample t-Test in R",
    "text": "11.7 Example of a One-Sample t-Test in R\n\nTo conduct a one-sample t-Test in R, first, let’s read in a dataset nhanes2016.csv from your book files and go ahead and load the tidyverse. Tidyverse is loaded so that we can subset the dataset in the next example.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\n\n\nSecond, lets select a variable to test. We are going to compare the mean of variable BPXSY1 to 120, which makes it a two-tailed test (\\(= vs \\neq\\)) with the \\(\\mu\\) set to 120.\nOnce we have our problem idea, we can go through the 5 steps listed above.\nStep 1: Write null and alternative.\n\nH0: There is no difference between mean systolic BP and the cutoff for normal BP, 120 mmHG.\nHA: There is a difference between mean systolic BP and the cutoff for normal BP, 120 mmHG.\n\nSteps 2 and 3: Calculating test-statistic and p-value.\n\nAgain, the t.test() command does both, the statement and output are listed below.\n\n\n\nt.test(x = nhanes.2016$BPXSY1, mu = 120, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  nhanes.2016$BPXSY1\nt = 2.4491, df = 7144, p-value = 0.01435\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.1077 120.9711\nsample estimates:\nmean of x \n 120.5394 \n\n\n\nSteps 4 and 5: Interpret probability and results.\n\nWe found a test statistic is 2.4491, and a corresponding p-value is .01435. If our alpha was set to .05, .01435 is smaller than that (p-value &lt; alpha) This indicates that we should reject the null (\\(H_0\\)) and support the alternative (\\(H_A\\)) that the true mean is not equal to 120. This is in regards to blood pressure.\nTherefore, the sample mean systolic BP not equal to 120, or more formally the mean systolic blood pressure in a sample of 7,145 people was 120.54 (sd = 18.62). And our one-sample t-test found this mean to be statistically significantly different from the hypothesized mean of 120 [t(7144) = 2.449; p = 0.01435]. This indicates that the sample likely came from a population with a mean systolic blood pressure not equal to 120, signifying that the true population is likely not equal to 120.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#example-of-a-one-sample-using-a-subset",
    "href": "ttests.html#example-of-a-one-sample-using-a-subset",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "11.8 Example of a One-Sample Using a Subset",
    "text": "11.8 Example of a One-Sample Using a Subset\n\nLet’s do another example using a subset of the data. In this example, let’s create a subset of people 65+ years old to run the same two-tailed test. This allows us to see if the people 65 years and older had a different blood pressure than 120.\nIn the command below, we use tidyverse to set up a new data frame object named nhanes.2016.65plus to save the filtered data.\n\n\nnhanes.2016.65plus &lt;- nhanes.2016 %&gt;%\n    filter(RIDAGEYR &gt;= 65)\n\n\nStep 1: Write null and alternative.\n\nH0: There is no difference between mean systolic BP and the cutoff for normal BP, 120 mmHG when people are 65 or over.\nHA: There is a difference between mean systolic BP and the cutoff for normal BP, 120 mmHG when people are 65 or over.\n\nSteps 2 and 3: Calculating test-statistic and p-value.\nAgain, we can use one t.test() command to handle steps 2 and 3.\n\n\nt.test(x = nhanes.2016.65plus$BPXSY1, mu = 120)\n\n\n    One Sample t-test\n\ndata:  nhanes.2016.65plus$BPXSY1\nt = 29.238, df = 1232, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 135.4832 137.7106\nsample estimates:\nmean of x \n 136.5969 \n\n\n\nSteps 4 and 5: Interpret probability and results.\n\nBased on our output, we found a test statistic is 29.238, and a corresponding p-value is .000. If our alpha was set to .05, .000 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the true mean is not equal to 120 for people 65 and older. This again is in regards to blood pressure.\nThe mean systolic blood pressure in a sample of 1233 NHANES participants who were age 65 and above was 136.60 (sd = 19.93). The mean systolic blood pressure was found to be statistically significantly different from the hypothesized mean of 120 via a one-sample t-test (t(1232) = 29.238, p &lt; 0.001). The true mean systolic blood pressure in the population of adults 65 and older is likely not equal to 120.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#t.test-command-for-independent-samples-t-test",
    "href": "ttests.html#t.test-command-for-independent-samples-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.1 t.test() Command for Independent Samples t-Test",
    "text": "12.1 t.test() Command for Independent Samples t-Test\n\nIn base R, the t.test() command is useful for getting the t for the Independent samples t-test. This is the same t.test() command used above.\nThe command needs to be altered to handle the second group. In the command, the arguments include a formula which is formatted as t.test(continuous_variable ~ grouping_variable) or t.test(population1, population2). Deciding on the correct format of the argument depends on the shape of the data discussed later on.\n\n\n\n\nTilde or Comma\n\n\n\nA separate (\\(d_0\\)) value can be set here under the (\\(\\mu\\)) parameter if the value is not equal to 0, however 0 is the most common which tests for differences between populations (i.e., is the mean of population 1 different than mean of population 2). The alternative statement can be set at “two.sided”, “greater”, or “less”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-1-specify-null-and-alternative-hypothesis-1",
    "href": "ttests.html#step-1-specify-null-and-alternative-hypothesis-1",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.2 Step 1: Specify Null and Alternative Hypothesis",
    "text": "12.2 Step 1: Specify Null and Alternative Hypothesis\n\nWhen conducting hypothesis tests concerning \\(\\mu_1  - \\mu_2\\) , the competing hypotheses will take one of the following forms:\n\nTwo-tailed Test: \\(H_0: \\mu_1  - \\mu_2 =  d_0\\) versus \\(H_0: \\mu_1  - \\mu_2 \\neq  d_0\\)\nRight-tailed Test: \\(H_0: \\mu_1  - \\mu_2 &lt;=  d_0\\) versus \\(H_0: \\mu_1  - \\mu_2 &gt;  d_0\\)\nLeft-tailed Test: \\(H_0: \\mu_1  - \\mu_2 &gt;=  d_0\\) versus \\(H_0: \\mu_1  - \\mu_2 &lt;  d_0\\)\n\nWhere \\(d_0\\) is the hypothesized difference between \\(\\mu_1  - \\mu_2\\).\n\n\n\n\nIndependent Samples Table\n\n\n\nExample of a two-tailed test:\nDoes spending differ between men and women?\n\n\\(H_0\\): No difference in spending between men and women. (\\(=\\)).\n\\(H_A\\): There is a difference in spending between men and women (\\(\\neq\\)).\nExample of a two-tailed command in R:\n\nThe statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. Therefore, the format t.test(var1, var2) should be used.\n\n\n\n\nSpend &lt;- read.csv(\"data/Spend.csv\")\nsummary(Spend)\n\n    MenSpend        WomenSpend    \n Min.   : 49.00   Min.   : 13.00  \n 1st Qu.: 89.25   1st Qu.: 62.50  \n Median : 99.00   Median : 84.00  \n Mean   :100.90   Mean   : 87.23  \n 3rd Qu.:117.75   3rd Qu.:100.50  \n Max.   :140.00   Max.   :220.00  \n\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"two.sided\", mu = 0)\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.1259\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.984055 31.317388\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.1259\n# which is greater than .05.\n\n\nThe alternative \\(H_A\\) and \\(\\mu\\) are not changed off their default values, so the statement above could be simplified from what is provided to the statement below.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend)\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.1259\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.984055 31.317388\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n\n\nExample of a right-tailed test: Is men’s spending (\\(\\bar{x}_1\\)) greater than that of women’s spending (\\(\\bar{x}_2\\))?\n\n\\(H_0\\): Men’s spending is less than or equal to women’s spending (\\(&lt;=\\)).\n\\(H_A\\): Men’s spending is greater than women’s spending (\\(&gt;\\)).\nExample of a right-tailed command in R:\n\nAgain, the statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. The alternative is changed off the default value, so the statement parameter is required.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.06296\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.0521     Inf\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.06296\n# which is greater than .05.\n\n\nExample of a left-tailed test: Is men’s spending (\\(\\bar{x}_1\\)) less than that of women’s spending (\\(\\bar{x}_2\\))?\n\n\\(H_0\\): Men’s spending is greater than or equal to women’s spending (\\(&gt;=\\)).\n\\(H_A\\): Men’s spending is less than women’s spending (\\(&lt;\\)).\nExample of a left-tailed command in R:\n\nAgain, the statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. The alternative is changed off the default value, so the statement parameter is required.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.937\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 28.38543\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.937\n# which is greater than .05.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-2-compute-the-test-statistic",
    "href": "ttests.html#step-2-compute-the-test-statistic",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.3 Step 2: Compute the Test Statistic",
    "text": "12.3 Step 2: Compute the Test Statistic\nFormula for the Test Statistic: \\(t = (m_1 - m_2)/\\sqrt((s^2_1/n_1)+(s^2_2/n_2))\\)\n\nIn the independent samples t-test formula, \\(m_1\\) is the mean of one group and \\(m_2\\) is the mean of the other group; the difference between the means makes up the numerator. The larger the difference between the group means, the larger the numerator will be and the larger the t-statistic will be!\nThe denominator includes the variances for the first group, \\(s^2_1\\), and the second group, \\(s^2_2\\) along with the sample sizes for each group, \\(n_1\\) and \\(n_2\\).\nDegrees of freedom are computed as n – k.\nThere is a 95% confidence interval around the difference between the groups.\nThe rules are the same here as the above t-test for steps 3 - 5.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-3-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level",
    "href": "ttests.html#step-3-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.4 Step 3: Calculate the p-value and Compare it to a Predetermined Alpha level",
    "text": "12.4 Step 3: Calculate the p-value and Compare it to a Predetermined Alpha level",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-4-interpret-the-probability",
    "href": "ttests.html#step-4-interpret-the-probability",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.5 Step 4: Interpret the Probability",
    "text": "12.5 Step 4: Interpret the Probability",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-5-write-a-conclusion",
    "href": "ttests.html#step-5-write-a-conclusion",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.6 Step 5: Write a Conclusion",
    "text": "12.6 Step 5: Write a Conclusion",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#example-of-an-independent-samples-t-test-in-r",
    "href": "ttests.html#example-of-an-independent-samples-t-test-in-r",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.7 Example of an Independent Samples t-Test in R",
    "text": "12.7 Example of an Independent Samples t-Test in R\n\nUsing the nhanes data set, let’s bring over some code from the last section so that we can work this example. This includes reading in the data set and creating the subset for the ages 65 and over.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\nnhanes.2016.65plus &lt;- nhanes.2016 %&gt;%\n    filter(RIDAGEYR &gt;= 65)\n\n\nWhen conducting independent samples, grouping variables are common, so you can see if there is a difference between groups. In this example, we can test for a difference in blood pressure by sex (Male vs Female). This is coded as 1 and 2 in the data set, so we need to recode it before we begin the official test.\n\n\nnhanes.2016.cleaned &lt;- nhanes.2016 %&gt;%\n    mutate(RIAGENDR = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(sex = RIAGENDR) %&gt;%\n    rename(systolic = BPXSY1)\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): There is no difference between systolic blood pressure for males and females (\\(=\\)).\n\\(H_A\\): There is a difference between systolic blood pressure for males and females (\\(\\neq\\)).\nThis is a two tailed test.\n\nSteps 2 and 3: Calculating test-statistic and p-value.\n\nThe statement below assumes that the grouping variable (sex) is in one column, and the continuous variable (systolic) is in another column. In this case, instead of a comma between the two columns of sex (like above), we write the statement as t.test(continuousVar ~ groupingVar) like below.\n\n\n\n# All other parameters can be left at default values\nt.test(nhanes.2016.cleaned$systolic ~ nhanes.2016.cleaned$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  nhanes.2016.cleaned$systolic by nhanes.2016.cleaned$sex\nt = 7.3135, df = 7143, p-value = 2.886e-13\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n 2.347882 4.067432\nsample estimates:\n  mean in group Male mean in group Female \n            122.1767             118.9690 \n\n\n\nSteps 4 and 5: Interpret probability and results.\n\nWe found a test statistic is 7.3135, and a corresponding p-value is .000. If our alpha was set to .05, .000 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the men and women had different systolic blood pressure. Thus, there is a statistically significant difference in the mean blood pressure of males and females in the 2016 data set (t(7143) = 7.3135, p = .000).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#example-independent-t-test-using-a-subset",
    "href": "ttests.html#example-independent-t-test-using-a-subset",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "12.8 Example Independent t-test Using a Subset",
    "text": "12.8 Example Independent t-test Using a Subset\n\nCreate a subset of the data frame of people 65+ years old to run the same test on blood pressure (two-tailed test).\n\n\nnhanes.2016.65plus.clean &lt;- nhanes.2016.65plus %&gt;%\n    drop_na(BPXSY1) %&gt;%\n    mutate(sex = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(systolic = BPXSY1)\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): There is no difference between systolic blood pressure for males and females 65 years or older\n\\(H_A\\): There is a difference between systolic blood pressure for males and females 65 years or older\nThis is a two tailed test.\n\nSteps 2 and 3: Calculating test-statistic and p-value.\n\n\nt.test(nhanes.2016.65plus.clean$systolic ~ nhanes.2016.65plus.clean$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  nhanes.2016.65plus.clean$systolic by nhanes.2016.65plus.clean$sex\nt = -2.0141, df = 1231, p-value = 0.04422\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -4.5080955 -0.0591939\nsample estimates:\n  mean in group Male mean in group Female \n            135.4486             137.7323 \n\n\n\nSteps 4 and 5: Interpret probability and results.\n\nWe found a test statistic is -2.0141, and a corresponding p-value is 0.04422. If our alpha was set to .05, 0.04422 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the men and women 65 or older had different systolic blood pressure. Thus, there is a statistically significant difference in the mean blood pressure of males and females over the age of 65 participating in the 2016 NHANES (t(1231) = -2.01, p = 0.044).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#t.test-command-for-dependent-samples-t-test",
    "href": "ttests.html#t.test-command-for-dependent-samples-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.1 t.test() Command for Dependent Samples t-Test",
    "text": "13.1 t.test() Command for Dependent Samples t-Test\n\nRequires the paired = TRUE parameter: The default for the command is paired=FALSE, which is an independent samples t-test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-1-write-null-and-alternative-hypothesis",
    "href": "ttests.html#step-1-write-null-and-alternative-hypothesis",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.2 Step 1: Write Null and Alternative Hypothesis",
    "text": "13.2 Step 1: Write Null and Alternative Hypothesis\n\nWhen conducting hypothesis tests concerning \\(\\mu_d\\) , the competing hypotheses will take one of the following forms:\n\nTwo-tailed Test: \\(H_0: \\mu_d  = d_0\\) versus \\(H_A: \\mu_d \\neq  d_0\\)\nRight-tailed Test: \\(H_0: \\mu_d &lt;= d_0\\) versus \\(H_A: \\mu_d&gt;  d_0\\)\nLeft-tailed Test: \\(H_0: \\mu_d &gt;= d_0\\) versus \\(H_A: \\mu_d&lt; d_0\\)\n\nWhere \\(d_0\\) typically is equal to 0, but not always. If \\(d_0\\) is something other than 0, then you change the mu parameter in R in the t.test() command.\n\n\n\n\nDependent Samples Table\n\n\n\nExample of a two-tailed test:\nIs the weight before and after quitting smoking different from each other?\n\nThe before and after signifies a paired test.\n\\(H_0\\): No difference in weight before and after smoking (\\(=\\)).\n\\(H_A\\): There is a difference in weight before and after smoking (\\(\\neq\\)).\nExample of a two-tailed command in R for a paired test:\n\n\n\nSmoke &lt;- read.csv(\"data/Smoke.csv\")\n# Other default values are left off this statement. Assumes a mu at\n# 0.\nt.test(Smoke$After, Smoke$Before, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 2.695e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 4.870942 9.129058\nsample estimates:\nmean difference \n              7 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 2.695e-08.\n\n\nExample of a right-tailed test:\nIs the weight after quitting smoking (\\(\\mu_1\\)) greater than the weight before (\\(\\mu_2\\))?\n\n\\(H_0\\): Weight after smoking is less than or equal to the weight before (\\(&lt;=\\)).\n\\(H_A\\): Weight after smoking is greater the weight to before (\\(&gt;\\)).\nExample of a right-tailed command in R for a paired test:\n\n\n\n# The paired statement is true (due to the before after) If we bring\n# WeightBeforeQuitting to the right side of the equation we get\n# WeightAfterQuitting &gt; WeightBeforeQuitting, which is what our\n# hypothesis wants.  Therefore, in the statement below, after comes\n# first as m1 followed by a comma, followed by the before as m2.  The\n# alternative parameter is then set to 'greater' Other default values\n# are left off this statement\nt.test(Smoke$After, Smoke$Before, paired = TRUE, alternative = \"greater\")\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 1.347e-08\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 5.223767      Inf\nsample estimates:\nmean difference \n              7 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 1.347e-08.\n\n\nExample of a left-tailed test:\nIs the weight after quitting smoking (\\(\\mu_1\\)) less than the weight before (\\(mu_2\\))?\n\n\\(H_0\\): Weight after smoking is greater than or equal to the weight before (\\(&gt;=\\)).\n\\(H_A\\): Weight after smoking is less to the weight before (\\(&lt;\\)).\nExample of a left-tailed command in R for a paired test:\n\n\n\n# The alternative parameter switches to 'less' leaving all else the\n# same as above.  Again, other default values are left off this\n# statement\nt.test(Smoke$After, Smoke$Before, paired = TRUE, alternative = \"less\")\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 1\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n     -Inf 8.776233\nsample estimates:\nmean difference \n              7 \n\n# This test is not significant as indicated by a p-value of 1, which\n# is greater than .05.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-2-compute-the-test-statistic-1",
    "href": "ttests.html#step-2-compute-the-test-statistic-1",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.3 Step 2: Compute the Test Statistic",
    "text": "13.3 Step 2: Compute the Test Statistic\nFormula for the Test Statistic: \\(t = (m_d-d_0)/\\sqrt((s^2_d/n_d)\\)\n\nDependent samples t-test formula: The \\(m_d\\) is the mean of the differences between the related measures, the \\(s^2_d\\) is the variance of the mean difference between the measures, and \\(n_d\\) is the sample size. The dependent samples t-test worked a little differently from the independent samples t-test. In this case, the formula uses the mean of the differences between the two related measures.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-3-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level-1",
    "href": "ttests.html#step-3-calculate-the-p-value-and-compare-it-to-a-predetermined-alpha-level-1",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.4 Step 3: Calculate the p-value and Compare it to a Predetermined Alpha level",
    "text": "13.4 Step 3: Calculate the p-value and Compare it to a Predetermined Alpha level\n\nAgain, the rules are the same here as above for steps 3 - 5.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-4-interpret-the-probability-1",
    "href": "ttests.html#step-4-interpret-the-probability-1",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.5 Step 4: Interpret the Probability",
    "text": "13.5 Step 4: Interpret the Probability",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#step-5.-write-a-conclusion",
    "href": "ttests.html#step-5.-write-a-conclusion",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.6 Step 5. Write a Conclusion",
    "text": "13.6 Step 5. Write a Conclusion\n\nThe steps involved with setting up the null and alternative hypotheses and calculating the test statistic are based on different rules and formulas for all three t-tests, but the rest of the steps are the same.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#example-of-an-dependent-samples-t-test-in-r",
    "href": "ttests.html#example-of-an-dependent-samples-t-test-in-r",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "13.7 Example of an Dependent Samples t-Test in R",
    "text": "13.7 Example of an Dependent Samples t-Test in R\n\nGiven the nhanes.2016 dataset, let’s ask the following research question: Is there a difference between measure 1 and 2 for systolic BP?\nFirst, bring back in the data set from the last section.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): No difference between measures 1 and 2 for systolic BP. (\\(=\\))\n\\(H_A\\): There is a difference between measures 1 and 2 for systolic BP. (\\(\\neq\\)).\nThis is a two tailed test.\n\nUse the nhanes.2016.cleaned from above but be sure to rename BPXSY2 to systolic2 (two-tailed test).\n\n\nnhanes.2016.cleaned &lt;- nhanes.2016 %&gt;%\n    mutate(RIAGENDR = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(sex = RIAGENDR) %&gt;%\n    rename(systolic = BPXSY1) %&gt;%\n    rename(systolic2 = BPXSY2) %&gt;%\n    mutate(diff.syst = systolic - systolic2)\n\n\nSteps 2 and 3: Calculate test-statistic and p-value.\n\n\nt.test(nhanes.2016.cleaned$systolic, nhanes.2016.cleaned$systolic2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  nhanes.2016.cleaned$systolic and nhanes.2016.cleaned$systolic2\nt = 9.3762, df = 7100, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.4310514 0.6589360\nsample estimates:\nmean difference \n      0.5449937 \n\n\n\nSteps 4 and 5: Interpret probability and results\n\nWe found a test statistic is 9.3762, and a corresponding p-value is 0.000. If our alpha was set to .05, 0.000 is smaller than that (p-value &lt; alpha). This indicates that there is a difference in systolic blood pressure 1 and 2. Thus, there is a statistically significant difference in the mean blood pressure of reading 1 and 2 in the 2016 NHANES (t(7100) = 9.3762, p = 0.000).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "9.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#at-a-glance",
    "href": "anova.html#at-a-glance",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "In order to succeed in this section, you will need to apply what you learned about variable types (quantitative versus categorical) and hypothesis testing in earlier lessons We will learn how to conduct and interpret an ANOVA, which allows us to compare groups (from a categorical/factor variable) with respect to a continuous variable. Unlike the t-test that is limited to comparing two groups, the ANOVA is meant for three or more groups. We can also follow up a significant ANOVA with post-hoc tests to see which groups are different from each other in regards to the continuous variable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#lesson-objectives",
    "href": "anova.html#lesson-objectives",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.2 Lesson Objectives",
    "text": "9.2 Lesson Objectives\n\nConduct and interpret a one-way ANOVA.\nChoose and use post-hoc tests and contrasts.\nConduct and interpret a two-way ANOVA.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#consider-while-reading",
    "href": "anova.html#consider-while-reading",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.3 Consider While Reading",
    "text": "9.3 Consider While Reading\n\nIn this lesson, we continue with our discussion of both hypothesis testing and using inference. However, we are no longer limiting ourselves to one or two populations. ANOVA consists of the calculations that provide information about levels of variability within a regression model and form a basis for calculating tests of significance. This means that ANOVA can be conducted as we will do here, and as part of a regression analysis.\nANOVA is used to determine if there are differences among three or more populations. In cases of only two groups, an independent samples t-test should be used as was discussed in an earlier lesson. When reading, be sure to make connections back to variable data type, and know that with ANOVA, we are testing for group differences where groups are the categorical X variable or variables, and we want to see if there are group differences in regards to a continuous/numerical Y.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#omnibus-test",
    "href": "anova.html#omnibus-test",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "10.1 Omnibus test",
    "text": "10.1 Omnibus test\n\nA significant result indicates the omnibus test is significant and that there is a difference between the means. This only suggests that there is at least one group difference somewhere between groups. This is not useful in determining which means are different from each other. Therefore, for the alternative hypothesis to be supported, at least one group must be different from the rest.\nThen, if we find a significant omnibus test, we continue our analysis with planned contrasts and post-hoc tests, which determine which means are statistically significantly different from one another.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-methodology",
    "href": "anova.html#anova-methodology",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "10.2 ANOVA Methodology",
    "text": "10.2 ANOVA Methodology\n\nWe first compute the amount of variability between the sample means. This is known as the between-treatments estimate, which compares the sample means to the overall mean, sometimes called the grand mean, or the average pf all the values from the data set.\nThen, we measure how much variability there is within each sample. This is known as the within-treatments estimate, which is essentially a measure of error.\nA ratio of the first quantity to the second forms our test statistic which follows the \\(F_{df1,df2}\\) distribution, where the degrees of freedom are calculated from the number of groups - 1 (\\(df_1: k-1\\)) and the total number of observations minus the number of groups (\\(df_2: n_t-k\\)).\n\n\nNote an F distribution behaves differently than a z- or a t- distribution.\n\nThe z-distribution shows how many sample standard deviations (SD) some value is away from the mean.\nThe t-distribution shows how many standard errors (SE) away from the mean.\nThe F-distribution is used to compare 2 populations’ variances.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#some-reasons-for-error",
    "href": "anova.html#some-reasons-for-error",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "11.1 Some Reasons For Error",
    "text": "11.1 Some Reasons For Error\n\nMeasurement error refers to the difference between a measured quantity and its true value which could be due to random error or systematic error.\n\nRandom error refers to naturally occurring errors that are to be expected.\nSystematic error refers to mis-calibrated instruments causing error in measurement.\n\nBias — the tendency of a sample statistic to systematically over- or underestimate a population parameter.\n\nSelection bias refers to a systematic exclusion of certain groups from consideration for the sample.\nNonresponse bias refers to a systematic difference in preferences between respondents and non-respondents to a survey or a poll.\nSocial Desirability bias refers to a bias that refers to the systematic difference between a group’s “socially acceptable” responses to a survey or poll.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#steps-for-conducting-a-one-way-anova",
    "href": "anova.html#steps-for-conducting-a-one-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "12.1 Steps for Conducting a One-Way Anova",
    "text": "12.1 Steps for Conducting a One-Way Anova\n\nWrite the null and alternate hypotheses.\nCompute the F-test statistic.\nCompute the probability for the test statistic (p-value).\nInterpret the probability.\nWrite a conclusion.\nIf model is significant, run post-hoc tests.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#statistics-in-a-one-way-anova-table",
    "href": "anova.html#statistics-in-a-one-way-anova-table",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "12.2 Statistics in a One-Way ANOVA Table",
    "text": "12.2 Statistics in a One-Way ANOVA Table\n\nThere are a number of statistics being calculated with an aov() command, with the goal of producing the F-test statistic, which corresponds to a p-value that we can interpret the same way as we did in the earlier lessons (p-value &lt; alpha = significant result - reject the \\(H_0\\)).\n\n\n\n\nOne Way ANOVA Formulas\n\n\n\n12.2.1 Explained Variance\n\nIn a one-way ANOVA, first, we compute the explained variance, which in a one-way ANOVA is the sum of squares due to treatments (SSTR), where the treatment is our grouping variable. The explained variance suggests that the variation in outcome can be explained by a model, or in our case, a grouping variable. In computing the SSTR, we square the deviation between each group mean (\\(\\bar{y}_j\\)) and the grand mean \\(\\bar{y}\\) and multiply it by the sample size (\\(n_j\\)) and sum up all the values \\(\\sum\\). This leads to the following formula:\n\n\\(SSTR = \\sum{n_j*(\\bar{y}_j-\\bar{y}) ^2}\\)\n\nDegrees of freedom for the SSTR are computed by the number of groups \\((k-1)\\).\n\n\\(df_{sstr} = k-1\\)\n\nThe mean square due to treatment (MSTR) takes the value for the SSTR and divides by the treatment’s degrees of freedom \\((k-1)\\).\n\n\\(MSTR = SSTR/(k-1)\\)\n\nAbove in the overall methodology, the MSTR corresponds to the between-treatments estimate.\n\n\n\n12.2.2 Unexplained Variance\n\nThere is also a measure of unexplained variance, which we term error. In a one-way ANOVA, this unexplained variance refers to variability in the outcome that is not explained by the grouping variable. In the table, this error is called the sum of squares error (SSE). SSE is calculated by first multiplying the sample size of group and subtracting one (\\(n_j-1\\)), and then multiplying that number by its group variance (\\(\\sigma^2_i\\)). Once all groups are calculated, we add it up (\\(\\sum\\)). This leads to the following formula:\n\n\\(SSE = \\sum^k_{j=1}{(n_j-1)*\\sigma^2_j}\\)\n\nThis formula above considers the formula and definition of variance (\\(\\sigma^2\\)) and simplifies the formula, which you can also use:\n\n\\(SSE = \\sum^k_{j=1}\\sum^n_{i=1}(y_{ij}-\\bar{y_j})^2/(n-k)\\)\n\nDegrees of freedom for the SSE are computed by taking the total number of observations (\\(n_t\\)) and subtracting the number of groups \\((k)\\).\n\n\\(df_{sse} = n_t-k\\)\n\nThe mean square due to error (MSE) takes the SSE and divides by the appropriate degrees of freedom (\\(n_t-k\\)).\n\n\\(MSE = SSE/(n_t-k)\\)\n\nAbove in the overall methodology, the MSE corresponds to the within-treatments estimate.\nThere is a total sum of squares (SST) being calculated in an ANOVA, which refers to the total variance (explained and unexplained). This value is calculated as the total number of observations minus 1 (\\(n_t-1\\)). We don’t see it explicitly in our R output, but it is needed to help understand the full model.\n\n\n\n12.2.3 Calculating F\n\nFinally, to calculate the F-test statistic, we take the MSTR (explained variability) and divide by the MSE (unexplained variability) considering the appropriate degrees of freedom (\\(k-1\\) and \\(n_t-k\\)).\nA p-value is computed from that statistic mathematically by using the correct command in R.\n\n\\(F_{k-1,n_t-k} = MSTR/MSE\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#example-of-a-one-way-anova",
    "href": "anova.html#example-of-a-one-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "12.3 Example of a One-Way ANOVA",
    "text": "12.3 Example of a One-Way ANOVA\n\nLike the t-test, a one-way ANOVA follows 5 steps, and then includes an optional 6th step after significance is found and confirmed in step 5.\n\n\n12.3.1 Step 1: Set Up Null and Alternative Hypothesis\n\nThe competing hypotheses for the one-way ANOVA:\n\n\\(H_0: \\mu_{Atlanta} = \\mu_{Houston}= \\mu_{LosAngeles} = \\mu_{SanFran} = \\mu_{DC}\\)\n\\(H_A:\\) Not all population means of congestion levels are equal among the cities\n\n\n\n\n12.3.2 Steps 2 and 3: Compute the F-test Statistic and p-Value\n\nFirst, we read in the data set and give a look at what is included.\n\nBecause we are using read.csv() command, we can set stringsAsFactors argument to TRUE so that the categorical variable will be coded appropriately upon download.\nThe summary() command suggests that there is one categorical variable, “City” and one continuous variable, “CongestionRating.”\n\n\ncongestionData &lt;- read.csv(\"data/congestion.csv\", stringsAsFactors = TRUE)\nsummary(congestionData)\n\n             City    CongestionRating\n Atlanta       :25   Min.   :44.00   \n Houston       :25   1st Qu.:56.00   \n Los Angeles   :25   Median :59.00   \n San Francisco :25   Mean   :60.06   \n Washington, DC:25   3rd Qu.:64.00   \n                     Max.   :77.00   \n\n\nWe use the aov() command for one-way and later on for a two-way ANOVA.\n\nIn either command, our formula is ContinuousVariable ~ CategoricalVariable - this will allow us to see if there are group differences in CongestionRating (continuous) based on city (categorical).\nTake a good look at the output from these commands to compare the results.\n\n\n\n# A common set of commands that work for both a one-way and two-way\n# ANOVA: aov() command with output in a followup command: either\n# anova() or summary() command.\nAnova1way &lt;- aov(CongestionRating ~ City, data = congestionData)\nsummary(Anova1way)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nCity          4   3251   812.7   37.25 &lt;2e-16 ***\nResiduals   120   2618    21.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# anova() command works exactly the same as summary here.  This\n# command is very prevalent in various textbooks and online\n# tutorials.\nanova(Anova1way)\n\nAnalysis of Variance Table\n\nResponse: CongestionRating\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nCity        4 3250.7  812.67  37.251 &lt; 2.2e-16 ***\nResiduals 120 2617.9   21.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nUsing the anova() or summary() commands, we can see the results from the ANOVA table, including the degrees of freedom (Df), sum of squares (Sum Sq), mean square (Mean Sq), F-test statistic (F value) and p-value (Pr(&gt;F)). It also includes asterisks (*) symbols if there are group differences due to our categorical variable. The more asterisks you see, the smaller the significant code. There is a table for these codes within the output marked Signif. codes.\nReading the codes:\n\nIf there are three asterisks, you would say that that your p-value is less than .001. If there are 2, your p-value is less than .01. 1 asterisk indicates a p-value &lt; .05.\nWe never say our p-value is 0, but instead say it is less than ___.\nWe do not use .1 as a significance level, although some statisticians mark a p-value less than .1 as “marginally significant”. There is a big debate about this, but I agree with the general consensus, so our threshold for this class is less than .05. Anything greater than or equal to .05 is not significant.\n\n\n\n\n12.3.3 Steps 4 and 5: Interpret the Probability and Write a conclusion.\n\nThe F-test statistic is 37.251 and the p-value is &lt; 2.2e-16 ***. This p-value is &lt; all typical alpha values like .05 or .001. This value is very close to 0. We would state that our p-value is less than .001 by looking at the table and noting the three asterisks.\n\nThe p-value result suggests that we support the alternative \\(H_A\\): Not all population means are equal.\nMore specifically we reject the null hypothesis \\(H_0\\) in support of \\(H_A\\) and conclude that not all congestion levels are equal among the cities.\n\n\n\n12.3.4 Step 6: If Model is Significant, Run Post-Hoc Tests\n\nPairwise comparisons: There are many tests to compute pairwise comparisons. The tests tend to vary in whether they minimize Type I or Type II error, and by how much. Many of the tests change the distribution in the calculation to alter the final results. We go over the following methods:\n\nBonferroni Method;\nTukeyHSD Method\n\n\n\n12.3.4.1 Bonferroni Method\n\nThe Bonferroni method is a pairwise post-hoc test that is used after finding a statistically significant ANOVA. This method conducts a t-test for each pair of means and adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant.\nThe function has several arguments, such as \\(x =\\) for the continuous variable (listed first); \\(g =\\) for the grouping or categorical variable (listed 2nd); and the p-value adjustment, \\(p.adj =\\), which can be set as __bonf_ for Bonferroni (listed 3rd).\nThe output is a matrix of p-values, testing each pair for group differences. A value &lt; alpha signifies significant group differences.\n\nBased on the output, there are no differences in congestion levels between Houston and Atlanta, San Francisco and Atlanta, and Houston and San Francisco.\nThere are group differences in congestion levels between all other remaining groups (p-value &lt; alpha).\n\n\n\npairwise.t.test(congestionData$CongestionRating, congestionData$City, p.adj = \"bonf\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  congestionData$CongestionRating and congestionData$City \n\n               Atlanta Houston Los Angeles San Francisco\nHouston        1.00000 -       -           -            \nLos Angeles    9.7e-15 1.3e-15 -           -            \nSan Francisco  1.00000 1.00000 &lt; 2e-16     -            \nWashington, DC 0.00269 0.00071 2.1e-06     3.8e-05      \n\nP value adjustment method: bonferroni \n\n\n\n\n12.3.4.2 Tukey HSD Method\n\nThe Tukey HSD method is another pairwise post-hoc test that is used after finding a statistically significant ANOVA. This method is used to determine which means are statistically significantly different from each other by comparing each pair of means. This method is less conservative than the Bonferroni post hoc test.\nThis test is modified from the Bonferroni test using a q-distribution instead of a t-distribution to calculate the answers.\nUsing the TukeyHSD() command, we can use the Anova1way object because we already used the aov() command to produce our model above This aov() command is required before or while running the TukeyHSD() command.\n\n\nTukeyHSD(Anova1way)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = CongestionRating ~ City, data = congestionData)\n\n$City\n                               diff        lwr        upr     p adj\nHouston-Atlanta               -0.48  -4.139015   3.179015 0.9962320\nLos Angeles-Atlanta           12.24   8.580985  15.899015 0.0000000\nSan Francisco-Atlanta         -1.44  -5.099015   2.219015 0.8113836\nWashington, DC-Atlanta         4.96   1.300985   8.619015 0.0024588\nLos Angeles-Houston           12.72   9.060985  16.379015 0.0000000\nSan Francisco-Houston         -0.96  -4.619015   2.699015 0.9499289\nWashington, DC-Houston         5.44   1.780985   9.099015 0.0006646\nSan Francisco-Los Angeles    -13.68 -17.339015 -10.020985 0.0000000\nWashington, DC-Los Angeles    -7.28 -10.939015  -3.620985 0.0000021\nWashington, DC-San Francisco   6.40   2.740985  10.059015 0.0000373\n\n\n\nBased on the output, we find similar results to the Bonferroni test. There are no differences in congestion levels between Houston and Atlanta, San Francisco and Atlanta, and Houston and San Francisco. There are group differences in congestion levels between all other remaining groups (p-value &lt; alpha).\nWith the TukeyHSD() command, we can also determine which group mean is higher than the other by looking at the “diff” score. If you see a negative score than the second group listed in the output is higher than the first group listed. Ignoring the insignificant p-values &gt; .05, we find that both San Francisco and Washington has significantly less congestion than Los Angeles. We find that Los Angeles and Washington have a significantly higher group mean than Atlanta. Los Angeles and Washington DC also have a significantly higher group mean than Houston. Finally, Washington DC has a significantly higher group mean than San Francisco.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#steps-for-conducting-a-two-way-anova",
    "href": "anova.html#steps-for-conducting-a-two-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "13.1 Steps for Conducting a Two-Way ANOVA",
    "text": "13.1 Steps for Conducting a Two-Way ANOVA\n\nWrite the null and alternate hypotheses for each grouping variable.\nCompute the F-test statistics for each grouping variable.\nCompute the probability for the test statistics (2 p-values for a 2-way ANOVA).\nInterpret the probabilities.\nWrite a conclusion.\nIf model is significant, run post-hoc tests.\n\n\nThe steps are the same as a one-way ANOVA listed above, with the exception that now we have 2 grouping variables, so we need 2 F-stats and 2 p-values.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#statistics-in-a-two-way-anova-table",
    "href": "anova.html#statistics-in-a-two-way-anova-table",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "13.2 Statistics in a Two-Way ANOVA Table",
    "text": "13.2 Statistics in a Two-Way ANOVA Table\n\nWe add another row to our ANOVA table when we move from a one-way ANOVA to a two-way ANOVA. The goal is the same, which is to produce a F-test statistic. In a two-way ANOVA, we will have two F-test statistics (one for each grouping variable). Each F-test statistic corresponds to a separate p-value that we can interpret the same way as we did in the last lessons (p-value &lt; alpha = significant result - reject the \\(H_0\\)). So we can find one group significant and not the other, neither significant, or both significant with one test.\n\n\n\n\nTwo Way ANOVA Formulas\n\n\n\n13.2.1 Explained Variance\n\nIn a two-way ANOVA, we still start by computing the explained variance. Since we have an additional grouping variable, or factor, we will have two sum of squares measures, a sum of squares for factor A (SSA) and a sum of squares for factor B (SSB). Factor B is typically called a row (r) factor, while Factor A is typically called a column (c) factor. However, the order of the output will depend on the order of your formula in R.\nThe explained variance still suggests that the variation in outcome can be explained by a model, but now it will be due to the two grouping variables. In computing the SSB, we take the number of columns (\\(c\\)) and we multiply by the sum of the squared deviation between each group B mean (\\(\\bar{y_j}\\)) and the grand mean \\(\\bar{y}\\).\n\n\\(SSB = c*\\sum^r_{j=1}{(\\bar{y_j}-\\bar{y}) ^2}\\)\n\nIn computing SSA, we take the number of rows and we multiply that number by the sum of the squared deviation between each group A mean and the grand mean\n\n\\(SSA =r*\\sum^c_{i=1}{(\\bar{y_i}-\\bar{y}) ^2}\\)\n\nDegrees of freedom for the SSB are computed by the number of rows - 1 \\((r-1)\\), and the degrees of freedom for SSA are computed as the number of columns - 1 \\((c-1)\\).\n\n\\(df_{ssb} = r-1\\)\n\\(df_{ssa} = c-1\\)\n\nThe mean square due to Factor B (MSB) and Factor A (MSA) both take the value from the appropriate sum of squares (SSB and SSA) and divides that factors degrees of freedom \\((r-1)\\) or \\((c-1)\\).\n\n\\(MSB = SSB/(r-1)\\)\n\\(MSA = SSA/(c-1)\\)\n\n\n\n\n13.2.2 Unexplained Variance\n\nIn a two-way ANOVA, there is still only one measure of unexplained variance, which we term residual error.\nThe SSE is calculated after deriving the total variance (SST = both explained and unexplained), and subtracting the explained variance (SSB and SSA).\n\n\\(SSE = SST - (SSA + SSB)\\)\n\nDegrees of freedom for the SSE are computed by taking the total number of observations (\\(n_t\\)) and subtracting the number of rows (r) and the number of columns (c) and adding 1 \\((n_t-c-r+1)\\).\n\n\\(df_{sse} = n_t-c-r+1\\)\n\nThe mean square due to error (MSE) takes the SSE and divides by the appropriate degrees of freedom \\((n_t-c-r+1)\\).\n\n\\(MSE = SSE/(n_t-c-r+1)\\)\n\nThere is a total sum of squares (SST) being calculated mentioned above, which again refers to the total variance (explained and unexplained). To get this calculation, we take the squared deviation between each observation and the grand mean and we sum up all of our values.\nThere is also a total degrees of freedom in a two-way ANOVA, which is the total number of observations minus 1 (\\(n_t-1\\)). We don’t see it explicitly in our R output, but it is needed to help understand the full model.\n\n\n\n13.2.3 Calculating F-test Statistics\n\nFinally, to calculate the 2 F-test statistics, given a two-way ANOVA, we take both the MSB (explained variability for factor B) and the MSA (explained variability for factor A) and divide both by the MSE (unexplained variability) considering the appropriate degrees of freedom (\\(c-1\\) and \\(r-1\\) and \\(n_t-c-r+1\\)).\nThis gives us two scores to evaluate. A p-value is computed from each F-test statistic mathematically by using the correct command in R.\n\n\\(F1_{c-1,n_t-c-r+1 }= MSB/MSE\\)\n\\(F2_{r-1,n_t-c-r+1 }= MSA/MSE\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#example-of-a-two-way-anova",
    "href": "anova.html#example-of-a-two-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "13.3 Example of a Two-way ANOVA",
    "text": "13.3 Example of a Two-way ANOVA\n\nLet’s run an example to compare SAT scores to see if they are comparable from different instructors {Instructor 1, 2, and 3} and across 4 races {Asian.American, Black, Mexican.American, and White}.\n\n\n13.3.1 Step 1: Set Up Null and Alternative Hypothesis\n\nThe competing hypotheses for the two-way ANOVA are two-fold:\nHypothesis 1:\n\n\\(H_0: \\mu_{Instructor1} = \\mu_{Instructor2}= \\mu_{Instructor3}\\)\n\\(H_A:\\) Not all population means of SAT scores are equal among the instructors\n\nHypothesis 2:\n\n\\(H_0: \\mu_{Asian.American} = \\mu_{Black} = \\mu_{Mexical.American} =  \\mu_{White}\\)\n\\(H_A:\\) Not all population means of SAT scores are equal among races\n\n\n\n\n13.3.2 Steps 2 and 3: Compute the F-test Statistics and p-Values\n\nLike a one-way ANOVA, in a two-way ANOVA, we start with an appropriate data set.\n\nAgain, because we are using read.csv() command, we can set stringsAsFactors argument to TRUE so that the categorical variable will be coded appropriately upon download.\nThe summary() command suggests that there are two categorical variables, “Instructor” and “Race”, and one continuous variable, “SAT.”\n\n\nSATdata &lt;- read.csv(\"data/SAT.csv\", stringsAsFactors = TRUE)\nsummary(SATdata)\n\n        Instructor               Race         SAT      \n Instructor A:40   Asian.American  :30   Min.   :1190  \n Instructor B:40   Black           :30   1st Qu.:1328  \n Instructor C:40   Mexican.American:30   Median :1492  \n                   White           :30   Mean   :1468  \n                                         3rd Qu.:1592  \n                                         Max.   :1772  \n\n\n\n\n\n13.3.3 Steps 4 and 5: Interpret the Probabilities and Write a Conclusion\n\nAnova2way &lt;- aov(SAT ~ Instructor + Race, data = SATdata)\nsummary(Anova2way)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nInstructor    2    5932    2966   1.084  0.342    \nRace          3 2353300  784433 286.624 &lt;2e-16 ***\nResiduals   114  311995    2737                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nIn the ANOVA results, we find that there was no difference in SAT score based on the Instructor of record. Specifically, we find a F-test statistic of 1.084 and an associated p-value at .342. That p-value is &gt; alpha (.05), and therefore, we fail to reject the null hypothesis \\((H_0:  \\mu_{Instructor1} = \\mu_{Instructor2}= \\mu_{Instructor3})\\).\nWe also find that Race is significant, with a F-test statistic of 286.624 and an associated p-value of &lt;2e-16 ***. This means that we can reject the null hypothesis and support the alternative hypothesis (\\(H_A:\\) Not all population means of SAT scores are equal among races).\nThis means that we can conduct post-hoc tests on our Race variable. If we conduct any post-hoc tests on Instructor, we should find no differences in groups (which we already found in the ANOVA results above). This means additional testing the Instructor variable is an unnecessary step.\n\n\n\n13.3.4 Step 6: If Model is Significant, Run Post-Hoc Tests\n\nUsing the pairwise.t.test() command, we can run a Bonferroni test on just race by using the command below. This result suggest that there are group differences between all groups. We can tell that because all p-values are less than alpha at .05.\n\n\n\npairwise.t.test(SATdata$SAT, SATdata$Race, p.adj = \"bonf\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  SATdata$SAT and SATdata$Race \n\n                 Asian.American Black   Mexican.American\nBlack            &lt; 2e-16        -       -               \nMexican.American &lt; 2e-16        4.9e-09 -               \nWhite            0.0044         &lt; 2e-16 &lt; 2e-16         \n\nP value adjustment method: bonferroni \n\n\n\nUsing the TukeyHSD() Method, we do receive output for both Race and Instructor. As expected, all p-values under the instructor grouping are well above an alpha of .05, meaning not significant (Fail to reject \\(H_0\\)).\nReceiving similar results to the Bonferroni test with regards to Race, we find all group differences significant. Specifically, Black Americans, Mexican Americans, and White Americans scored lower on their SAT than Asian Americans. Mexican Americans and White Americans scored higher on their SAT than Black Americans. White Americans scored higher than Mexican Americans. Using this command, we can tell which group scored lower than the other by looking at the difference score “diff”.\n\n\nTukeyHSD(Anova2way)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = SAT ~ Instructor + Race, data = SATdata)\n\n$Instructor\n                             diff       lwr      upr     p adj\nInstructor B-Instructor A  -0.275 -28.05409 27.50409 0.9996954\nInstructor C-Instructor A -15.050 -42.82909 12.72909 0.4056475\nInstructor C-Instructor B -14.775 -42.55409 13.00409 0.4189723\n\n$Race\n                                      diff        lwr        upr     p adj\nBlack-Asian.American            -339.33333 -374.55195 -304.11472 0.0000000\nMexican.American-Asian.American -248.86667 -284.08528 -213.64805 0.0000000\nWhite-Asian.American             -46.90000  -82.11862  -11.68138 0.0040188\nMexican.American-Black            90.46667   55.24805  125.68528 0.0000000\nWhite-Black                      292.43333  257.21472  327.65195 0.0000000\nWhite-Mexican.American           201.96667  166.74805  237.18528 0.0000000",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "10.0.1 Lesson Objectives",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#lesson-objectives",
    "href": "correlation.html#lesson-objectives",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "Compute and interpret Pearson’s r correlation coefficient.\nConduct an inferential statistical test for Pearson’s r correlation coefficient.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#consider-while-reading",
    "href": "correlation.html#consider-while-reading",
    "title": "10  Correlation Analysis",
    "section": "10.2 Consider While Reading",
    "text": "10.2 Consider While Reading\n\nIn the lesson, we are looking at inference for correlation. We will learn how to conduct a correlation test. This will close the loop on scatterplots, which we learned in the Data Visualization lesson help us describe visually a relationship between variables. Consider what we learned and perhaps revisit the scatterplot lecture notes to then determine how visualizations can help in making inferences alongside regression and correlation results.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#covariance",
    "href": "correlation.html#covariance",
    "title": "10  Correlation Analysis",
    "section": "10.1 Covariance",
    "text": "10.1 Covariance\n\nCovariance (\\(s_{xy}\\) or \\(cov_{xy}\\)) is a numerical measure that describes the direction of the linear relationship between two variables, x and y and reveals the direction of that linear relationship.\nThe formula for covariance is as follows:\n\n\\(cov_{xy} = \\sum^n_{i=1}(x_i-m_x)*(y_i-m_y)/(n-1)\\)\nWhere \\(x_i\\) and \\(y_i\\) are the observed values for each observation, \\(m_x\\) and \\(m_y\\) are the mean values for each variable, \\(i\\) represents an individual observation, and \\(n\\) represents the sample size.\n\n\n\nx &lt;- c(3, 8, 5, 2)\ny &lt;- c(12, 14, 8, 4)\n\ndevX &lt;- x - mean(x)\ndevY &lt;- y - mean(y)\n\ncovXY &lt;- sum(devX * devY)/(length(x) - 1)\ncovXY\n\n[1] 8.333333\n\n# We can verify this by using cov() function in R.\ncov(x, y)\n\n[1] 8.333333",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#correlation-coefficient",
    "href": "correlation.html#correlation-coefficient",
    "title": "10  Correlation Analysis",
    "section": "10.2 Correlation Coefficient",
    "text": "10.2 Correlation Coefficient\n\nA correlation coefficient (\\(r_{xy}\\)) describes both the direction and strength of the relationship between \\(x\\) and \\(y\\). \\(r_{xy} = cov_{sy}/(s_xs_y)\\) or using the standardized formula in the book:\n\n\\(r_{xy} = \\sum^n_{i=1}(z_x*z_y)/(n-1)\\)\n\n\n\n# Calculated manually\ncovXY/(sd(x) * sd(y))\n\n[1] 0.7102387\n\n# We can verify this by using cor() function in R.\ncor(x, y)\n\n[1] 0.7102387\n\n\n\n10.2.1 Rules for the Correlation Coefficient\n\nThe correlation coefficient has the same sign as the covariance; however, its value ranges between −1 and +1 whereas \\(-1 \\le r_{xy} \\le +1\\).\nThe absolute value of the coefficient reflects the strength of the correlation. So a correlation of −.70 is stronger than a correlation of +.50.\n\n\n\n\nCorrelation Coefficient",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#rules-for-the-correlation-coefficient",
    "href": "correlation.html#rules-for-the-correlation-coefficient",
    "title": "10  Correlation Analysis",
    "section": "10.5 Rules for the Correlation Coefficient",
    "text": "10.5 Rules for the Correlation Coefficient\n\nThe correlation coefficient has the same sign as the covariance; however, its value ranges between −1 and +1 whereas \\(-1 \\le r_{xy} \\le +1\\).\nThe absolute value of the coefficient reflects the strength of the correlation. So a correlation of −.70 is stronger than a correlation of +.50.\n\n\n\n\nCorrelation Coefficient",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#scatterplots-to-visualize-relationship",
    "href": "correlation.html#scatterplots-to-visualize-relationship",
    "title": "10  Correlation Analysis",
    "section": "11.1 Scatterplots to Visualize Relationship",
    "text": "11.1 Scatterplots to Visualize Relationship\n\nLet’s do an example to first visualize the data, and then to calculate the correlation coefficient.\nFirst, read in a .csv called DebtPayments.csv. This data set has 26 observations and 4 variables:\n\nA character variable with a bunch of metropolitan areas listed;\nAn integer numeric debt;\nA numeric variable Income;\nA numeric variable Unemployment.\n\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nstr(Debt_Payments)\n\n'data.frame':   26 obs. of  4 variables:\n $ Metropolitan.area: chr  \"Washington, D.C.\" \"Seattle\" \"Baltimore\" \"Boston\" ...\n $ Debt             : int  1285 1135 1133 1133 1104 1098 1076 1045 1024 1017 ...\n $ Income           : num  103.5 81.7 82.2 89.5 75.9 ...\n $ Unemployment     : num  6.3 8.5 8.1 7.6 8.1 9.3 10.6 12.4 12.9 9.7 ...\n\n\n\nNext, plot the relationship between 2 continuous variables.\n\nThere are a few ways to write the plot command using ggplot. We went over these in the Data Visualization lesson. Again we said:\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs() command (for labels) or stat_smooth() command to generate a regression line.\n\n\n\nDebt_Payments %&gt;%\n    ggplot(aes(Income, Debt)) + geom_point(color = \"#183028\", shape = 2) +\n    stat_smooth(method = \"lm\", color = \"#789F90\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nIn the above plot, there is a strong positive relationship (upward trend) that should be confirmed with a correlation test.\nIn a second example below, we look at Unemployment as the X variable. This scatterplot is much more difficult to use in determining whether the correlation will be significant. It looks negative, but there is not a strong linear trend to the data. This will also need to be confirmed with a correlation test.\n\n\nDebt_Payments %&gt;%\n    ggplot(aes(Unemployment, Debt)) + geom_point(color = \"#183028\", shape = 2) +\n    stat_smooth(method = \"lm\", color = \"#789F90\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nIn many scatterplots using big data, the observations are too numerous to see a good relationship. In that case, the statistical test can trump this visual aid. However, in a lot of cases the scatterplot does help visualize the relationship between 2 continuous variables.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#interpreting-the-strength-of-the-correlation",
    "href": "correlation.html#interpreting-the-strength-of-the-correlation",
    "title": "10  Correlation Analysis",
    "section": "11.2 Interpreting the Strength of the Correlation",
    "text": "11.2 Interpreting the Strength of the Correlation\n\nStatisticians differ on what is called a strong correlation versus weak correlation, and it depends on the context. A .9 may be required for a strong correlation in one field, and a .5 in another. Generally speaking in business, the absolute value of a correlation .8 or above is considered strong, between .5 and .8 is considered moderate, and between a .2 and .5 is considered weak.\nThe following is consistent with what is most generally used:\n\n\n\n\nCorrelation Strength",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#cor.test-command",
    "href": "correlation.html#cor.test-command",
    "title": "10  Correlation Analysis",
    "section": "12.1 cor.test() Command",
    "text": "12.1 cor.test() Command\n\nThe cor() command gives you just the correlation coefficient. This command can be useful if you are testing many correlations at one time. In the below statement, I can use \\(cor(Variable1, Variable2)\\) to see the correlation between 2 continuous variables.\n\n\ncor(Debt_Payments$Income, Debt_Payments$Debt)\n\n[1] 0.8675115\n\n\n\nThe cor.test() command tests the hypothesis whether \\(r=0\\) or not. This command comes with a p-value and t-test statistic (along with the correlation coefficient).\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Debt)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Debt\nt = 8.544, df = 24, p-value = 9.66e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7231671 0.9392464\nsample estimates:\n      cor \n0.8675115 \n\n\n\nThis test shows a strong positive correlation of .8675 (&gt;.8) which is significant. Our p-value is 9.66e-09 or &lt; .001 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between .723 and .939.\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Unemployment)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Unemployment\nt = -3.0965, df = 24, p-value = 0.004928\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7636089 -0.1852883\nsample estimates:\n       cor \n-0.5342931 \n\n\n\nThis test shows a moderate negative correlation of -.534 (&lt;.5 and .8) which is significant. Our p-value is 0.004928 or &lt; .01 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between -.765 and -.185. This confidence interval is wider than the one listed above. This is due to the noise in the relationship we noted in the scatterplot - the correlation is weaker, the relationship does not look as linear, the confidence decreases. Even though this is true, we must note that we still found a significant correlation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "11  Linear Regression Analysis",
    "section": "",
    "text": "11.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#at-a-glance",
    "href": "regression.html#at-a-glance",
    "title": "11  Linear Regression Analysis",
    "section": "",
    "text": "In order to succeed at this lesson, you will use a statistical model, called linear regression, that helps us understand an outcome variable and also allows us to make predictions for future instances, or the next occurrence. A linear regression model is quite commonly used in many fields and is often the foundation for future more advanced analysis like machine learning. Linear regression can be a fairly easy way to design and test a research model, and in doing so allows us to gauge helpful predictor variables in understanding a phenomenon. Not only can we understand a research model, but we can also make stronger predictions on the future.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#lesson-objectives",
    "href": "regression.html#lesson-objectives",
    "title": "11  Linear Regression Analysis",
    "section": "11.2 Lesson Objectives",
    "text": "11.2 Lesson Objectives\n\nExplore the statistical model for a line.\nCompute the slope and intercept in a simple linear regression.\nInterpret a regression model significance and model fit.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#consider-while-reading",
    "href": "regression.html#consider-while-reading",
    "title": "11  Linear Regression Analysis",
    "section": "11.3 Consider While Reading",
    "text": "11.3 Consider While Reading\n\nThis lesson allows us to look at inference for correlation and inference for regression analysis. Where an ANOVA is mostly concerned about differences between means of categorical data, linear regression is mostly concerned about correctly estimating response or prediction while also forming a regression line. We will learn how to conduct a single and multiple linear regression test. This will close the loop on scatterplots, which we learned in the data visualization lesson to help us describe visually a relationship between variables. Consider what we learned and perhaps revisit the scatterplot lecture notes to then determine how visualizations can help in making inferences alongside regression and correlation results.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#overview-of-the-linear-regression",
    "href": "regression.html#overview-of-the-linear-regression",
    "title": "11  Linear Regression Analysis",
    "section": "12.1 Overview of the Linear Regression",
    "text": "12.1 Overview of the Linear Regression\n\nLinear regression analysis is a predictive modelling technique used for building mathematical and statistical models that characterize relationships between a dependent (continuous) variable and one or more independent, or explanatory variables (continuous or categorical), all of which are numerical.\nThis technique is useful in forecasting, time series modelling, and finding the causal effect between variables.\nSimple linear regression involves one explanatory variable and one response variable.\n\nExplanatory variable: The variable used to explain the dependent variable, usually denoted by X. Also known as an independent variable or a predictor variable.\nResponse variable: The variable we wish to explain, usually denoted by Y. Also known as a dependent variable or outcome variable.\n\nMultiple linear regression involves two or more explanatory variables, while still only one response variable.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#estimating-using-a-simple-linear-regression-model",
    "href": "regression.html#estimating-using-a-simple-linear-regression-model",
    "title": "11  Linear Regression Analysis",
    "section": "12.2 Estimating Using a Simple Linear Regression Model",
    "text": "12.2 Estimating Using a Simple Linear Regression Model\n\nWhile the correlation coefficient may establish a linear relationship, it does not suggest that one variable causes the other.\nWith regression analysis, we explicitly assume that the response variable is influenced by other explanatory variables.\nUsing regression analysis, we may predict the response variable given values for our explanatory variables.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#inexact-relationships",
    "href": "regression.html#inexact-relationships",
    "title": "11  Linear Regression Analysis",
    "section": "12.3 Inexact Relationships",
    "text": "12.3 Inexact Relationships\n\nIf the value of the response variable is uniquely determined by the values of the explanatory variables, we say that the relationship is deterministic.\n\nBut if, as we find in most fields of research, that the relationship is inexact due to omission of relevant factors, we say that the relationship is inexact.\nIn regression analysis, we include a stochastic error term, that acknowledges that the actual relationship between the response and explanatory variables is not deterministic.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#regression-as-anova",
    "href": "regression.html#regression-as-anova",
    "title": "11  Linear Regression Analysis",
    "section": "12.4 Regression as ANOVA",
    "text": "12.4 Regression as ANOVA\n\nANOVA conducts an F - test to determine whether variation in Y is due to varying levels of X.\nANOVA is used to test for significance of regression:\n\n\\(H_0\\): population slope coefficient \\(\\beta_1\\) \\(=\\) 0\n\\(H_A\\): population slope coefficient \\(\\beta_1\\) \\(\\neq\\) 0\n\nR reports the p-value (Significant F).\nRejecting \\(H_0\\) indicates that X explains variation in Y.\n\n\n\n\nLinear Regression Models",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#the-simple-linear-regression-model",
    "href": "regression.html#the-simple-linear-regression-model",
    "title": "11  Linear Regression Analysis",
    "section": "12.5 The Simple Linear Regression Model",
    "text": "12.5 The Simple Linear Regression Model\n\nThe simple linear regression model is defined as \\(y= \\beta_0+\\beta_1 𝑥+\\varepsilon_1\\), where \\(y\\) and \\(x\\) are the response and explanatory variables, respectively and \\(\\varepsilon_1\\) is the random error term. \\(\\beta_0\\) and \\(\\beta_1\\) are the unknown parameters to be estimated.\nSometimes, this equation can be represented using different variable names like \\(y=mx+b\\) This is the same equation as above, but different notation.\nBy fitting our data to the model, we obtain the equation \\(\\hat{y} = b_0 + b_1*x\\), where \\(\\hat{y}\\) is the estimated response variable, \\(b_0\\) is the estimate of \\(\\beta_0\\) (Intercept) and \\(b_1\\) is the estimate of \\(\\beta_1\\) (Slope).\nSince the predictions cannot be totally accurate, the difference between the predicted and actual value represents the residual \\(e=y-\\hat{y}\\).\n\n\n\n\nRegression Equation",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#the-least-squares-estimates",
    "href": "regression.html#the-least-squares-estimates",
    "title": "11  Linear Regression Analysis",
    "section": "12.6 The Least Squares Estimates",
    "text": "12.6 The Least Squares Estimates\n\nThe two parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by minimizing the sum of squared residuals.\n\nThe slope coefficient \\(\\beta_1\\) is estimated as \\(b_1 = \\sum(x_i-\\bar{x}*y_i-\\bar{y})/\\sum(x_i-\\bar{x})^2\\)\nThe intercept parameter \\(\\beta_0\\) is estimated as \\(b_0 = \\hat{y}-b_1*\\bar{x}\\)\nAnd we use this information to make the regression equation given the formula above: \\(\\hat{y} = b_0 + b_1*x\\),",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#goodness-of-fit",
    "href": "regression.html#goodness-of-fit",
    "title": "11  Linear Regression Analysis",
    "section": "12.7 Goodness of Fit",
    "text": "12.7 Goodness of Fit\n\nGoodness of fit refers to how well the data fit the regression line. I will introduce three measures to judge how well the sample regression fits the data.\n\nStandard Error of the Estimate\nThe Coefficient of Determination (\\(R^2\\))\nThe Adjusted \\(R^2\\)\n\nIn order to make sense of the goodness of fit measures, we need to go back to the idea of explained and unexplained variance we learned in Chapter 7. Variance can also be known as a difference.\n\nUnexplained Variance = SSE or Sum of Squares Error: This equals the sum of squared difference between A and B, or between the sum of squares of the difference between observation (\\(y_i\\)) and our predicted value of y (\\(\\hat{y}\\)).\n\n\\(SSE = \\sum^n_{i=1}(y_i - \\hat{y})^2\\)\n\nExplained Variance = SSR or Sum of Squares Regression: This equals the sum of squared difference between B and C, or between the sum of squares of the difference between our predicted value (\\(\\hat{y}\\)) and the mean of y (\\(\\bar{y}\\)).\n\n\\(SSR = \\sum^n_{i=1}(\\hat{y} - \\bar{y})^2\\)\n\nTotal Variance = SST or Total Sum of Squares: This equals the sum of squared difference between A and C, or between the sum of squares of the difference between observation (\\(y_i\\)) and the mean of y (\\(\\bar{y}\\)).\n\nThe SST can be broken down into two components: the variation explained by the regression equation (the regression sum of squares or SSR) and the unexplained variation (the error sum of squares or SSE).\n\\(SST = \\sum^n_{i=1}(y_i - \\bar{y})^2\\)\n\n\n\n\n\nVisualization of Sum of Squares (SS)\n\n\n\n12.7.1 The Standard Error of the Estimate\n\nThe Standard Error of the Estimate, also known as the Residual Standard Error (RSE) (and labelled such in R), is the variability between observed (\\(y_i\\)) and predicted (\\(\\hat{y}\\)) values, targeting the unexplained variance in the figure above. This measure is a lack of fit of the model to the data.\nTo compute the standard error of the estimate, we first compute the SSE and the MSE.\n\nSum of Squares Error: \\(SSE = \\sum^n_{i=1}e^2_i = \\sum^n_{i=1}(y_i - \\hat{y})^2\\)\nDividing SSE by the appropriate degrees of freedom, n – k – 1, yields the mean squared error:\n\nMean Squared Error: \\(MSE = SSE/(n-k-1)\\)\n\nThe square root of the MSE is the standard error of the estimate, se.\n\nStandard Error of Estimate: \\(se = \\sqrt(MSE) = \\sqrt(SSE/(n-k-1))\\)\n\n\n\n\n\n12.7.2 R Output\n\nWe do not see the SSE or the SST in our R output, but we will see the Standard Error of the Estimate labelled Residual Standard Error. We will look at all these measures together in R output later on in the lecture.\n\n\n\n12.7.3 The Coefficient of Determination (\\(R^2\\))\n\nCoefficient of determination refers to the percentage of variance in one variable that is accounted for by another variable or by a group of variables.\nThis measure of R-squared is a measure of the “fit” of the line to the data.\n\\(R^2\\) = 1-SSE/SST where \\(SSE = \\sum^n_{i=1}(y_i - \\hat{y})^2\\) and \\(SST = \\sum^n_{i=1}(y_i - \\bar{y})^2\\)\n\n\\(R^2\\) can also be computed by SSR/SST, which provides the same answer as above.\nThis measure of fit targets both the unexplained and the explained variance.\n\\(R^2\\) can also be calculated using the formula for Pearson’s r and squaring it. This gives us the same answer.\n\nPearson’s \\(r\\) is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions.\n\n(Pearson’s \\(r\\))\\(^2\\) = \\(r^2_{xy} = (cov_{xy}/(s_x*s_y))^2\\).\n\nYou need to see how all these formulas relate to see why all these formulas give you the same answer.\n\n\n\n12.7.4 Interpreting (\\(R^2\\))\n\nThis measure is easier to interpret over standard error. In particular, the (\\(R^2\\)) quantifies the fraction of variation in the response variable that is explained by changes in the explanatory variables.\nThe (\\(R^2\\)) gives you a score between 0 and 1. A score of 1 indicates that your Y variable is perfectly explained by your X variable or variables (where we only have one in simple regression).\nThe \\(R^2\\) measure has strength of determination like the correlation coefficient, only all measures are from 0 to 1.\n\nThe closer you get to one, the more explained variance we achieve.\nA score of 0 indicates that no variance is explained by the X variable or variables. In this case, the variable would not be a significant predictor variable of Y.\nAgain, the strength of the relationship varies by field, but generally, a .2 to a .5 is considered weak, a .5 to a .8 is considered moderate, and above a .8 is considered strong.\n\n\n\n\n12.7.5 R Output\n\nWe do see the \\(R^2\\) in our R output labelled Multiple R-squared.\n\n\n\n12.7.6 Limitations of The Coefficient of Determination (\\(R^2\\))\n\nMore explanatory variables always result in a higher \\(R^2\\).\nBut some of these variables may be unimportant and should not be in the model.\nThis is only applicable with multiple regression, discussed below.\n\n\n\n12.7.7 The Adjusted \\(R^2\\)\n\nThe Adjusted \\(R^2\\) tries to balance the raw explanatory power against the desire to include only important predictors.\n\nAdjusted \\(R^2 = 1-(1-R^2)*((n-1)/(n-k-1))\\).\nThe Adjusted \\(R^2\\) penalizes the \\(R^2\\) for adding additional explanatory variables.\n\n\n\n\n12.7.8 R Output\n\nWith our other goodness-of-fit measures, we typically allow the computer to compute the Adjusted \\(R^2\\) using commands in R.\n\nTherefore, we do see the Adjusted \\(R^2\\) in our R output labelled Adjusted R-squared.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#example-of-simple-linear-regression-in-r",
    "href": "regression.html#example-of-simple-linear-regression-in-r",
    "title": "11  Linear Regression Analysis",
    "section": "12.8 Example of Simple Linear Regression in R",
    "text": "12.8 Example of Simple Linear Regression in R\n\nThe broad steps are the same as we used in Chapter 6 and 7 when setting up the t-tests and ANOVA hypothesis: 1. set up the hypothesis 2. compute the Test Statistic, 3. calculate probability 4. interpret and 5. write a conclusion.\nStep 1: Set Up the Null and Alternative Hypothesis\n\\(H_0\\): The slope of the line is equal to zero.\n\\(H_A\\): The slope of the line is not equal to zero.\nStep 2 and 3: Compute the Test Statistic and Calculate Probability\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nSimple &lt;- lm(Debt ~ Income, data = Debt_Payments)\nsummary(Simple)\n\n\nCall:\nlm(formula = Debt ~ Income, data = Debt_Payments)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-107.087  -38.767   -5.828   50.137  101.619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  210.298     91.339   2.302   0.0303 *  \nIncome        10.441      1.222   8.544 9.66e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.26 on 24 degrees of freedom\nMultiple R-squared:  0.7526,    Adjusted R-squared:  0.7423 \nF-statistic:    73 on 1 and 24 DF,  p-value: 9.66e-09\n\n\n\nSteps 4 and 5: Interpret the Probability and Write a Conclusion\nWe interpret the probability and write a conclusion after looking at the following:\n\n\nGoodness of Fit Measures\nThe F-test statistic\nThe p-value from the hypothesis\n\n\nWe then can interpret the hypothesis and make the regression equation if needed to make predictions.\n\n\n12.8.1 Examining the Goodness of Fit Measures\n\nLike the ANOVA, we need the summary() command to see the regression output.\nThis output includes the following:\n\nStandard Error of Estimate at 63.26\na \\(R^2\\) of 0.7526.\nan Adjusted \\(R^2\\) of 0.7423\n\nWe do not really see a difference in \\(R^2\\) and the Adjusted \\(R^2\\) because we only have a simple linear regression, which includes one X. The other thing that could create a difference between these measures is having a small sample size. A small sample size could adjust the \\(R^2\\) down (like it did a little here \\(n=28\\)). In both cases, a score around .75 indicates that we are explaining about 75% of the variance in Debt Payments, which is specifically stated as 75.26% in the \\(R^2\\) value.\nThe Standard Error of Estimate - known in R as the RSE or Residual Standard Error at 63.26. Before we interpret this, we need to know the scale, and income is in 1000s. Therefore, for every 1000 dollars difference in Income we could be off on our debt payments by 63.26 dollars.\nAgain, most statisticians interpret the \\(R^2\\) and the adjusted \\(R^2\\) over the RSE.\n\n\n\n12.8.2 Examining the F-Test Statistic\n\nThe F-statistic is a ratio of explained information (in the numerator) to unexplained information (in the denominator). If a model explains more than it leaves unexplained, the numerator is larger and the F-statistic is greater than 1. F-statistics that are much greater than 1 are explaining much more of the variation in the outcome than they leave unexplained. Large F-statistics are more likely to be statistically significant.\nWe see a F-statistic in our output with a p-value of 9.66e-09, which is less than .05. This indicates that our overall model is significant, which in simple regression means that our one X predictor variable is also significant. If this F-statistic is significant, we can interpret the hypothesis.\n\n\n\n12.8.3 Examining the Hypothesis\n\nWe see a p-value on the Income row of 9.66e-09 from a t-value. This is also significant at .05 level. This p-value is the same as the F-test statistic because we only have one X variable.\nThis significance means our predictor variable does influence the Y variable and that we can reject the null hypothesis and show support for our alternative hypothesis.\n\n\n\n12.8.4 Interpreting the Hypothesis\n\nA \\(b_1\\) estimate of 10.441 indicates that for 1000 dollars of Income (again our data is in 1000s), the payment of dept will increase by 10.441.\nA \\(b_0\\) estimate of 210.298 indicates where the regression line will start given a Y at 0.\n\nThis gives us a regression equation at \\(\\hat{y} = 210.298 + 10.441*Income\\)\nWe can graph this regression line using the abline() command on our plot we did earlier.\nAs you can see from the chart, there was no data at the Y intercept, but if I extend the chart out, it does hit the y axis at 210 like stated.\nAlso from the chart, we note that for each unit of income we go to the right, we go up by 10.441 units in dept payments.\n\n\n\nplot(Debt ~ Income, data = Debt_Payments, xlim = c(0, 120), ylim = c(0,\n    1350))\nabline(Simple)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#predictions",
    "href": "regression.html#predictions",
    "title": "11  Linear Regression Analysis",
    "section": "12.9 Predictions",
    "text": "12.9 Predictions\n\nWe can calculate this using the regression equation or use the predict() command in order to calculate different values of y given values of x based on our regression equation we got in the step above.\nThe coef() function returns the model’s coefficients which are needed to make the regression equation.\n\n\n# What would be your debt payments if Income was 100 ( for 100,000)\ncoef(Simple)\n\n(Intercept)      Income \n  210.29768    10.44111 \n\n210.298 + 10.441 * (100)\n\n[1] 1254.398\n\npredict(Simple, data.frame(Income = 100))\n\n       1 \n1254.408",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#example-of-multiple-linear-regression-in-r",
    "href": "regression.html#example-of-multiple-linear-regression-in-r",
    "title": "11  Linear Regression Analysis",
    "section": "13.1 Example of Multiple Linear Regression in R",
    "text": "13.1 Example of Multiple Linear Regression in R\n\nThe steps to multiple linear regression are the same as simple linear regression except we should have more than one hypothesis.\n\n\nStep 1: Set up the null and alternative hypotheses\n\n\nHypothesis 1: Income affects Debt Payments.\n\n\\(H_0\\): The slope of the line in regards to Income is equal to zero.\n\\(H_A\\): The slope of the line in regards to Income is not equal to zero.\n\nHypothesis 2: Unemployment affects Debt Payments.\n\n\\(H_0\\): The slope of the line in regards to Unemployment is equal to zero.\n\\(H_A\\): The slope of the line in regards to Unemployment is not equal to zero.\n\n\n\nStep 2 and 3: Compute the test statistic and calculate probability\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nMultiple &lt;- lm(Debt ~ Income + Unemployment, data = Debt_Payments)\nsummary(Multiple)\n\n\nCall:\nlm(formula = Debt ~ Income + Unemployment, data = Debt_Payments)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.456  -38.454   -5.836   51.156  102.121 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  198.9956   156.3619   1.273    0.216    \nIncome        10.5122     1.4765   7.120 2.98e-07 ***\nUnemployment   0.6186     6.8679   0.090    0.929    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 64.61 on 23 degrees of freedom\nMultiple R-squared:  0.7527,    Adjusted R-squared:  0.7312 \nF-statistic:    35 on 2 and 23 DF,  p-value: 1.054e-07\n\n\n\nSteps 4 and 5: Interpret the probability and write a conclusion\n\n\nWe interpret the probability and write a conclusion after looking at the following:\n\nGoodness of Fit Measures\nThe F-test statistic\nThe p-values from the hypotheses\n\nWe then can interpret the hypotheses and make the regression equation if needed to make predictions.\n\n\n13.1.1 Examining the Goodness of Fit Measures\n\nLike the ANOVA, we need the summary() command to see the regression output.\nThis output includes the following:\n\nStandard error of estimate at 64.61 - was previously 63.26 with simple regression.\na \\(R^2\\) of 0.7527 - was previously 0.7526 with simple regression.\nan Adjusted \\(R^2\\) of 0.7312 - was previously 0.7423 with simple regression.\n\nLooking at the goodness of fit indices, they suggest that we are not explaining anything more by adding the unemployment variable over what we had with the Income variable. Our RSE and \\(R^2\\) are about the same, and our adjusted \\(R^2\\) has gone down - again paying the price for including an unnecessary variable.\n\n\n\n13.1.2 Examining the F-Test Statistic\n\nWe see a overall F-statistic in our output with a p-value of 1.054e-07, which is less than .05. This indicates that our overall model is significant - which in multiple regression means that at least one X predictor variable is significant. If this F-statistic is significant, we can interpret the hypotheses.\n\n\n\n13.1.3 Examining the Hypotheses\n\nHypothesis 1: The output shows a p-value on the Income row of 2.98e-07 from a t-value. This was significant in our simple regression model, and is still significant at .05 level here. This significance means our predictor variable does influence the Y variable and that we can reject the null hypothesis and show support for our alternative hypothesis.\nHypothesis 2: The output shows a p-value on the Unemployment row of 0.929 from a t-value. This is NOT significant at any appropriate level of alpha. This lack of significance means our predictor variable does NOT influence the Y variable and that we fail to reject the null hypothesis that there is a slope.\n\n\n\n13.1.4 Interpreting the Hypothesis\n\nBecause Unemployment is not significant, we should drop it from the model before creating the regression equation. The extra variable is only adding noise to our model and not adding anything useful in understanding debt payments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "This book is designed to complement a traditional probability and statistics course. We went over information to make sure you had some basics to really start learning R. This journey began with the basics of R, emphasizing that while multiple methods exist to achieve the same results in R, it’s crucial to find the approach that works best for you. As we learn R, you will get used to doing things your way to be able to slice and evaluate the data to find rich information from the data sets we look at. As long as the data was handled properly, it does not matter how we reach our goal using R as long as we do it ourselves.\nMastering R allows you to effectively clean, analyze, and interpret data, unlocking valuable insights from various datasets. We explored the essential practice of data cleaning, learning various techniques and popular functions within the dplyr package under the tidyverse. Proper data cleaning ensures the integrity and accuracy of your analysis. We delved into skewness, kurtosis, variables, and scales of measurement, focusing on summarizing qualitative and quantitative data. Visualizations were introduced as powerful tools to describe variables and uncover patterns in the data.\nWe examined basic probability rules were covered alongside binomial and continuous distributions. We examined the normal distribution, its limitations, and methods for transforming non-normal variables.\nWe learned how to set up null and alternative hypotheses, and the different types of hypotheses tests (two-tailed, right-tailed, and left-tailed). We discussed three t-tests: one-sample, independent samples, and dependent samples, interpreting results through the p-value approach. Note that the assumptions for t-tests were not covered and are reserved for a future discussion.\nWe used contingency tables to calculate probabilities with categorical variables and computed chi-squared statistics for tests of independence and goodness of fit.\nThe concepts of Type I and Type II errors were introduced, emphasizing their reduction. We covered one-way and two-way ANOVA, including conducting and evaluating post-hoc tests. We learned how to write and interpret simple and multiple linear regression models, including goodness of fit measures and hypothesis testing using regression. The lm() command was crucial for interpreting summary output, and we explored using both continuous and categorical variables as predictors.\nThis book has equipped you with the foundational skills to handle and analyze data using R. Remember, the key to mastering these techniques lies in consistent practice and finding the methods that best suit your analytical style.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability And Business Statistics",
    "section": "",
    "text": "1 Installing R and R Studio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#installing-r-on-windows",
    "href": "index.html#installing-r-on-windows",
    "title": "Probability And Business Statistics",
    "section": "1.1 Installing R on Windows",
    "text": "1.1 Installing R on Windows\n\nOpen an internet browser and go to https://www.r-project.org/.\nClick CRAN hyperlink underneath Download on the left.\nSelect a CRAN location (a mirror site) and click the corresponding link. I selected O-cloud. This brings you to the following link: https://cloud.r-project.org/\nClick on the “Download R for Windows” link at the top of the page if you have a Windows computer.\n\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer.\nRun the .exe file\nAccept all defaults and follow the installation instructions\nNow that R is installed, you need to download and install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#installing-r-on-a-mac",
    "href": "index.html#installing-r-on-a-mac",
    "title": "Probability And Business Statistics",
    "section": "1.2 Installing R on a Mac",
    "text": "1.2 Installing R on a Mac\n\nOpen an internet browser and go to https://www.r-project.org/.\nClick CRAN hyperlink underneath Download on the left.\nSelect a CRAN location (a mirror site) and click the corresponding link. I selected O-cloud. This brings you to the following link: https://cloud.r-project.org/\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nAccept the license agreement and all subsequent defaults\nWhen the installation completes, click Close.\nNow that R is installed, you need to download and install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#downloading-rstudio-for-mac-or-pc",
    "href": "index.html#downloading-rstudio-for-mac-or-pc",
    "title": "Probability And Business Statistics",
    "section": "1.3 Downloading RStudio For Mac or PC",
    "text": "1.3 Downloading RStudio For Mac or PC\n\nGo to the following URL: https://www.rstudio.com/products/rstudio/download/\nClick the Download button for Install RStudio.\nIt should recognize your operating system. If you see different an Installers for Supported Platforms section, click the version that is appropriate for your operating system. When the download is complete, run the install program, accepting all defaults.\nNote, RStudio might conflict with some Mac antivirus programs. Ensure your antivirus is not blocking it from installing. Only worry about this if you have difficulty installing the program.\n\n\n\n\nRStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#setting-options-to-your-liking",
    "href": "index.html#setting-options-to-your-liking",
    "title": "Probability And Business Statistics",
    "section": "5.1 Setting Options to Your Liking",
    "text": "5.1 Setting Options to Your Liking\n\nYou have many options to customize R Studio.\nGo to Tools &gt; Global Options to see the list of options for you to customize.\nA few are listed below.\n\nYou can set your general information including your default working directory (when not in a project).\nYou can customize the appearance to a theme that accommodates your learning style and visual preferences.\nYou can turn on a spell check.\n\n\n\n\n\nTools &gt; Global Options.R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#quick-keys-in-r",
    "href": "index.html#quick-keys-in-r",
    "title": "Probability And Business Statistics",
    "section": "5.2 Quick Keys in R",
    "text": "5.2 Quick Keys in R\n\nThere are a lot of quick keys in R to make you able to use it faster and more effectively. You may look over these and try on your own.\n\n\n\n\nConsole Quick Keys\n\n\n\n\n\nSource Quick Keys\n\n\n\n\n\nEditing Quick Keys",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#getting-help-in-r",
    "href": "index.html#getting-help-in-r",
    "title": "Probability And Business Statistics",
    "section": "5.3 Getting Help in R",
    "text": "5.3 Getting Help in R\n\nThere are lots of ways to get help in R.\nIn R, use the help search box to find information on a function, parameter, or package.\n\n?mean\nhelp.search(‘swirl’)\nhelp(package = ‘MASS’)\n?install.packages\n\n\n\n\n\nGetting Help\n\n\n\nYou should try to look up the tapply command to see what it does.\n\nUse ?tapply in your .R file to pull up tapply() command or type tapply in the Help box. * Formally, you should see that the command applies a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors. This means that the command does some math calculation (mean, sum, etc.) on a continuous variable after dividing the data by group.\n\nThe format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean.\nMore on that function later in Chapter 1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "introR.html#what-is-statistics",
    "href": "introR.html#what-is-statistics",
    "title": "2  Introduction to R and RStudio",
    "section": "2.1 What is Statistics?",
    "text": "2.1 What is Statistics?\n\nStatistics is the methodology of extracting useful information from a data set.\nNumerical results are not very useful unless they are accompanied with clearly stated actionable business insights.\nTo do good statistical analysis, you must do the following:\n\nFind the right data.\nUse the appropriate statistical tools.\nClearly communicate the numerical information in written language.\n\nWith knowledge of statistics:\n\nAvoid risk of making uninformed decisions and costly mistakes.\nDifferentiate between sound statistical conclusions and questionable conclusions.\n\nData and analytics capabilities have made a leap forward.\n\nGrowing availability of vast amounts of data.\nImproved computational power.\nDevelopment of sophisticated algorithms.\n\n\n\n2.1.1 Two Main Branches of Statistics\n\nDescriptive Statistics - collecting, organizing, and presenting the data.\nInferential Statistics - drawing conclusions about a population based on sample data from that population.\n\nA population consists of all items of interest.\nA sample is a subset of the population.\nA sample statistic is calculated from the sample data and is used to make inferences about the population parameter.\n\nReasons for sampling from the population:\n\nToo expensive to gather information on the entire population.\nOften impossible to gather information on the entire population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#setting-up-r",
    "href": "introR.html#setting-up-r",
    "title": "2  Introduction to R and RStudio",
    "section": "2.2 Setting up R",
    "text": "2.2 Setting up R\n\n2.2.1 Why R?\n\nR is a very sophisticated statistical software that allows you to enter commands one-at-a-time, or write scripts using the R language.\nEasily installed, state-of-the-art, and it is free and open source and supported by a well-established R Community.\nR can be used with RStudio, which is a graphical user interface that allows you to do the following:\n\nwrite, edit, and execute code;\ngenerate, view, and store plots;\nmanage files, objects and data frames;\nintegrate with version control systems.\n\nR comes with community that helps in the development of R resources.\n\nA package is developed by R users to do one specific thing or a set of related things that could store a collection of functions, data, and code.\nA library is the place where the package is located on your computer.\n\nA repository is a central location where many developed packages are located and available for download. There are 3 big repositories, but we use Comprehensive R Archive Network, or CRAN, which is R’s main repository with over 18,000 packages available.\nR’s community is vast, and you can always seek information from the community to try to help you with a R related issue.\nR is Called a Dynamically Typed Language\nIn R, a variable itself is not declared of any data type.\nRather it receives the data type of the R object that is assigned to it.\nWe can change a variable’s data type if we want, but it will inherit one based on the object assigned to it. We will learn some common ways to do this in the data preparation section.\n\n\n\n2.2.2 Creating a Project for Our Class\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\n\nR projects are a type of file which function with RStudio.\nThey have the .Rproj file extension.\nR projects are each associated with a directory.\nThey are useful when working with many files for one purpose, hence the name “project”\nA great feature is that they “know” which files are relevant to a project, when you open the project RStudio will load those files automatically.\n\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this program!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#creating-a-project-for-our-class",
    "href": "introR.html#creating-a-project-for-our-class",
    "title": "2  Introduction to R and RStudio",
    "section": "2.3 Creating a Project for Our Class",
    "text": "2.3 Creating a Project for Our Class\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\n\nR projects are a type of file which function with RStudio.\nThey have the .Rproj file extension.\nR projects are each associated with a directory.\nThey are useful when working with many files for one purpose, hence the name “project”\nA great feature is that they “know” which files are relevant to a project, when you open the project RStudio will load those files automatically.\n\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this program!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).\n\n \n\n2.3.1 R Script Files\n\nUsing R Script Files:\n\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nUsing the Console\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you must re-enter them at the command line. Nothing saves for later.\n\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise.R script files help to solve this issue.\n\n\n\n2.3.2 Create a New R Script File: Chapter1.R\n\nTo save your notes from today’s lecture, create a .R file named Chapter1.R and save it to your project file you made in the last class.\nThere are a couple of parts to this chapter, and we can add code from today’s chapter in one file so that our code is stacked nicely together.\n\nFor each new chapter, start a new file and save it to your project folder.\n\n\n\n\nScreenshot of R Environment\n\n\n\n\n2.3.3 Using a Prolog\n\nWe should include a prolog to introduce each R Script File during the course.\nA prolog is a set of comments at the top of a code file that provides information about what is in the file.\nIncluding a prolog is considered coding best practice.\nIt also names the files and resources used that facilitates identification.\nAn informal prolog is below:\n\n\n####################################\n# Project name: \n# Project purpose: \n# Code author name: \n# Date last edited: \n# Data used: \n# Libraries used: \n####################################\n\n\nOn your R Script File, add your own prolog following the template as shown.\nI like to add a line for Data used and Libraries used so we know what all we used in the script.\n\n\n####################################\n# Project name: Chapter 1\n# Project purpose: To create an R script file to learn about R. \n# Code author name: Pamela Schlosser\n# Date last edited: [Enter Date Here]\n# Data used: NA\n# Libraries used: NA\n####################################\n\n\nThen, as we work through our .R script and add data files or libraries to our code, we go back and edit the prolog.\n\n\n\n2.3.4 R Handles Text in Multiple Ways\n\nR can generally use single quotes or double quotes when marking text. However, if you use a single quote to start, use a single quote to end. The same for double quotes - ensure the pairing is the same quote type.\nYou sometimes need to be careful with nested quotes, but generally it does not matter which you use.\n\n\n\"This is a string\"\n\n[1] \"This is a string\"\n\n\"This is also a string\"\n\n[1] \"This is also a string\"\n\n\n\nWe can also add comments to our code to document our work and add notes to our self or to others.\n\n\n# This is a comment for documentation or annotation\n\n\nAdd the code above to your R file and run each line using Ctl + Enter or select all lines and click Run.\nTake note that nothing prints in the console after running a comment.\n\n\n\n\nChapter 1 R File\n\n\n\n\n2.3.5 Using Comments\n\nWe use comments to organize and explain code in our R Script file.\nBe sure to write clear code that does not need a lot of comments.\nInclude useful comments where needed so that anyone (including yourself in the future) can run and understand your code.\nIf something does not work, don’t delete it yet. Instead, comment it out while you troubleshoot it or to try different alternatives.\nNotice the prolog above is in comments.\n\n\n\n2.3.6 Note on R Markdown\n\nThese files were formatted with RMarkdown. RMarkdown is a simple formatting syntax for authoring documents of a variety of types, including PowerPoint and html files.\nOn the document, RMarkdown prints the command and then follows the command with the output after 2 hashtags.\nIn your R Script File, you only need to type in the command and then run your code to get the same output as presented here.\n\n\n\n\nReading our HTML file\n\n\n\n\n2.3.7 R is an Interactive Calculator\n\nAn important facet of R is that it should serve as your sole calculator.\nTry these commands in your .R file by typing them in and clicking Ctr + Enter on each line.\n\n\n3 + 4\n\n[1] 7\n\n3 * 4\n\n[1] 12\n\n3/4\n\n[1] 0.75\n\n3 + 4 * 100^2\n\n[1] 40003\n\n\n\nTake note that order of operations holds in R: PEMDAS\n\nParentheses ()\nExponents ^ and \\(**\\)\nDivision \\(/\\), Multiplication \\(*\\), modulo, and integer division\nAddition + and Subtraction -\n\nNote that modulo and integer division have the same priority level as multiplication and division, where modulo is just the remainder.\n\n\nprint(2 + 3 * 5 - 7^2%%4 + (5/2))\n\n[1] 18.5\n\n5/2  #parentheses: = 2.5\n\n[1] 2.5\n\n7^2  #exponent:= 49\n\n[1] 49\n\n3 * 5  #multiplication: = 15\n\n[1] 15\n\n17%%4  #modulo: = 1\n\n[1] 1\n\n17%/%4  #integer division: = 4\n\n[1] 4\n\n2 + 15  #addition: = 17\n\n[1] 17\n\n17 - 1  #subtraction: = 16\n\n[1] 16\n\n16 + 2.5  #addition: = 18.5\n\n[1] 18.5\n\n\n\n\n2.3.8 Running Commands\n\nThere are a few ways to run commands via your .R file.\n\nYou can click Ctr + Enter on each line.\nYou can select all the lines you want to run and select Ctr + Enter.\nYou can select all the lines you want to run and select the run button as shown in the Figure.\n\n\n\n\n\nRun Code\n\n\n\nNow that I have asked you to add a couple lines of code, after this point, when R code is shown on this file, you should add it to your .R script file along with any notes you want. I won’t explicitly say - “add this code.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#observations-and-variables",
    "href": "introR.html#observations-and-variables",
    "title": "2  Introduction to R and RStudio",
    "section": "2.4 Observations and Variables",
    "text": "2.4 Observations and Variables\n\nGoing back to the basics in statistics, we need to define an observation and variable so that we can know how to use them effectively in R in creating objects.\nAn Observation is a single row of data in a data frame that usually represents one person or other entity.\nA Variable is a measured characteristic of some entity (e.g., income, years of education, sex, height, blood pressure, smoking status, etc.).\nIn data frames in R, the columns are variables that contain information about the observations (rows).\n\nNote that we will break this code down later.\n\n\nincome &lt;- c(34000, 123000, 215000)\nvoted &lt;- c(\"yes\", \"no\", \"no\")\nvote &lt;- data.frame(income, voted)\nvote\n\n  income voted\n1  34000   yes\n2 123000    no\n3 215000    no\n\n\nObservations: People being measured.\nVariables: Information about each person (income and voted).\n\n\n# Shows the number of columns or variables\nncol(vote)\n\n[1] 2\n\n# Shows the number of rows or observations\nnrow(vote)\n\n[1] 3\n\n# Shows both the number of rows (observations and columns\n# (variables).\ndim(vote)\n\n[1] 3 2\n\n\n\n2.4.1 The Assignment Operator in Creating Objects\n\nEntering and Storing Variables in R requires you to make an assignment.\n\nWe use the assignment operator ‘&lt;-’ to assign a value or expression to a variable.\nWe typically do not use the = sign in R even though it works because it also means other things in R.\n\nSome examples are below to add to your .R file.\n\n\nstates &lt;- 29\nA &lt;- \"Apple\"\n# Equivalent statement to above - again = is less used in R.\nA = \"Apple\"\nprint(A)\n\n[1] \"Apple\"\n\n# Equivalent statement to above\nA\n\n[1] \"Apple\"\n\nB &lt;- 3 + 4 * 12\nB\n\n[1] 51\n\n\n\n\n\nThe Assignment Operator\n\n\n\n\n2.4.2 Naming Objects\n\nLine length limit: 80\nAlways use a consistent way of annotating code.\nCamel case is capitalizing the first letter of each word in the object name, with the exception of the first word.\nDot case puts a dot between words in a variable name while camel case capitalizes each word in the variable name.\nObject names appear on the left of assignment operator. We say an object receives or is assigned the value of the expression on the right.\n\n\nNaming Constants: A Constant contains a single numeric value.\n\n\nThe recommended format for constants is starting with a “k” and then using camel case. (e.g., kStates).\n\n\nNaming Functions: Functions are objects that perform a series of R commands to do something in particular.\n\n\nThe recommended format for Functions is to use Camel case with the first letter capitalized. (e.g., MultiplyByTwo).\n\n\nNaming Variables: A Variable is a measured characteristic of some entity.\n\n\nThe recommended format for variables is to use either the dot case or camel case. e.g., filled.script.month or filledScriptMonth.\nA valid variable name consists of letters, numbers, along with the dot or underline characters.\nA variable name must start with a letter, or the dot when not followed by a number.\nA variable cannot contain spaces.\nVariable names are case sensitive: x is different from X just as Age is different from AGE.\nThe value on the right must be a number, string, an expression, or another variable.\nSome Examples Using Variable Rules:\n\n\nAB.1 &lt;- \"Allowed?\"\n# Does not follow rules - not allowed Try the statement below with no\n# hashtag to see the error message .123 &lt;- 'Allowed?'\nA.A123 &lt;- \"Allowed?\"\nG123AB &lt;- \"Allowed?\"\n# Recommended format for constants\nkStates &lt;- 29\n\n\nDifferent R coders have different preferences, but consistency is key in making sure your code is easy to follow and for others to read. In this course, we will generally use the recommendation in the text which are listed above.\nWe tend to use one letter variable names (i.e., x) for placeholders or for simple functions (like naming a vector).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#built-in-functions",
    "href": "introR.html#built-in-functions",
    "title": "2  Introduction to R and RStudio",
    "section": "2.5 Built-in Functions",
    "text": "2.5 Built-in Functions\n\nR has thousands of built-in functions including those for summary statistics. Below, we use a few built-in functions with constant numbers. The sqrt(), max(), and min() functions compute the square root of a number, and find the maximum and minimum numbers in a vector.\n\n\nsqrt(100)\n\n[1] 10\n\nmax(100, 200, 300)\n\n[1] 300\n\nmin(100, 200, 300)\n\n[1] 100\n\n\n\nWe can also create variables to use within built-in functions.\nBelow, we create a vector x and use a few built-in functions as examples.\n\nThe sort() function sorts a vector from small to large.\n\n\nx &lt;- c(1, 2, 3, 3, 100, -10, 40)  #Creating a Vector x\nsort(x)  #Sorting the Vector x from Small to Large\n\n[1] -10   1   2   3   3  40 100\n\nmax(x)  #Finding Largest Element of Vector x\n\n[1] 100\n\nmin(x)  #Finding Smallest Element of Vector x\n\n[1] -10\n\n\n\n\n2.5.1 Built-in Functions: Setting an Argument\n\nThe standard format to a built-in function is functionName(argument)\n\nFor example, the square root function structure is listed as sqrt(x), where x is a numeric or complex vector or array.\n\n\n\n# Here, we are setting a required argument x to a value of 100. When\n# a value is set, it turns it to a parameter of the function.\nsqrt(x = 100)\n\n[1] 10\n\n# Because there is only one argument and it is required, we can\n# eliminate its name x= from our function call. This is discussed\n# below.\nsqrt(100)\n\n[1] 10\n\n\n\nThere is a little variety in how we can write functions to get the same results.\nA parameter is what a function can take as input. It is a placeholder and hence does not have a concrete value. An argument is a value passed during function invocation.\nThere are some default values set up in R in which arguments have already been set.\nThere are a few functions with no parameters like Sys.time() which produces the date and time. If you are not sure how many parameters a function has, you should look it up in the help.\n\n\n\n2.5.2 Default Values\n\nThere are many default values set up in R in which arguments have already been set to a particular value or field.\nDefault values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\nWhen only one argument is required, the argument is usually not set to have a default value.\n\n\n\n2.5.3 Built-in Functions: Using More than One Argument\n\nFor functions with more than one parameter, we must determine what arguments we want to include, and whether a default value was set and if we want to change it. Default values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\n\nFor example, the default S3 method for the seq() function is listed as the following: seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),length.out = NULL, along.with = NULL, …)\nDefault values have been set on each parameter, but we can change some of them to get a meaningful result.\nFor example, we set the from, to, and by parameter to get a sequence from 0 to 30 in increments of 5.\n\n\n\n# We can use the following code.\nseq(from = 0, to = 30, by = 5)\n\n[1]  0  5 10 15 20 25 30\n\n\n\nWe can simplify this function call even further:\n\nIf we use the same order of parameters as the instructions, we can eliminate the argument= from the function.\nSince we do list the values to the arguments in same order as the function is defined, we can eliminate the from=, to=, and by= to simplify the statement.\n\n\n# Equivalent statement as above\nseq(0, 30, 5)\n\n[1]  0  5 10 15 20 25 30\n\n\nIf you leave off the by parameter, it defaults at 1.\n\n\n# Leaving by= to default value of 1\nseq(0, 30)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\n\n\nThere can be a little hurdle deciding when you need the argument value in the function call. The general rule is that if you don’t know, include it. If it makes more sense to you to include it, include it.\n\n\n\n2.5.4 Tips on Arguments\n\nAlways look up a built-in function to see the arguments you can use.\nArguments are always named when you define a function.\nWhen you call a function, you do not have to specify the name of the argument.\nArguments have default values, which is used if you do not specify a value for that argument yourself.\nAn argument list comprises of comma-separated values that contain the various formal arguments.\nDefault arguments are specified as follows: parameter = expression\n\n\ny &lt;- 10:20\nsort(y)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\nsort(y, decreasing = FALSE)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#entering-and-loading-data",
    "href": "introR.html#entering-and-loading-data",
    "title": "2  Introduction to R and RStudio",
    "section": "2.9 Entering and Loading Data",
    "text": "2.9 Entering and Loading Data\n\n2.9.1 Creating a Vector\n\nA vector is the simplest type of data structure in R.\n\nA vector is a set of data elements that are saved together as the same type.\nWe have many ways to create vectors with some examples below.\n\nUse c() function, which is a generic function which combines its arguments into a vector or list.\n\n\nc(1, 2, 3, 4, 5)  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nIf numbers are aligned, can use the “:“ symbol to include numbers and all in between. This is considered an array.\n\n\n1:5  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nUse seq() function to make a vector given a sequence.\n\n\nseq(from = 0, to = 30, by = 5)  #Creates a sequence vector from 0 to 30 in increments on 5 \n\n[1]  0  5 10 15 20 25 30\n\n\n\nUse rep() function to repeat the elements of a vector.\n\n\nrep(x = 1:3, times = 4)  #Repeat the elements of the vector 4 times\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nrep(x = 1:3, each = 3)  #Repeat the elements of a vector 3 times each\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\n\n\n2.9.2 Creating a Matrix\n\nA matrix is another type of object like a vector or a list.\n\nA matrix has a rectangular format with rows and columns.\nA matrix uses matrix() function\nYou can include the byrow = argument to tell the function whether to fill across or down first.\nYou can also include the dimnames() function in addition to the matrix() to assign names to rows and columns.\n\nUsing matrix() function, we can create a matrix with 3 rows and 3 columns as shown below.\n\nTake note how the matrix fills in the new data.\n\n\n# Creating a Variable X that has 9 Values.\nx &lt;- 1:9\n# Setting the matrix.\nmatrix(x, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Note – we do not need to name the arguments because we go in the\n# correct order.  The function below simplifies the statement and\n# provides the same answer as above.\nmatrix(x, 3, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n2.9.2.1 Setting More Arguments in a Matrix\n\nThe byrow argument fills the Matrix across the row\nBelow, we can use the byrow statement and assign it to a variable m.\n\n\nm &lt;- matrix(1:9, 3, 3, byrow = TRUE)  #Fills the Matrix Across the Row and assigns it to variable m\nm  #Printing the matrix in the console\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\nThe dimnames() function adds labels to either the row and the column. In this case below both are added to our matrix m.\n\n\ndimnames(x = m) &lt;- list(c(\"2020\", \"2021\", \"2022\"), c(\"low\", \"medium\", \"high\"))\nm  #Printing the matrix in the console\n\n     low medium high\n2020   1      2    3\n2021   4      5    6\n2022   7      8    9\n\n\n\nYou try to make a matrix of 25 items, or a 5 by 5, and fill the matrix across the row and assign the matrix to the name m2.\nYou should get the answer below.\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n[5,]   21   22   23   24   25\n\n\n\n\n2.9.2.2 Differences between Data Frames and Matrices\n\nIn a data frame the columns contain different types of data, but in a matrix all the elements are the same type of data. A matrix is usually numbers.\nA matrix can be looked at as a vector with additional methods or dimensions, while a data frame is a list.\n\n\n\n\n2.9.3 Creating a Data Frame\n\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. In a data frame the rows are observations and columns are variables.\n\nData frames are generic data objects to store tabular data.\nThe column names should be non-empty.\nThe row names should be unique.\nThe data stored in a data frame can be of numeric, factor or character type.\nEach column should contain same number of data items.\nCombing vectors into a data frame using the data.frame() function\n\nBelow, we can create vectors for state, year enacted, personal oz limit medical marijuana.\n\n\nstate &lt;- c(\"Alaska\", \"Arizona\", \"Arkansas\")\nyear.legal &lt;- c(1998, 2010, 2016)\nounce.lim &lt;- c(1, 2.5, 3)\n\n\nThen, we can combine the 3 vectors into a data frame and name the data frame pot.legal.\n\n\npot.legal &lt;- data.frame(state, year.legal, ounce.lim)\n\n\nNext, check your global environment to confirm data frame was created.\n\n\n\n\nGlobal Environment\n\n\n\n\n2.9.4 Importing a Data Frame into R\n\nWhen importing data from outside sources, you can do the following:\n\n\nYou can import data from an R package using data() function.\nYou can also link directly to a file on the web.\nYou can import data through from your computer through common file extensions:\n\n.csv: comma separated values;\n.txt: text file;\n.xls or .xlsx: Excel file;\n.sav: SPSS file;\n.sasb7dat: SAS file;\n.xpt: SAS transfer file;\n.dta: Stata file.\n\n\n\nEach different file type requires a unique function to read in the file. With all the variety in file types, it is best to look it up in the R Community to help.\n\n\n2.9.4.1 Use data() function\n\nAll we need is the data() function to read in a data set that is part of R. R has many built in libraries now, so there are many data sets we can use for testing and learning statistics in R.\n\n\n# The mtcars data set is part of R, so no new package needs to be\n# downloaded.\ndata(\"mtcars\")\n\n\nLoad a data frame from a unique package in R.\n\nThere are also a lot of packages that house data sets. It is fairly easy to make a package that contains data and load it into CRAN. These packages need to be installed into your R one time. Then, each time you open R, you need to reload the library using the library() function.\nWhen your run the install.packages() function, do not include the # symbol. Then, after you run it one time, comment it out. There is no need to run this code a second time unless something happens to your RStudio.\n\n\n# install.packages('MASS') #only need to install package one time in\n# R\nlibrary(MASS)\n\n\n\ndata(\"Insurance\")\nhead(Insurance)\n\n  District  Group   Age Holders Claims\n1        1    &lt;1l   &lt;25     197     38\n2        1    &lt;1l 25-29     264     35\n3        1    &lt;1l 30-35     246     20\n4        1    &lt;1l   &gt;35    1680    156\n5        1 1-1.5l   &lt;25     284     63\n6        1 1-1.5l 25-29     536     84\n\n\n\n\n2.9.4.2 Accessing Variables\n\nYou can directly access a variable from a dataset using the $ symbol followed by the variable name.\nThe $ symbol facilitates data manipulation operations by allowing easy access to variables for calculations, transformations, or other analyses. For example:\n\n\nhead(Insurance$Claims)  #lists the first 6 Claims in the Insurance dataset.\n\n[1]  38  35  20 156  63  84\n\nsd(Insurance$Claims)  #provides the standard deviation of all Claims in the Insurance dataset.\n\n[1] 71.1624\n\n\n\n\n2.9.4.3 Setting up a Working Directory\n\nYou should have the data files from our LMS in a data folder on your computer. Your project folder would contain that data folder.\nBefore importing and manipulating data, you must find and edit your working directory to directly connect to your project folder!\nThese functions are good to put at the top of your R files if you have many projects going at the same time.\n\n\ngetwd()  #Alerts you to what folder you are currently set to as your working directory\n# For example, my working directory is set to the following:\n# setwd('C:/Users/Desktop/ProbStat') #Allows you to reset the working\n# directory to something of your choice.\n\n\nIn R, when using the setwd() function, notice the forward slashes instead of backslashes.\nYou can also go to Tools &gt; Global Options &gt; General and reset your default working directory when not in a project. This will pre-select your working directory when you start R.\nOr if in a project, like we should be, you can click the More tab as shown in the Figure below, and set your project folder as your working directory.\n\n\n\n\nSetting Your Working Directory\n\n\n\n\n2.9.4.4 Reading in Data from .csv\n\nReading in a .csv file is extremely popular way to read in data.\nThere are a few functions to read in .csv files. And these functions would change based on the file type you are importing.\n\n\n2.9.4.4.1 read.csv() function\n + Extremely popular way to read in data.\n + read.csv() is a base R function that comes built-in with R: No library necessary\n\nAll your datasets should be in a data folder in your working directory so that you and I have the same working directory. This creates a relative path to our working directory.\nIn R, when working with data files stored on your computer, it’s important to understand the difference between absolute and relative file paths. An absolute reference gives the complete path to the file, starting from the root directory. This path is specific to your system, and it doesn’t change regardless of where the R script is located. For example, on a Windows machine, you might use something like read.csv(\"C:/Users/username/Documents/data.csv\"). This path will always point to the same file, but it can make your code less portable since it only works on your machine or if others have the exact same file structure.\nOn the other hand, a relative reference specifies the file’s path relative to the location of your R script or working directory. It is more flexible because it assumes the file is located in a directory relative to the current project or script. For example, if your script and data file are in the same folder, you could use read.csv(\"data.csv\"). If the file is in a subdirectory, you would reference it relatively like read.csv(\"data/data.csv\"). Relative paths make your code more portable and easier to share since it will work as long as the folder structure remains consistent.\nUsing relative paths is often a best practice, especially in collaborative projects or when sharing code. You can check your current working directory in R with getwd() and set it with setwd().\nThe structure of the function is datasetName &lt;- read.csv(“data/dataset.csv”).\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n# or equivalently\ngss.2016 &lt;- read.csv(\"data/gss2016.csv\")\n# Examine the contents of the file\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n# Or equivalently, we can shorten this to the following code\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n\n\n2.9.4.5 Using tidyverse to load data\n\n2.9.4.5.1 read_csv function\n\nIn R, both read.csv() and read_csv() are used to read CSV files, but they come from different packages and have important differences. read.csv() is part of base R and is widely used for loading CSV files into data frames, as in data &lt;- read.csv(\"data/data.csv\"). It can be slower with large datasets and automatically converts strings to factors unless stringsAsFactors = FALSE.\nread_csv(), from the readr package in the tidyverse, is faster and better suited for large datasets. You’d use it like data &lt;- readr::read_csv(\"data/data.csv\"). It doesn’t convert strings to factors by default and provides clearer error messages.read_csv() is often preferred for performance and better handling of data types, especially in larger datasets or tidyverse projects.\nGeneral Takeaways:\nread_csv() is a function from the readr package, which is part of the tidyverse ecosystem.\nread_csv() is generally faster than read.csv() as it’s optimized for speed, making it more efficient, particularly for large datasets.\n\n\n# install.packages(tidyverse) ## Only need to install one time on\n# your computer. #install.packages links have been commented out\n# during processing of RMarkdown.  Activate the library, which you\n# need to access each time you open R and RStudio\nlibrary(tidyverse)\n\n\n# Now open the data file to evaluate with tidyverse\ngss.2016b &lt;- read_csv(file = \"data/gss2016.csv\")\n\n\n\n\n\n2.9.5 Summarize Data\n\nIn R, summary() and summarize() serve different purposes. summary() is part of base R and gives a quick overview of data, returning descriptive statistics for each column. For example, summary(mtcars) provides the min, max, median, and mean for numeric columns and counts for factors. It’s useful for a broad snapshot of your dataset.\nIn contrast, summarize() (or summarise()) is from the dplyr package and allows for custom summaries. For instance, mtcars %&gt;% summarize(avg_mpg = mean(mpg), max_hp = max(hp)) returns the average miles per gallon and the maximum horsepower. It’s more flexible and is often used with group_by() for grouped calculations. In conclusion, summary() gives automatic overviews, while summarize() is better for tailored summaries.\nUse the summary() function to examine the contents of the file.\n\n\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\nAgain, we can eliminate the object = because it is the first argument and is required.\n\n\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n2.9.5.1 Explicit Use of Libraries\n\nYou can activate a library one time using library::function() format\nFor example, we can use the summarize() function from dplyr which is part of tidyverse installed earlier.\nSince dplyr is part of tidyverse, there is actually no need to activate it when we have already activated tidyverse in this session, however, it does help when conflicts are present. More on that later.\n\nThe line below says to take the the gss.2016 data object and summarize the length of age using the dplyr library.\n\n\ndplyr::summarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\nIn the line of code above, we see package::function(). If we initiate the library like below, we do not need the beginning of the statement. The code below provides the same answer as the way written above.\n\n\nlibrary(dplyr)\nsummarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\n\nYou try to access the ChickWeight dataset from the MASS package and summarize it generally using summary() and str() functions.\nEarlier, we looked up the tapply() function in the help bar and found that the format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean. Use the tapply() function to take the mean weight of Chicks based on Diet. See the answer below.\nWhile there are a few ways to get group means in R. I like the tapply() function which is used to apply a function over subsets of a vector. The function splits the data into subsets based on a given factor or factors, applies a specified function to each subset, and then returns the results in a convenient form. Here, we are applying the mean function.\nTo access a variable, use dataset$variable or you can also attach a dataset to your code so you can just use the variable name later on. So use ChickWeight$weight and ChickWeight$Diet appropriately in the function alongside the FUN mean.\n\n\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n       1        2        3        4 \n102.6455 122.6167 142.9500 135.2627",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#summary",
    "href": "introR.html#summary",
    "title": "2  Introduction to R and RStudio",
    "section": "2.11 Summary",
    "text": "2.11 Summary\n\nIn this lesson, we went over information to make sure you had some basics to really start learning R. There are a lot of ways to get the same things done in R; you have to find the way that works best for you. As we learn R, you will get used to doing things your way to be able to slice and evaluate the data to find rich information from the data sets we look at. As long as the data was handled properly, it does not matter how we reach our goal using R as long as we do it ourselves.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "index.html#installing-r-and-r-studio-1",
    "href": "index.html#installing-r-and-r-studio-1",
    "title": "Probability And Business Statistics",
    "section": "1.1 Installing R and R Studio",
    "text": "1.1 Installing R and R Studio\n\n1.1.1 Installing R on Windows\n\nOpen an internet browser and go to https://www.r-project.org/.\nClick CRAN hyperlink underneath Download on the left.\nSelect a CRAN location (a mirror site) and click the corresponding link. I selected O-cloud. This brings you to the following link: https://cloud.r-project.org/\nClick on the “Download R for Windows” link at the top of the page if you have a Windows computer.\n\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer.\nRun the .exe file\nAccept all defaults and follow the installation instructions\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.1.2 Installing R on a Mac\n\nOpen an internet browser and go to https://www.r-project.org/.\nClick CRAN hyperlink underneath Download on the left.\nSelect a CRAN location (a mirror site) and click the corresponding link. I selected O-cloud. This brings you to the following link: https://cloud.r-project.org/\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nAccept the license agreement and all subsequent defaults\nWhen the installation completes, click Close.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.1.3 Downloading RStudio For Mac or PC\n\nGo to the following URL: https://www.rstudio.com/products/rstudio/download/\nClick the Download button for Install RStudio.\nIt should recognize your operating system. If you see different an Installers for Supported Platforms section, click the version that is appropriate for your operating system. When the download is complete, run the install program, accepting all defaults.\nNote, RStudio might conflict with some Mac antivirus programs. Ensure your antivirus is not blocking it from installing. Only worry about this if you have difficulty installing the program.\n\n\n\n\nRStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#take-your-first-look-at-r",
    "href": "index.html#take-your-first-look-at-r",
    "title": "Probability And Business Statistics",
    "section": "1.1 Take Your First Look at R",
    "text": "1.1 Take Your First Look at R\n\nOn the right, two windows, each with tabs\n\nThere are many ways to customize the look of RStudio for accessibility purposes with a couple being the following:\n\nYou can resize the windows using the splitters.\nYou can maximize/restore the windows within the left/right panels using the familiar Windows controls in the upper-right of each window.\n\n\nOn the left, the Console Window reproduces the R environment.\n\nObserve the command line with the \\(&gt;\\) symbol.\n\n\n\n\n\nConsole",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#creating-a-project-for-our-class",
    "href": "index.html#creating-a-project-for-our-class",
    "title": "Probability And Business Statistics",
    "section": "1.2 Creating a Project for Our Class",
    "text": "1.2 Creating a Project for Our Class\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\nOpen up R Studio and Let’s Take a look around.\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this class, so it is good practice to keep everything organized!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).\n\n\n\n\nCreateProject\n\n\n\n\n\nCreating a New Project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#r-script-files-myfirstrscript.r",
    "href": "index.html#r-script-files-myfirstrscript.r",
    "title": "Probability And Business Statistics",
    "section": "1.3 R Script files (MyFirstRscript.R)",
    "text": "1.3 R Script files (MyFirstRscript.R)\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you have to re-enter them at the command line. Nothing saves for later.\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise. Fortunately, you can make a .R script file to solve this issue.\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nCreate your first R Script file within your Project for testing purposes.\n\nGo to File &gt; New File &gt; R Script\nSave this file as MyFirstRscript.R in your project folder you just made. You should see this new file under Files like mine is in the bottom right panel. As we create new files, continue to save them into your project folder.\n\nOn the .R file presented to the left, comments are added as denoted by the hashtag which you can type or push ctrl + shift + c.\nIf you type in your console, it will not save it for later. However, if you save code in this R Script file, you can open your file at a later date to rerun your code. Also, as we move through the class, feel free to document all your notes in your .R file via # called comments. More on comments later.\n\n\n\n\nScreenshot of MyFirstScript.R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#using-r-more-effectively",
    "href": "index.html#using-r-more-effectively",
    "title": "Probability And Business Statistics",
    "section": "1.4 Using R More Effectively",
    "text": "1.4 Using R More Effectively\n\n1.4.1 Setting Options to Your Liking\n\nYou have many options to customize R Studio.\nGo to Tools &gt; Global Options to see the list of options for you to customize.\nA few are listed below.\n\nYou can set your general information including your default working directory (when not in a project).\nYou can customize the appearance to a theme that accommodates your learning style and visual preferences.\nYou can turn on a spell check.\n\n\n\n\n\nTools &gt; Global Options.R\n\n\n\n\n1.4.2 Quick Keys in R\n\nThere are a lot of quick keys in R to make you able to use it faster and more effectively. You may look over these and try on your own.\n\n\n\n\nConsole Quick Keys\n\n\n\n\n\nSource Quick Keys\n\n\n\n\n\nEditing Quick Keys\n\n\n\n\n1.4.3 Getting Help in R\n\nThere are lots of ways to get help in R.\nIn R, use the help search box to find information on a function, parameter, or package.\n\n?mean\nhelp.search(‘swirl’)\nhelp(package = ‘MASS’)\n?install.packages\n\n\n\n\n\nGetting Help\n\n\n\nYou should try to look up the tapply command to see what it does.\n\nUse ?tapply in your .R file to pull up tapply() command or type tapply in the Help box. * Formally, you should see that the command applies a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors. This means that the command does some math calculation (mean, sum, etc.) on a continuous variable after dividing the data by group.\n\nThe format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean.\nMore on that function later in the first lessons.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#accessing-data-files-for-the-book",
    "href": "index.html#accessing-data-files-for-the-book",
    "title": "Probability And Business Statistics",
    "section": "1.5 Accessing Data Files for the Book",
    "text": "1.5 Accessing Data Files for the Book\n\nTo download the data sets for the book, go to our Blackboard datasets page and download them to your computer\nOnce downloaded, unzip the file by right-clicking and selecting “Extract All”, and then move the subfolder named data to your working directory.\nIn the example below, my project folder for the class is called BUAD231 and the subfolder that contains all the data files is called data. You can put your folder anywhere on the hard drive of your computer, but do not download it to places on the cloud like OneDrive.\n\n\n\n\nSaving Our Book Files Into a “data” Folder in Your Working Directory\n\n\n\nWe will use a number of data files plus more for homework/projects, so be prepared to use these files throughout the class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Probability And Business Statistics",
    "section": "1.6 Summary",
    "text": "1.6 Summary\n\nAt the end of this section, you should have downloaded and browsed R in RStudio. You should have looked at the quick keys in R to make editing your R documents easier. You should set up a project folder for the course where you want to on your computer, and made your first .R script file located in that project folder. You should have tried a line of code or two as presented. Finally, you should have looked around the RStudio environment and found the help tab.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "introR.html#using-r-script-files",
    "href": "introR.html#using-r-script-files",
    "title": "2  Introduction to R and RStudio",
    "section": "2.3 Using R Script Files",
    "text": "2.3 Using R Script Files\n\nUsing R Script Files:\n\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nUsing the Console\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you must re-enter them at the command line. Nothing saves for later.\n\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise.R script files help to solve this issue.\n\n\n2.3.1 Create a New R Script File: Chapter1.R\n\nTo save your notes from today’s lecture, create a .R file named Chapter1.R and save it to your project file you made in the last class.\nThere are a couple of parts to this chapter, and we can add code from today’s chapter in one file so that our code is stacked nicely together.\n\nFor each new chapter, start a new file and save it to your project folder.\n\n\n\n\nScreenshot of R Environment\n\n\n\n\n2.3.2 Using a Prolog\n\nWe should include a prolog to introduce each R Script File during the course.\nA prolog is a set of comments at the top of a code file that provides information about what is in the file.\nIncluding a prolog is considered coding best practice.\nIt also names the files and resources used that facilitates identification.\nAn informal prolog is below:\n\n\n####################################\n# Project name: \n# Project purpose: \n# Code author name: \n# Date last edited: \n# Data used: \n# Libraries used: \n####################################\n\n\nOn your R Script File, add your own prolog following the template as shown.\nI like to add a line for Data used and Libraries used so we know what all we used in the script.\n\n\n####################################\n# Project name: Chapter 1\n# Project purpose: To create an R script file to learn about R. \n# Code author name: Pamela Schlosser\n# Date last edited: [Enter Date Here]\n# Data used: NA\n# Libraries used: NA\n####################################\n\n\nThen, as we work through our .R script and add data files or libraries to our code, we go back and edit the prolog.\n\n\n\n2.3.3 R Handles Text in Multiple Ways\n\nR can generally use single quotes or double quotes when marking text. However, if you use a single quote to start, use a single quote to end. The same for double quotes - ensure the pairing is the same quote type.\nYou sometimes need to be careful with nested quotes, but generally it does not matter which you use.\n\n\n\"This is a string\"\n\n[1] \"This is a string\"\n\n\"This is also a string\"\n\n[1] \"This is also a string\"\n\n\n\nWe can also add comments to our code to document our work and add notes to our self or to others.\n\n\n# This is a comment for documentation or annotation\n\n\nAdd the code above to your R file and run each line using Ctl + Enter or select all lines and click Run.\nTake note that nothing prints in the console after running a comment.\n\n\n\n\nChapter 1 R File\n\n\n\n\n2.3.4 Using Comments\n\nWe use comments to organize and explain code in our R Script file.\nBe sure to write clear code that does not need a lot of comments.\nInclude useful comments where needed so that anyone (including yourself in the future) can run and understand your code.\nIf something does not work, don’t delete it yet. Instead, comment it out while you troubleshoot it or to try different alternatives.\nNotice the prolog above is in comments.\n\n\n\n2.3.5 Note on R Markdown\n\nThese files were formatted with RMarkdown. RMarkdown is a simple formatting syntax for authoring documents of a variety of types, including PowerPoint and html files.\nOn the document, RMarkdown prints the command and then follows the command with the output after 2 hashtags.\nIn your R Script File, you only need to type in the command and then run your code to get the same output as presented here.\n\n\n\n\nReading our HTML file\n\n\n\n\n2.3.6 R is an Interactive Calculator\n\nAn important facet of R is that it should serve as your sole calculator.\nTry these commands in your .R file by typing them in and clicking Ctr + Enter on each line.\n\n\n3 + 4\n\n[1] 7\n\n3 * 4\n\n[1] 12\n\n3/4\n\n[1] 0.75\n\n3 + 4 * 100^2\n\n[1] 40003\n\n\n\nTake note that order of operations holds in R: PEMDAS\n\nParentheses ()\nExponents ^ and \\(**\\)\nDivision \\(/\\), Multiplication \\(*\\), modulo, and integer division\nAddition + and Subtraction -\n\nNote that modulo and integer division have the same priority level as multiplication and division, where modulo is just the remainder.\n\n\nprint(2 + 3 * 5 - 7^2%%4 + (5/2))\n\n[1] 18.5\n\n5/2  #parentheses: = 2.5\n\n[1] 2.5\n\n7^2  #exponent:= 49\n\n[1] 49\n\n3 * 5  #multiplication: = 15\n\n[1] 15\n\n17%%4  #modulo: = 1\n\n[1] 1\n\n17%/%4  #integer division: = 4\n\n[1] 4\n\n2 + 15  #addition: = 17\n\n[1] 17\n\n17 - 1  #subtraction: = 16\n\n[1] 16\n\n16 + 2.5  #addition: = 18.5\n\n[1] 18.5\n\n\n\n\n2.3.7 Running Commands\n\nThere are a few ways to run commands via your .R file.\n\nYou can click Ctr + Enter on each line.\nYou can select all the lines you want to run and select Ctr + Enter.\nYou can select all the lines you want to run and select the run button as shown in the Figure.\n\n\n\n\n\nRun Code\n\n\n\nNow that I have asked you to add a couple lines of code, after this point, when R code is shown on this file, you should add it to your .R script file along with any notes you want. I won’t explicitly say - “add this code.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-qualitative-data",
    "href": "descriptives.html#summarizing-qualitative-data",
    "title": "3  Descriptive Statistics",
    "section": "3.1 Summarizing Qualitative Data",
    "text": "3.1 Summarizing Qualitative Data\n\nQualitative data is information that cannot be easily counted, measured, or easily expressed using numbers.\n\nNominal variables: a type of categorical variable that represents discrete categories or groups with no inherent order or ranking\n\ngender (male, female)\nmarital status (single, married, divorced)\neye color (blue, brown, green)\n\nOrdinal variables: categories possess a natural order or ranking\n\na Likert scale measuring agreement with a statement (e.g., strongly disagree, disagree, neutral, agree, strongly agree)\n\n\nA frequency distribution shows the number of observations in each category for a factor or categorical variable.\nGuidelines when constructing frequency distribution:\n\nClasses or categories are mutually exclusive (they are all unique).\nClasses or categories are exhaustive (a full list of categories).\n\nTo calculate frequencies, first, start with a variable that has categorical data.\n\n\n# Create a vector with some data that could be categorical\nSample_Vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"A\", \"B\", \"A\", \"C\", \"A\", \"B\")\n# Create a data frame with the vector\ndata &lt;- data.frame(Sample_Vector)\n\n\nTo count the number of each category value, we can use the table() command.\nThe output shows a top row of categories and a bottom row that contains the number of observations in the category.\n\n\n# Create a table of frequencies\nfrequencies &lt;- table(data$Sample_Vector)\nfrequencies\n\n\nA B C \n5 3 2 \n\n\n\nRelative frequency is how often something happens divided by all outcomes.\nThe relative frequency is calculated by \\(f_i/n\\), where \\(f_i\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nWe can use the prop.table() command to calculate relative frequency by dividing each category’s frequency by the sample size.\n\n\n# Calculate proportions\nproportions &lt;- prop.table(frequencies)\n\n\nThe cumulative relative frequency is given by \\(cf_i/n\\), where \\(cf_i\\) is the cumulative frequency of class \\(i\\).\nThe cumsum() function calculates the cumulative distribution of the data\n\n\n# Calculate cumulative frequencies\ncumulfreq &lt;- cumsum(frequencies)\n# Calculate cumulative proportions\ncumulproportions &lt;- cumsum(prop.table(frequencies))\n\n\nThe rbind() function is used to combine multiple data frames or matrices by row. The name “rbind” stands for “row bind”. Since the data produced by the table is in rows, we can use rbind to link them together.\n\n\n# combine into table\nfrequency_table &lt;- rbind(frequencies, proportions, cumulfreq, cumulproportions)\n# Print the table\nfrequency_table\n\n                   A   B    C\nfrequencies      5.0 3.0  2.0\nproportions      0.5 0.3  0.2\ncumulfreq        5.0 8.0 10.0\ncumulproportions 0.5 0.8  1.0\n\n\n\nWe can transpose a table using the t() command, which flips the dataset.\n\n\nTransposedData &lt;- t(frequency_table)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0\n\n\n\nFinally, sometimes we need to transform our calculations into a dataset.\nThe as.data.frame function is used to coerce or convert an object into a data frame.\nas.data.frame() is used when you have an existing object that needs to be coerced into a data frame. data.frame(), on the other hand, is for creating a data frame from scratch by specifying the data directly. Therefore, both as.data.frame() and data.frame() are used to convert or create data frames in R.\nas.data.frame() coerces an existing object (such as a list, matrix, or vector) into a data frame. Data.frame is used to create a new data frame from individual vectors or lists.\nas.data.frame() accepts a wider variety of inputs (like lists, matrices, and vectors), while data.frame() directly accepts vectors and lists to construct the data frame.\n\n\nTransposedData &lt;- as.data.frame(TransposedData)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-quantitative-data",
    "href": "descriptives.html#summarizing-quantitative-data",
    "title": "3  Descriptive Statistics",
    "section": "3.2 Summarizing Quantitative Data",
    "text": "3.2 Summarizing Quantitative Data\n\n3.2.1 Defining and Calculating Central Tendency\n\nThe term central location refers to how numerical data tend to cluster around some middle or central value.\nMeasures of central location attempt to find a typical or central value that describes a variable.\nWhy frequency distributions do not work for numeric variables:\n\nNumeric variables measured on a continuum.\nInstead, we calculate descriptive statistics including central tendency and spread of the values for a numeric variable.\n\nWe will examine the three mostly widely used measures of central location: mean, median and mode.\nThen we discuss a percentile: a measure of relative position.\n\n\n3.2.1.1 Using the Mean\n\nThe arithmetic mean or simply the mean is a primary measure of central location. It is often referred to as the average. Simply add up all the observations and divide by the number of observations.\nThe numerator (top of the fraction) is the sum (sigma) of all the values of x from the first value (i = 1) to the last value (n) divided by the number of values (n).\n\\(m_x = (\\sum_{i=1}^{n} x_{i})/n\\)\nConsider the salaries of employees at a company: \nWe can use the mean() command to calculate the mean in R.\n\n\n# Create Vector of Salaries\nsalaries &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000)\n# Calculate the mean using the mean() command\nmean(salaries)\n\n[1] 154285.7\n\n\n\nNote that due to at least one outlier this mean does not reflect the typical salary - more on that later.\nIf we edit our vector to include NAs, we have to account for this. This is a common way to handle NAs in functions that do not allow for them.\n\n\nsalaries2 &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000, NA,\n    NA)\n# Calculate the mean using the mean() command Notice that it does not\n# work\nmean(salaries2)\n\n[1] NA\n\n# Add in na.rm parameter to get it to produce the mean with no NAs.\nmean(salaries2, na.rm = TRUE)\n\n[1] 154285.7\n\n\n\nNote that there are other types of means like the weighted mean or the geometric mean.\n\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\), where are the weights associated to the values.\nAn example is below.\n\n\nvalues &lt;- c(4, 7, 10, 5, 6)\nweights &lt;- c(1, 2, 3, 4, 5)\nweighted_mean &lt;- weighted.mean(values, weights)\nweighted_mean\n\n[1] 6.533333\n\n\n\n\n3.2.1.2 Using the Median\n\nThe median is another measure of central location that is not affected by outliers.\nWhen the data are arranged in ascending order, the median is:\n\nThe middle value if the number of observations is odd, or\nThe average of the two middle values if the number of observations is even.\n\nConsider the sorted salaries of employees presented earlier which contains an odd number of observations.\n\nOn the same salaries vector created above, use median() command to calculate the median in R.\n\n\n# Calculate the median using the median() command\nmedian(salaries)\n\n[1] 90000\n\n\n\nNow compare to the mean and note the large difference in numbers signifying that at least one outlier is most likely present.\nSpecifically, if the mean and median are different, it is likely the variable is skewed and contains outliers.\n\n\nmean(salaries)\n\n[1] 154285.7\n\n\n\nFor another example, consider the sorted data below that contains an even number of values.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\n\n\nWhen data contains an even number of values, the median is the average of the 2 sorted middle numbers (12.56 and 13.47).\n\n\nmedian(GrowthFund)\n\n[1] 13.015\n\n(12.56 + 13.47)/2\n\n[1] 13.015\n\n# The mean is still the average\nmean(GrowthFund)\n\n[1] 10.088\n\n\n\n\n3.2.1.3 Using the Mode\n\nThe mode is another measure of central location.\nThe mode is the most frequently occurring value in a data set.\nThe mode is useful in summarizing categorical data but can also be used to summarize quantitative data.\nA data set can have no mode, one mode (unimodal), two modes (bimodal) or many modes (multimodal).\nThe mode is less useful when there are more than three modes.\n\n\n\n3.2.1.4 Example of Function with Salary Variable\n\nConsider the salary of employees presented earlier. The mode is $40,000 since this value appears most often.\n\n\n# Try this command with and without it.\nnames(sort(x = table(salaries), decreasing = TRUE))\n\n[1] \"40000\"  \"65000\"  \"90000\"  \"145000\" \"150000\" \"550000\"\n\n\n\n40,000 appears 2 times and is the mode because that occurs most often.\n\n\n\n3.2.1.5 Finding No Mode\n\nLook at the sort(table()) commands with the GrowthFund Vector we made earlier.\nI added a [1:3] at the end of the statement to produce the 3 highest frequencies found in the vector.\n\n\nsort(table(GrowthFund), decreasing = TRUE)[1:3]\n\nGrowthFund\n-38.32   1.71   3.17 \n     1      1      1 \n\n\n\nEven if you use this command, you still need to evaluate the data more systematically to verify the mode. If the highest frequency of the sorted table is 1, then there is no mode.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#defining-and-calculating-spread",
    "href": "descriptives.html#defining-and-calculating-spread",
    "title": "3  Descriptive Statistics",
    "section": "3.3 Defining and Calculating Spread",
    "text": "3.3 Defining and Calculating Spread\n\nSpread is a measure of distance values are from the central value.\nEach measure of central tendency has one or more corresponding measures of spread.\nMean: use variance or standard deviation to measure spread.\n\nskewness and kurtosis help measure spread as well.\n\nMedian: use range or interquartile range (IQR) to measure spread.\nMode: use the index of qualitative variation to measure spread.\n\nNot formally testing here with a function.\n\n\n\n3.3.1 Spread to Report with the Mean\n\n3.3.1.1 Evaluating Skewness\n\nSkewness is a measure of the extent to which a distribution is skewed.\nCan evaluate skewness visually with histogram.\n\nA histogram is a visual representation of a frequency or a relative frequency distribution.\nBar height represents the respective class frequency (or relative frequency).\nBar width represents the class width.\n\n\n\n\n\nEvaluating Skewness Visually\n\n\n\n\n3.3.1.2 Skewed Distributions: Median Not Same as Mean\n\nSometimes, a histogram is difficult to tell if skewness is present or if the data is relatively normal or symmetric.\nIf Mean is less than Median and Mode, then the variable is Left-Skewed.\nIf the Mean is greater than the Median and Mode, then the variable is Right-Skewed.\nIf the Mean is about equal to the Median and Mode, then the variable has a symmetric distribution.\nIn R, we can easily look at mean and median with the summary() command.\n\n\n\n\nEvaluating Skewness Using Mean and Median\n\n\n\nMean is great when data are normally distributed (data is not skewed).\nMean is not a good representation of skewed data where outliers are present.\n\nAdding together a set of values that includes a few very large or very small values like those on the far left of a left-skewed distribution or the far right of the right-skewed distribution will result in a large or small total value in the numerator of Equation and therefore the mean will be a large or small value relative to the actual middle of the data.\n\n\n\n\n3.3.1.3 Using skew() Command in R\n\nThe skew() command is from the semTools package. The install.packages() command is commented out below, but install it one time on your R before commenting it out.\n\n\n# install the semTools package if necessary.\n# install.packages('semTools') Activate the library\nlibrary(semTools)\n\n\nAfter the package is installed and loaded, run the skew() command on the salaries vector made above.\n\n\nskew(salaries)\n\n skew (g1)         se          z          p \n2.31126775 0.92582010 2.49645450 0.01254418 \n\n\n\n\n3.3.1.4 Interpreting the skew() Command Results\n\nse = standard error\nz = skew/se\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nSalary: Our sample size was small, &lt;50, so the z value of 2.496 in regards to the salary vector indicates there is a problem with skewness.\nGrowthFund: We can check the skew of GrowthFund.\n\n\nskew(GrowthFund)\n\n  skew (g1)          se           z           p \n-1.38071963  0.77459667 -1.78250137  0.07466751 \n\n\n\nGrowthFund was also considered a small sample size, so the same -2/2 thresholds are used. Here, our z value is -1.78250137, which is in normal range. This indicates there is no problem with skewness.\n\n\n\n\n3.3.2 Histograms\n\nA histogram is a graphical representation of the distribution of numerical data.\nIt consists of a series of contiguous rectangles, or bars, where the area of each bar corresponds to the frequency of observations within a particular range or bin of values.\nThe x-axis typically represents the range of values being measured, while the y-axis represents the frequency or count of observations falling within each range.\nHistograms are commonly used in statistics and data analysis to visualize the distribution of a dataset and identify patterns or trends.\nThey are particularly useful for understanding the central tendency, variability, and shape of the data distribution - this includes our observation of skewness.\nWorks much better with larger datsets.\n\n\n3.3.2.1 Commands to Make a Histogram\n\nhist() command in base R.\ngeom_histogram() command in ggplot2 package.\na hist using the GrowthFund dataset does not look that great because its sample size is so small.\n\n\nhist(GrowthFund)\n\n\n\n\n\n\n\n\n\n\n3.3.2.2 hist vs geom_histogram\n\nIn R, hist() and geom_histogram() are both used to create histograms, but they belong to different packages and have slightly different functionalities.\n\n\n# Making an appropriate data.frame to use the hist() command\nHousePrice &lt;- c(430, 520, 460, 475, 670, 521, 670, 417, 533, 525, 538,\n    370, 530, 525, 430, 330, 575, 555, 521, 350, 399, 560, 440, 425, 669,\n    660, 702, 540, 460, 588, 445, 412, 735, 537, 630, 430)\nHousePrice &lt;- data.frame(HousePrice)\n\n\nhist(): This function is from the base R graphics package and is used to create histograms. It provides a simple way to visualize the distribution of a single variable.\n\n\n# Using base R to create the histogram.\nhist(HousePrice$HousePrice, breaks = 5, main = \"A Histogram\", xlab = \"House Prices (in $1,000s)\",\n    col = \"yellow\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\ngeom_histogram(): This function is from the ggplot2 package, which is part of the tidyverse. It is used to create histograms as part of a more flexible and powerful plotting system.\n\n\n# Using geom_histogram() command to create the histogram.\nggplot(HousePrice, aes(x = HousePrice)) + geom_histogram(binwidth = 100,\n    boundary = 300, color = \"black\", fill = \"yellow\") + labs(title = \"A Histogram\",\n    x = \"House Prices (in $1,000s)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe could add more parameters here to make the 2 histograms look identical, but this configuration of parameters is very close. Take note that there are a lot more parameters you can add to the geom_histogram() command than you can with base R to make it look more professional. Be sure to look them up and also check with the notes in the book, which focuses on geom_histogram instead of hist().\nVariance is a measure of spread for numeric variables that is essentially the average of the squared differences between each observation value on some variable and the mean for that variable with population variance. \\[Population Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2}/N\\] \\[Sample Var(x) = s^2 = \\sum{(x_i-\\bar{x})^2}/(n-1)\\]\nStandard deviation is the square root of the variance.\n\nUse var() command and sd() command to calculate sample variance and sample standard deviation.\n\n\n\n## Calculated from Small Sample\nx &lt;- c(1, 2, 3, 4, 5)\nsum((x - mean(x))^2/(5 - 1))\n\n[1] 2.5\n\nvar(x)\n\n[1] 2.5\n\nsqrt(var(x))\n\n[1] 1.581139\n\nsd(x)\n\n[1] 1.581139\n\nsd(HousePrice$HousePrice)  #102.6059\n\n[1] 102.6059\n\nvar(HousePrice$HousePrice)  #10527.97\n\n[1] 10527.97\n\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n0.3173182 0.4082483 0.7772676 0.4370009 \n\n\nLooking at Spread for a Larger Dataset\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\nsummary(customers$Spending, na.rm = TRUE)  #mean and median\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\nmean(customers$Spending, na.rm = TRUE)  #mean by itself\n\n[1] 659.555\n\nmedian(customers$Spending, na.rm = TRUE)  #median by itself\n\n[1] 662\n\n### Spread to Report with the Mean\nsd(customers$Spending, na.rm = TRUE)\n\n[1] 350.2876\n\nvar(customers$Spending, na.rm = TRUE)\n\n[1] 122701.4\n\n\n\n\n3.3.2.3 Kurtosis in Evaluating Mean Spread\n\nKurtosis is the sharpness of the peak of a frequency-distribution curve or more formally a measure of how many observations are in the tails of a distribution.\nThe formula for kurtosis is as follows: Kurtosis = \\(\\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum \\left( \\frac{(X_i - \\bar{X})^4}{s^4} \\right) - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\n\nWhere:\n\n\\(n\\) is the sample size\n\\(X_i\\) is each individual value\n\\(\\bar{X}\\) is the mean of the data\n\\(s\\) is the standard deviation\nA normal distribution will have a kurtosis value of three, where distributions with kurtosis around 3 are described as mesokurtic, significantly higher than 3 indicate leptokurtic, and significantly under 3 indicate platykurtic.\nThe kurtosis() command from the semTools package subtracts 3 from the kurtosis, so we can evaluate values by comparing them to 0. Positive values will be indicative to a leptokurtic distribution and negative will indicate a platykurtic distribution. To see if kurtosis (leptokurtic or platykurtic) is significant, we confirm them by first evaluating the z-score to see if the variable is normal or not. The same cutoff values from skew also apply for the z for small, medium, and large sample sizes in kurtosis. These are the same basic rules for the rules in judging skewness.\n\n\n\n\nEvaluate Kurtosis\n\n\n\nThe rules of determining problematic distributions with regards to kurtosis are below.\n\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nIf kurtosis is found, then evaluate the excess kur score to see if it is positive or negative to determine whether it is leptokurtic or platykurtic.\n\n\n\n# z-value is 3.0398, which is &gt; 2 indicating leptokurtic Small sample\n# size: range is -2 to 2\nkurtosis(salaries)\n\nExcess Kur (g2)              se               z               p \n    5.628711065     1.851640200     3.039851407     0.002366949 \n\n# z-value is 2.20528007, which is &gt; 2 indicating leptokurtic Small\n# sample size: range is -2 to 2\nkurtosis(GrowthFund)\n\nExcess Kur (g2)              se               z               p \n     3.41640519      1.54919334      2.20528007      0.02743445 \n\n# Small sample size: range is -2 to 2 Skewness and kurtosis are both\n# in range.\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n0.3173182 0.4082483 0.7772676 0.4370009 \n\nkurtosis(HousePrice$HousePrice)  #normal\n\nExcess Kur (g2)              se               z               p \n     -0.5399982       0.8164966      -0.6613601       0.5083814 \n\n\n\nLet’s do a few more examples using the customers dataset.\n\n\n# Noted sample size at 200 observations or a medium sample size.\n# Using threshold –3.29 to 3.29 to assess normality.\n\n#-3.4245446445 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$Spending)\n\nExcess Kur (g2)              se               z               p \n  -1.1862970634    0.3464101615   -3.4245446445    0.0006158307 \n\ngeom_histogram(binwidth = 100, fill = \"pink\", color = \"black\")\n\ngeom_bar: na.rm = FALSE, orientation = NA\nstat_bin: binwidth = 100, bins = NULL, na.rm = FALSE, orientation = NA, pad = FALSE\nposition_stack \n\nsemTools::skew(customers$Spending)  ##normal indicating no skewness\n\n  skew (g1)          se           z           p \n-0.01836861  0.17320508 -0.10605123  0.91554171 \n\n# Normal: 2.977622119 is in between -3.29 and 3.29\nkurtosis(customers$Income)\n\nExcess Kur (g2)              se               z               p \n    1.031478559     0.346410162     2.977622119     0.002904939 \n\nggplot(customers, aes(Income)) + geom_histogram(binwidth = 10000, fill = \"pink\",\n    color = \"black\")\n\n\n\n\n\n\n\nsemTools::skew(customers$Income)  #Skewed right\n\n   skew (g1)           se            z            p \n8.741804e-01 1.732051e-01 5.047083e+00 4.486067e-07 \n\n#-3.7251961028 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$HHSize)\n\nExcess Kur (g2)              se               z               p \n  -1.2904457837    0.3464101615   -3.7251961028    0.0001951634 \n\nggplot(customers, aes(HHSize)) + geom_histogram(binwidth = 1, fill = \"pink\",\n    color = \"black\")\n\n\n\n\n\n\n\nsemTools::skew(customers$HHSize)  #normal\n\n  skew (g1)          se           z           p \n-0.08891507  0.17320508 -0.51335141  0.60770553 \n\n# Normal: -0.20056607 is in between -3.29 and 3.29\nkurtosis(customers$Orders)\n\nExcess Kur (g2)              se               z               p \n    -0.06947812      0.34641016     -0.20056607      0.84103789 \n\ngeom_histogram(binwidth = 5, fill = \"pink\", color = \"black\")\n\ngeom_bar: na.rm = FALSE, orientation = NA\nstat_bin: binwidth = 5, bins = NULL, na.rm = FALSE, orientation = NA, pad = FALSE\nposition_stack \n\nsemTools::skew(customers$Orders)  ##skewed right\n\n   skew (g1)           se            z            p \n7.885502e-01 1.732051e-01 4.552697e+00 5.296255e-06 \n\n\n\n\n\n3.3.3 Spread to Report with the Median\n\nRange = Maximum Value – Minimum Value.\n\nSimplest measure.\nFocuses on Extreme values.\nUse commands diff(range()) or max() – min().\n\nIQR: Difference between the first and third quartiles.\n\nUse IQR() command or quantile() command.\n\n\nsummary(customers$Spending, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\ndiff(range(customers$Spending, na.rm = TRUE))\n\n[1] 1200\n\nmax(customers$Spending, na.rm = TRUE) - min(customers$Spending, na.rm = TRUE)\n\n[1] 1200\n\nIQR(customers$Spending, na.rm = TRUE)\n\n[1] 578.5\n\n\n\n\n\n3.3.4 Spread to Report with the Mode\n\nWhile there is no great function to test for spread, you can look at the data and see if it is concentrated around 1 or 2 frequencies. If it is, then the spread is distorted towards those high frequency values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summary",
    "href": "descriptives.html#summary",
    "title": "3  Descriptive Statistics",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\nIn this lesson, we worked through descriptive statistics including skewness and kurtosis. We learned about variables and scales of measurement, how to summarize qualitative and quantitative data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "dataprep.html#evaluating-data-types",
    "href": "dataprep.html#evaluating-data-types",
    "title": "4  Data Preparation",
    "section": "4.1 Evaluating Data Types",
    "text": "4.1 Evaluating Data Types\n\nThere are a number of data types in R that are common to programming and statistical analysis.\nA data type of a variable specifies the type of data that is stored inside that variable. Sometimes when you read in data it is in the correct type, and other times, you need to force it into the type you need to conduct the analysis through a process called coercion.\n\nFactor (Nominal or Ordinal)\nNumeric (Real or Discrete (Integer))\nCharacter\nLogical\nDate",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#common-dplyr-functions",
    "href": "dataprep.html#common-dplyr-functions",
    "title": "4  Data Preparation",
    "section": "4.3 Common dplyr Functions",
    "text": "4.3 Common dplyr Functions\n\n4.3.1 Arrange\n\nSorting or arranging the dataset allows you to specify an order based on variable values.\nSorting allows us to review the range of values for each variable, and we can sort based on a single or multiple variables.\nNotice the difference between sort() and arrange() functions below.\n\nThe sort() function sorts a vector.\nThe arrange() function sorts a dataset based on a variable.\n\nTo conduct an example, read in the data set called gig.csv from your working directory.\n\n\ngig &lt;- read.csv(\"data/gig.csv\", stringsAsFactors = TRUE, na.strings = \"\")\ndim(gig)\n\n[1] 604   4\n\nhead(gig)\n\n  EmployeeID  Wage     Industry        Job\n1          1 32.81 Construction    Analyst\n2          2 46.00   Automotive   Engineer\n3          3 43.13 Construction  Sales Rep\n4          4 48.09   Automotive      Other\n5          5 43.62   Automotive Accountant\n6          6 46.98 Construction   Engineer\n\n\n\nUsing the arrange() function, we add the dataset, followed by a comma and then add in the variable we want to sort. This arranges from small to large.\nBelow is code to rearrange data based on Wage and save it in a new object.\n\n\nsortTidy &lt;- arrange(gig, Wage)\nhead(sortTidy)\n\n  EmployeeID  Wage     Industry        Job\n1        467 24.28 Construction   Engineer\n2        547 24.28 Construction  Sales Rep\n3        580 24.28 Construction Accountant\n4        559 24.42 Construction   Engineer\n5         16 24.76   Automotive Programmer\n6        221 24.76   Automotive Programmer\n\n\n\nWe can apply a desc() function inside the arrange function to re-sort from high to low like shown below.\n\n\nsortTidyDesc &lt;- arrange(gig, desc(Wage))\nhead(sortTidyDesc)\n\n  EmployeeID  Wage     Industry        Job\n1        110 51.00 Construction      Other\n2         79 50.00   Automotive   Engineer\n3        348 49.91 Construction Accountant\n4        373 49.91 Construction Accountant\n5        599 49.84   Automotive   Engineer\n6         70 49.77 Construction Accountant\n\n\n\n\n4.3.2 Subsetting or Filtering\n\nSubsetting or filtering a data frame is the process of indexing, or extracting a portion of the data set that is relevant for subsequent statistical analysis. Subsetting allows you to work with a subset of your data, which is essential for data analysis and manipulation. One of the most common ways to subset in R is by using square brackets []. We can also use the filter() function from tidyverse.\nWe use subsets to do the following:\n\nView data based on specific data values or ranges.\nCompare two or more subsets of the data.\nEliminate observations that contain missing values, low-quality data, or outliers.\nExclude variables that contain redundant information, or variables with excessive amounts of missing values.\n\nWhen working with data frames, you can subset by rows and columns using two indices inside the square brackets: data[row, column]. For example, if you have df &lt;- data.frame(a = 1:3, b = c(“X”, “Y”, “Z”)), df[1, 2] would return the value “X”, which is the first row and second column. If you want the entire first row, you would use df[1, ], and to get the second column, you’d use df[, 2].\nYou can also use logical conditions to subset. For instance, x[x &gt; 20] would return all values in x greater than 20, and in a data frame, you could filter rows where a certain condition holds, such as df[df$a &gt; 1, ], which returns rows where column a has values greater than 1.\nLet’s do an example using the customers.csv file we read in earlier as customers in the last lesson. Base R provides several methods for subsetting data structures. Below uses base R by using the square brackets dataset[row, column] format.\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\", stringsAsFactors = TRUE)\n\n# To subset, note the dataset[row,column] format Results hidden to\n# save space, but be sure to try this code in your .R file.  Data in\n# 1st row\ncustomers[1, ]\n# Data in 2nd column\ncustomers[, 2]\n# Data for 2nd column/1st observation (row)\ncustomers[1, 2]\n# First 3 columns of data\ncustomers[, 1:3]\n\n\nOne of the most powerful and intuitive ways to subset data frames in R is by using the filter() function from the dplyr package, which is part of the tidyverse. Tidyverse is extremely popular when filtering data.\nThe filter function is used to subset rows of a data frame based on certain conditions.\nThe below example filters data by the College variable when category values are “Yes” and saves the filtered dataset into an object called college.\n\n\n# Filtering by whether the customer has a 'Yes' for college.  Saving\n# this filter into a new object college which you should see in your\n# global environment.\ncollege &lt;- filter(customers, College == \"Yes\")\n# Showing first 6 records of college - note the College variable is\n# all Yes's.\nhead(college)\n\n   CustID    Sex     Race  BirthDate College HHSize Income Spending Orders\n1 1530016 Female    Black 12/16/1986     Yes      5  53000      241      3\n2 1531136   Male    White   5/9/1993     Yes      5  94000      843     12\n3 1532160   Male    Black  5/22/1966     Yes      2  64000      719      9\n4 1532307   Male    White  9/16/1964     Yes      4  60000      582     13\n5 1532387   Male    White  8/27/1957     Yes      2  67000      452      9\n6 1533017 Female Hispanic  5/14/1985     Yes      3  84000      153      2\n  Channel\n1      SM\n2      TV\n3      TV\n4      SM\n5      SM\n6     Web\n\n\n\nUsing the filter command, we can add filters pretty easily by using an & for and, or an | for or. The statement below filters by College and Income and save the new dataset in an object called twoFilters.\n\n\ntwoFilters &lt;- filter(customers, College == \"Yes\" & Income &lt; 50000)\nhead(twoFilters)\n\n   CustID    Sex     Race  BirthDate College HHSize Income Spending Orders\n1 1533697 Female    Asian  10/8/1974     Yes      3  42000      247      3\n2 1535063 Female    White 12/17/1982     Yes      3  42000      313      4\n3 1544417   Male Hispanic  3/14/1980     Yes      4  46000      369      3\n4 1547864 Female Hispanic  6/15/1987     Yes      2  44000      500      5\n5 1550969 Female    White   4/8/1978     Yes      4  47000      774     16\n6 1553660 Female    White   8/2/1988     Yes      2  47000      745      5\n  Channel\n1     Web\n2      TV\n3      TV\n4      TV\n5      TV\n6      SM\n\n\n\nNext, we can do an or statement. The example below uses the filter command to filter by more than one category in the same field using the | in between the categories.\n\n\nTwoRaces &lt;- filter(customers, Race == \"Black\" | Race == \"White\")\nhead(TwoRaces)\n\n   CustID    Sex  Race  BirthDate College HHSize Income Spending Orders Channel\n1 1530016 Female Black 12/16/1986     Yes      5  53000      241      3      SM\n2 1531136   Male White   5/9/1993     Yes      5  94000      843     12      TV\n3 1532160   Male Black  5/22/1966     Yes      2  64000      719      9      TV\n4 1532307   Male White  9/16/1964     Yes      4  60000      582     13      SM\n5 1532387   Male White  8/27/1957     Yes      2  67000      452      9      SM\n6 1533791   Male White 10/27/1999     Yes      1  97000     1028     17     Web\n\n\n\nThe str_detect() function is used to detect the presence or absence of a pattern (regular expression) in a string or vector of strings. It returns a logical vector indicating whether the pattern was found in each element of the input vector.\nUsing str_detect it with a filter function allows you to pull observations based on the inclusion of a string pattern.\n\n\nlibrary(tidyverse)\nBirthday2000 &lt;- filter(customers, str_detect(BirthDate, \"1985\"))\n\n\n\n4.3.3 Select\n\nIn R, the select() function is part of the dplyr package, which is used for data manipulation. The select() function is specifically designed to subset or choose specific columns from a data frame. It allows you to select variables (columns) by their names or indices.\nBoth statements below select Income, Spending, and Orders variables from the customers dataset and form them into a new dataset called smallData.\nThe statements are written with and without the chaining operator.\n\n\nsmallData &lt;- select(customers, Income, Spending, Orders)\nhead(smallData)\n\n  Income Spending Orders\n1  53000      241      3\n2  94000      843     12\n3  64000      719      9\n4  60000      582     13\n5  47000      845      7\n6  67000      452      9\n\n\n\n\n4.3.4 Piping (Chaining) Operator\n\nThe pipe operator takes the output of the expression on its left-hand side and passes it as the first argument to the function on its right-hand side. This enables you to chain multiple functions together, making the code easier to understand and debug.\nIf we want to keep our code tidy, we can add the piping operator (%&gt;%) to help combine our lines of code into a new object or overwriting the same object.\nThis operator allows us to pass the result of one function/argument to the other one in sequence.\nThe below example uses a select function to pull Income, Spending, and Orders variables fromt he customers dataset and save it as a new object called smallData. It is an identical request to the one directly above, but written with the piping operator.\n\n\nsmallData &lt;- customers %&gt;%\n    select(Income, Spending, Orders)\n\n\n\n4.3.5 Counting\n\nCounting allows us to gain a better understanding and insights into the data.\nThis helps to verify that the data set is complete or determine if there are missing values.\nIn R, the length() function returns the number of elements in a vector, list, or any other object with a length attribute. It essentially counts the number of elements in the specified object.\n\n\n# Gives the length of Industry\nlength(gig$Industry)\n\n[1] 604\n\n\n\nFor counting using tidyverse, we typically use the filter and count function together to filter by a value or state and then count the filtered data.\nIn the function below, I use the piping operator to link together the filter and count functions into one command.\nNote that we need a piping operator (%&gt;%) before each new function that is part of the chunk.\n\n\n# Counting with a Categorical Variable Here we are filtering by\n# Automotive Industry and then counting the number and saving it in a\n# new object called countAuto\ncountAuto &lt;- gig %&gt;%\n    filter(Industry == \"Automotive\") %&gt;%\n    count()\ncountAuto  #190\n\n    n\n1 190\n\n\n\nBelow, we are filtering by Wage and the counting.\n\n\n# Counting with a Numerical Variable We could also save this in an\n# object.\ngig %&gt;%\n    filter(Wage &gt; 30) %&gt;%\n    count()  ##536\n\n    n\n1 536\n\n\n\nWe learned that there are 190 employees in the automotive industry and there are 536 employees who earn more than $30 per hour.\nWe could also calculate the number of people with wages under or equal to 30.\n\n\n# We find 68 Wages under or equal to 30\nWageLess30 &lt;- gig %&gt;%\n    filter(Wage &lt;= 30) %&gt;%\n    count()  #\nWageLess30\n\n   n\n1 68\n\n\n\nHow many Accountants are there in the Job Category of the gig data set. The answer is shown below. Use filter and count to calculate this answer.\n\n\n\n   n\n1 83\n\n\n\n\n4.3.6 Handling Missing Data\n\nAfter a data set is loaded, there are two common strategies for dealing with missing values.\n\n\nThe omission strategy recommends that observations with missing values be excluded from subsequent analysis.\nThe imputation strategy recommends that the missing values be replaced with some reasonable imputed values.\n\nNumeric variables: replace with the average.\nCategorical variables: replace with the predominant category.\n\n\n\n4.3.6.1 Limitations of Using a Missing Data Technique\n\nRecommended Closer Evaluation of Missing Data\nThere are limitations of both techniques listed above (omission and imputation).\n\nIf a large number of values are missing, mean imputation will likely distort the relationships among variables, leading to biased results.\nRemoving missing values could also significantly reduce your data set size.\nMissing data needs to be closely evaluated and verified within each variable whether the data is truly blank, has no answer, or is marked with a character value such as the text N/A.\nIf the variable that has many missing values is deemed unimportant or can be represented using a proxy variable that does not have missing values, the variable may be excluded from the analysis.\n\nMissing data needs to be closely evaluated to see if the missing value is meaningful or not.\n\nFor instance, getting data on how many pregnancies would only be applicable to people born of women gender, and blank value for people born of male gender, who are unable to have children, would be expected. In taking this example further, if variable 1 targeted the question, “how many pregnancies have you have had,” we would expect missing data or NAs for all the men. If comparing that variable to a second variable “Incubated from COVID-19: Yes/No” we would not want to omit all the blanks in the dataset because then we would eliminate analysis of an entire gender. Thus a different technique should be chosen besides omitting the blanks to be able to evaluate more concisely.\n\n\nIf a value is not blank and is considered missing, data needs to be mutated to be consistent with the technique of coding true missing values.\n\n\n\n4.3.6.2 The na.rm Parameter\n\ny &lt;- c(1, 2, NA, 3, 4, NA)\n# These lines runs, but do not give you anything useful.\nsum(y)\n\n[1] NA\n\nmean(y)\n\n[1] NA\n\n\n\nMany functions in R include parameters that will ignore NAs for you.\n\nsum() and mean() are examples of this, and most summary statistics like median() and var() also use the na.rm parameter to ignore the NAs. Always check the help to determine if na.rm is a parameter.\n\n\nsum(y, na.rm = TRUE)\n\n[1] 10\n\nmean(y, na.rm = TRUE)\n\n[1] 2.5\n\n# na.omit removes the NAs from the vector of dataset. Here, it works\n# for removing NAs from the vector.\ny &lt;- na.omit(y)\n\n\n\n\n4.3.6.3 is.na()\n\nIn R, the is.na() function is used to check for missing (NA) values in objects like vectors, data frames, or arrays. It returns a logical vector of the same length as the input object, where TRUE indicates a missing value and FALSE indicates a non-missing value.\n\n\n# Counts the number of all NA values in the entire dataset\nCountAllBlanks &lt;- sum(is.na(gig))\nCountAllBlanks\n\n[1] 26\n\n# Gives the observation number of the observations that include NA\n# values\nwhich(is.na(gig$Industry))\n\n [1]  24 139 361 378 441 446 479 500 531 565\n\n# Produces a dataset with observations that have NA values in the\n# Industry field.\nShowBlankObservations &lt;- gig %&gt;%\n    filter(is.na(Industry))\nShowBlankObservations\n\n   EmployeeID  Wage Industry        Job\n1          24 42.58     &lt;NA&gt;  Sales Rep\n2         139 42.18     &lt;NA&gt;   Engineer\n3         361 31.33     &lt;NA&gt;      Other\n4         378 48.09     &lt;NA&gt;      Other\n5         441 32.35     &lt;NA&gt; Accountant\n6         446 30.76     &lt;NA&gt; Accountant\n7         479 42.85     &lt;NA&gt; Consultant\n8         500 43.13     &lt;NA&gt;  Sales Rep\n9         531 43.13     &lt;NA&gt;   Engineer\n10        565 38.98     &lt;NA&gt; Accountant\n\n# Counts the number of observations that have NA values in the\n# Industry field. Industry is categorical, so we can count values\n# based on it.\nCountBlanks &lt;- sum(is.na(gig$Industry))\nCountBlanks\n\n[1] 10\n\nlibrary(tidyverse)\n# Counts the number of observations that have NA values in the Wage\n# field.\nCountBlanks &lt;- sum(is.na(gig$Wage))\nCountBlanks\n\n[1] 0\n\n\n\n\n4.3.6.4 Using na_if()\n\nThe na_if() function in tidyr is used to replace specific values in a column with NA (missing) values. This function can be particularly useful when you want to standardize missing values across a dataset or when you want to replace certain values with NA for further data processing\n\n\nTurnNA &lt;- gig %&gt;%\n    mutate(Job = na_if(Job, \"Other\"))\nhead(TurnNA)\n\n  EmployeeID  Wage     Industry        Job\n1          1 32.81 Construction    Analyst\n2          2 46.00   Automotive   Engineer\n3          3 43.13 Construction  Sales Rep\n4          4 48.09   Automotive       &lt;NA&gt;\n5          5 43.62   Automotive Accountant\n6          6 46.98 Construction   Engineer\n\n\n\n\n4.3.6.5 na.omit() vs. drop_na()\n\nBoth functions return a new object with the rows containing missing values removed.\nna.omit() is a base R function, so it doesn’t require any additional package installation where drop_na() requires loading the tidyr package, which is part of the tidyverse ecosystem.\ndrop_na() fits well into tidyverse pipelines, making it easy to integrate with other tidyverse functions where na.omit() can also be used in pipelines but might require additional steps to fit seamlessly.\n\n\n# install.packages('Amelia')\nlibrary(Amelia)\ndata(\"africa\")\nsummary(africa)\n\n      year              country       gdp_pc            infl        \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.400  \n 1st Qu.:1977   Burundi     :20   1st Qu.: 513.8   1st Qu.:  4.760  \n Median :1982   Cameroon    :20   Median :1035.5   Median :  8.725  \n Mean   :1982   Congo       :20   Mean   :1058.4   Mean   : 12.753  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1244.8   3rd Qu.: 13.560  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.890  \n                                  NA's   :2                         \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4332190  \n Median : 59.59   Median :0.1667   Median : 5853565  \n Mean   : 62.60   Mean   :0.2889   Mean   : 5765594  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7355000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n NA's   :5                                           \n\nsummary(africa$gdp_pc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  376.0   513.8  1035.5  1058.4  1244.8  2723.0       2 \n\nsummary(africa$trade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.35   38.52   59.59   62.60   81.16  134.11       5 \n\nafrica1 &lt;- na.omit(africa)\nsummary(africa1)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n## to drop all at once.\nafrica2 &lt;- africa %&gt;%\n    drop_na()\nsummary(africa2)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n\n\nYou try to load the airquality dataset from base R and look at a summary of the dataset.\n\nSum the number of NAs in airquality.\nOmit all the NAs from airquality and save it in a new data object called airqual and take a new summary of it.\n\n\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n\n\n\n[1] 44\n\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n\n\n\n\n\n\n4.3.7 Summarize\n\nThe summarize() command is used to create summary statistics for groups of observations in a data frame.\nIn R, summary() and summarize() serve different purposes. summary() is part of base R and gives a quick overview of data, returning descriptive statistics for each column. For example, summary(mtcars) provides the min, max, median, and mean for numeric columns and counts for factors. It’s useful for a broad snapshot of your dataset.\nIn contrast, summarize() (or summarise()) is from the dplyr package and allows for custom summaries. For instance, mtcars %&gt;% summarize(avg_mpg = mean(mpg), max_hp = max(hp)) returns the average miles per gallon and the maximum horsepower. It’s more flexible and is often used with group_by() for grouped calculations. In conclusion, summary() gives automatic overviews, while summarize() is better for tailored summaries.\nIn the example below, we can summarize more than one thing into tidy output.\n\n\ngig %&gt;%\n    drop_na() %&gt;%\n    summarize(mean.days = mean(Wage), sd.days = sd(Wage), var.days = var(Wage),\n        med.days = median(Wage), iqr.days = IQR(Wage))\n\n  mean.days  sd.days var.days med.days iqr.days\n1  40.14567 7.047058 49.66103    41.82   11.465\n\n\n\n\n4.3.8 Group_by\n\ngroup_by is used for grouping data by one or more variables. When you use group_by() on a data frame, it doesn’t actually perform any computations immediately. Instead, it sets up the data frame in such a way that any subsequent operations are performed within these groups\nsummarize() is often used in combination with group_by() to calculate summary statistics within groups\n\n\n## summarize data by Industry variable.\ngroupedData &lt;- gig %&gt;%\n    group_by(Industry) %&gt;%\n    summarize(meanWage = mean(Wage))\ngroupedData\n\n# A tibble: 4 × 2\n  Industry     meanWage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Automotive       43.4\n2 Construction     38.3\n3 Tech             40.7\n4 &lt;NA&gt;             39.5\n\n## same function with na's dropped.\ngroupedData &lt;- gig %&gt;%\n    drop_na() %&gt;%\n    group_by(Industry) %&gt;%\n    summarize(meanWage = mean(Wage))\ngroupedData\n\n# A tibble: 3 × 2\n  Industry     meanWage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Automotive       43.4\n2 Construction     38.4\n3 Tech             40.7\n\n\n\n\n4.3.9 Mutate\n\nmutate() is part of the dplyr package, which is used for data manipulation. The mutate() function is specifically designed to create new variables (columns) or modify existing variables in a data frame. It is commonly used in data wrangling tasks to add calculated columns or transform existing ones.\nOne example is below, but note that there are many things you can do with the mutate function.\n\n\n# making a new variable called calculation that multiplies gdp_pc by\n# infl variables in the africa1 dataset.\nafrica.mutated &lt;- mutate(africa1, calculation = gdp_pc * infl)\nhead(africa.mutated)\n\n  year      country gdp_pc  infl trade    civlib population calculation\n1 1972 Burkina Faso    377 -2.92 29.69 0.5000000    5848380    -1100.84\n2 1973 Burkina Faso    376  7.60 31.31 0.5000000    5958700     2857.60\n3 1974 Burkina Faso    393  8.72 35.22 0.3333333    6075700     3426.96\n4 1975 Burkina Faso    416 18.76 40.11 0.3333333    6202000     7804.16\n5 1976 Burkina Faso    435 -8.40 37.76 0.5000000    6341030    -3654.00\n6 1977 Burkina Faso    448 29.99 41.11 0.6666667    6486870    13435.52\n\n\n\nBelow is an example with the iris dataset, which is part of base R.\n\n\ndata(\"iris\")\n## Selecting 2 variables from the iris dataset: Sepal.Length and\n## Petal.Length\nselected_data &lt;- select(iris, Sepal.Length, Petal.Length)\nhead(selected_data)\n\n  Sepal.Length Petal.Length\n1          5.1          1.4\n2          4.9          1.4\n3          4.7          1.3\n4          4.6          1.5\n5          5.0          1.4\n6          5.4          1.7\n\n# Filter rows based on a condition: Species = setosa\nfiltered_data &lt;- filter(iris, Species == \"setosa\")\nhead(filtered_data)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Arrange rows by the Sepal.Length column\narranged_data &lt;- arrange(iris, Sepal.Length)\n# Create a new column by mutating the data by transforming\n# Petal.Width to the log form.\nmutated_data &lt;- mutate(iris, Petal.Width_Log = log(Petal.Width))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#full-examples",
    "href": "dataprep.html#full-examples",
    "title": "4  Data Preparation",
    "section": "4.4 Full Examples",
    "text": "4.4 Full Examples\n\n4.4.1 gss.2016 Data Cleaning\n\nFirst, because we made some edits to the data set, reread in version a using the read.csv command. This brings the data set back to its original form. It is always a good idea to read the dataset back in when you are unsure about whether you have made a mistake during data preparation that could cause a lack of data integrity.\n\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n\n\nBefore we remove any missing data, we need it to be the correct data type. In this case, grass should be a factor.\n\n\n# We coerced this variable earlier, but the object was called\n# gss.2016.  Since we reread in the data set, this needs to be done\n# again.\ngss.2016$grass &lt;- as.factor(gss.2016$grass)\n\n\nThe statement below is an equivalent to the function above, but written with the piping operator. It is overwriting gss.2016 after conducting the coercion to factor.\nWe added the mutate function because we are going to add other data cleaning tasks to this statement.\n\n\ngss.2016 &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass))\n\n\n4.4.1.1 Piping to More Functions: Missing Data\n\nIn the code below, the as.factor() command has been moved inside a broader mutate statement (that uses tidyverse library) and piped to it the na_if() command that handles missing data. If you use more than one data manipulation statement, the mutate() command is needed to help organize your code with one mutate() is needed for each major change you are making.\nIn the code below, we created a new object gss.2016.cleaned to help store the cleaned version of the dataset. This helps maintain data integrity because your original dataset is still intact and each time, you rerun the entire chunk, which includes all the changes at one time.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    # Moved coercion statement into a mutate function to keep code\n    # tidy\nmutate(grass = as.factor(grass)) %&gt;%\n    # Moving DK value to NA for not applicable\nmutate(grass = na_if(x = grass, y = \"DK\"))\n\n# Check the summary, there should be 110 + 3 = 113 in the NA category\nsummary(object = gss.2016.cleaned)\n\n       grass          age           \n DK       :   0   Length:2867       \n IAP      : 911   Class :character  \n LEGAL    :1126   Mode  :character  \n NOT LEGAL: 717                     \n NA's     : 113                     \n\n\n\n\n4.4.1.2 Drop Levels\n\nThe droplevels function is part of base R and is used to drop unused levels from factor variables in a data frame. It works by removing any levels from a factor variable that are not present in the data.\nNext, we want to edit our code to convert IAP and DK to NA values and drop levels that have are empty.\n\nNote the Piping operator added to the end of the DK line so you can keep going with new commands editing gss.2016.cleaned.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    # Added piping operator\nmutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    # Turn to na if value of grass = IAP\nmutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    # Drop levels in grass that have no values\nmutate(grass = droplevels(x = grass))\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass          age           \n LEGAL    :1126   Length:2867       \n NOT LEGAL: 717   Class :character  \n NA's     :1024   Mode  :character  \n\n\n\n\n\n4.4.1.3 Coercing to Numeric\n\nNext, we handle a numerical variable, age. Age again has an issue being able to be numerical data type because it has “89 OR OLDER” as a value. Before using the as.numeric() command, we need to recode it. We did this above as a stand-alone statement.\n\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    # Added piping operator\nmutate(grass = droplevels(x = grass)) %&gt;%\n    # Ensure variable can be coded as numeric and fix if necessary.\nmutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    # Coerce into numeric\nmutate(age = as.numeric(x = age))\n\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass           age       \n LEGAL    :1126   Min.   :18.00  \n NOT LEGAL: 717   1st Qu.:34.00  \n NA's     :1024   Median :49.00  \n                  Mean   :49.16  \n                  3rd Qu.:62.00  \n                  Max.   :89.00  \n                  NA's   :10     \n\n\n\nThe recode() command that is part of dplyr is like the ifelse() command that is in base R. There are a lot of ways to recode in R.\nFinally, we want to take our numerical variable, age, and cut it at certain breaks to make categories that can be easily analyzed.\n\nThis also ensures that anyone above 89 is coded correctly in a category instead of as the value 89. This again brings back data integrity.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nHere, we are using it to cut age into a categorical variable.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    # Added piping operator\nmutate(age = as.numeric(age)) %&gt;%\n    # Cut numeric variable into groupings\nmutate(age.cat = cut(age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n    \"30 - 59\", \"60 - 74\", \"75+\")))\n\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass           age           age.cat    \n LEGAL    :1126   Min.   :18.00   &lt; 30   : 481  \n NOT LEGAL: 717   1st Qu.:34.00   30 - 59:1517  \n NA's     :1024   Median :49.00   60 - 74: 598  \n                  Mean   :49.16   75+    : 261  \n                  3rd Qu.:62.00   NA's   :  10  \n                  Max.   :89.00                 \n                  NA's   :10                    \n\n\n\n\n\n\n4.4.2 brfss Data Cleaning\n\nThe full codebook where this screenshot is taken is brfss_2014_codebook.pdf.\n\n\n\n\nEvaluate CodeBook Before Making Decisions\n\n\n\nbrfss &lt;- read.csv(\"data/brfss.csv\")\nsummary(brfss)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n\n\n4.4.2.1 Qualitative Variable\n\nTo look at an example, the one below seeks to understand the healthcare issue in reporting gender based on different definitions. The dataset is part of the Behavioral Risk Factor Surveillance System (brfss) dataset (2014), which includes lots of other variables besides reported gender.\n\n\n# Load the data\nbrfss &lt;- read.csv(\"data/brfss.csv\")\n# Summarize the TRNSGNDR variable\nsummary(object = brfss$TRNSGNDR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    4.00    4.00    4.06    4.00    9.00  310602 \n\n# Find frequencies\ntable(brfss$TRNSGNDR)\n\n\n     1      2      3      4      7      9 \n   363    212    116 150765   1138   1468 \n\n\n\nSince this table is not very informative, we need to do some edits.\nCheck the class of the variable to see the issue with analyzing it as a categorical variable.\n\n\nclass(brfss$TRNSGNDR)\n\n[1] \"integer\"\n\n\n\nFirst, we need to change the TRNSGNDR variable to a factor using as.factor().\n\n\n# Change variable from numeric to factor\nbrfss$TRNSGNDR &lt;- as.factor(brfss$TRNSGNDR)\n# Check data type again to ensure factor\nclass(brfss$TRNSGNDR)\n\n[1] \"factor\"\n\n\n\nThen, we need to do some data cleaning on the TRNSGNDR Variable.\n\n\nbrfss.cleaned &lt;- brfss %&gt;% \n  mutate(TRNSGNDR = recode_factor(TRNSGNDR,\n      '1' = 'Male to female',\n      '2' = 'Female to male',\n      '3' = 'Gender non-conforming',\n      '4' = 'Not transgender',\n      '7' = 'Not sure',\n      '9' = 'Refused'))\n\n\nWe can use the levels() command to show the factor levels made with the mutate() command above.\n\n\nlevels(brfss.cleaned$TRNSGNDR)\n\n[1] \"Male to female\"        \"Female to male\"        \"Gender non-conforming\"\n[4] \"Not transgender\"       \"Not sure\"              \"Refused\"              \n\n\n\nCheck the summary.\n\n\nsummary(brfss.cleaned$TRNSGNDR)\n\n       Male to female        Female to male Gender non-conforming \n                  363                   212                   116 \n      Not transgender              Not sure               Refused \n               150765                  1138                  1468 \n                 NA's \n               310602 \n\n\n\nTake a good look at the table to interpret the frequencies in the output above. The highest percentage was the “NA’s” category, followed by “Not transgender”. Removing the NA’s moved the “Not transgender” category to over 97% of observations.\n\n\n\n4.4.2.2 Quantitative Variable\n\nLet’s use the cleaned dataset to make more changes to the continuous variable PHYSHLTH. In the codebook, it looks like the data is most applicable to the first 2 categories. The 1-30 days coding and the 88 coding, which means 0 days of physical illness and injury.\n\nUsing cleaned data, we need to prep the variable a little more before getting an accurate plot.\nSpecifically, we need to null out the 77 and 99 values and make sure the 88 coding is set to be 0 for 0 days of illness and injury.\n\n\n\nbrfss.cleaned &lt;- brfss %&gt;%\n    mutate(TRNSGNDR = recode_factor(TRNSGNDR, `1` = \"Male to female\", `2` = \"Female to male\",\n        `3` = \"Gender non-conforming\", `4` = \"Not transgender\", `7` = \"Not sure\",\n        `9` = \"Refused\")) %&gt;%\n    # Turn the 77 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to be numeric value of 0.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\n\n\nThe histogram showed most people have between 0 and 10 unhealthy days per 30 days.\nNext, evaluate mean, median, and mode for the PHYSHLTH variable after ignoring the blanks.\n\n\nmean(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 4.224106\n\nmedian(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 0\n\nnames(x = sort(x = table(brfss.cleaned$PHYSHLTH), decreasing = TRUE))[1]\n\n[1] \"0\"\n\n\n\nWhile the mean is higher at 4.22, the median and most common number is 0.\n\n\n## Spread to Report with the Mean\nvar(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 77.00419\n\nsd(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 8.775203\n\n## Spread to Report with Median\nsummary(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.000   0.000   4.224   3.000  30.000   10303 \n\nrange(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1]  0 30\n\nmax(brfss.cleaned$PHYSHLTH, na.rm = TRUE) - min(brfss.cleaned$PHYSHLTH,\n    na.rm = TRUE)\n\n[1] 30\n\nIQR(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 3\n\n\n\nlibrary(semTools)\n# Plot the data\nbrfss.cleaned %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram()\n\n\n\n\n\n\n\n# Calculate Skewness and Kurtosis\nskew(brfss.cleaned$PHYSHLTH)\n\n   skew (g1)           se            z            p \n2.209078e+00 3.633918e-03 6.079054e+02 0.000000e+00 \n\nkurtosis(brfss.cleaned$PHYSHLTH)\n\nExcess Kur (g2)              se               z               p \n   3.474487e+00    7.267836e-03    4.780634e+02    0.000000e+00 \n\n\n\nThe skew results provide a z of 607.905 (6.079054e+02) which is much higher than 7 (for large datasets). This indicates a clear right skew which means the data is not normally distributed.\nThe kurtosis results are also very leptokurtic with a score of 478.063.\n\n\n\n4.4.2.3 Using Filters Example\n\nBelow takes an example of the brfss data to filter by certain variable statuses.\n\nThe first filter() chose observations that were any one of the three categories of transgender included in the data. Used the | “or” operator for this filter().\nThe second filter chose people in an age category above category 4 but below category 12, in the age categories 5 through 11.\nThe last filter used the !is.na to choose observations where HADMAM variable was not NA.\n\nNext, we reduce data set to contain only variables used to create table by using the select() command.\nNext, we change all the remaining variables in data set to factors using mutate_all() command. This not only changes the strings to factors, but also changes the numerical variables to factors.\nFinally, we use mutate() commands to change the variable category to something meaningful(from the codebook).\n\nNotice the backslash before the apostrophe in Don’t in the X_INCOMG recode. This is to prevent the .R file from ending the quotations. You could use double quotes around the statement to bypass this, or add the backslash like I did here.\n\n\nbrfss_small &lt;- brfss.cleaned %&gt;%\n    filter(TRNSGNDR == \"Male to female\" | TRNSGNDR == \"Female to male\" |\n        TRNSGNDR == \"Gender non-conforming\") %&gt;%\n    filter(X_AGEG5YR &gt; 4 & X_AGEG5YR &lt; 12) %&gt;%\n    filter(!is.na(HADMAM)) %&gt;%\n    select(TRNSGNDR, X_AGEG5YR, X_RACE, X_INCOMG, X_EDUCAG, HLTHPLN1, HADMAM) %&gt;%\n    mutate_all(as.factor) %&gt;%\n    # The next few mutates add labels to categorical variables based\n    # on the codebook.\nmutate(X_AGEG5YR = recode_factor(X_AGEG5YR, `5` = \"40-44\", `6` = \"45-49\",\n    `7` = \"50-54\", `8` = \"55-59\", `9` = \"60-64\", `10` = \"65-69\", `11` = \"70-74\")) %&gt;%\n    mutate(X_INCOMG = recode_factor(X_INCOMG, `1` = \"Less than 15,000\",\n        `2` = \"15,000 to less than 25,000\", `3` = \"25,000 to less than 35,000\",\n        `4` = \"35,000 to less than 50,000\", `5` = \"50,000 or more\", `9` = \"Don't know/not sure/missing\")) %&gt;%\n    mutate(X_EDUCAG = recode_factor(X_EDUCAG, `1` = \"Did not graduate high school\",\n        `2` = \"Graduated high school\", `3` = \"Attended college/technical school\",\n        `4` = \"Graduated from college/technical school\", `9` = NA_character_)) %&gt;%\n    mutate(HLTHPLN1 = recode_factor(HLTHPLN1, `1` = \"Yes\", `2` = \"No\",\n        `7` = \"Don't know/not sure/missing\", `9` = \"Refused\")) %&gt;%\n    mutate(X_RACE = recode_factor(X_RACE, `1` = \"White\", `2` = \"Black\",\n        `3` = \"Native American\", `4` = \"Asian/Pacific Islander\", `5` = \"Other\",\n        `6` = \"Other\", `7` = \"Other\", `8` = \"Other\", `9` = \"Other\"))\n# print a summary\nsummary(brfss_small)\n\n                  TRNSGNDR   X_AGEG5YR                     X_RACE   \n Male to female       : 77   40-44:27   White                 :152  \n Female to male       :113   45-49:27   Black                 : 31  \n Gender non-conforming: 32   50-54:32   Native American       :  4  \n Not transgender      :  0   55-59:44   Asian/Pacific Islander:  6  \n Not sure             :  0   60-64:44   Other                 : 29  \n Refused              :  0   65-69:24                               \n                             70-74:24                               \n                        X_INCOMG                                     X_EDUCAG \n Less than 15,000           :46   Did not graduate high school           :24  \n 15,000 to less than 25,000 :44   Graduated high school                  :86  \n 25,000 to less than 35,000 :19   Attended college/technical school      :68  \n 35,000 to less than 50,000 :26   Graduated from college/technical school:44  \n 50,000 or more             :65                                               \n Don't know/not sure/missing:22                                               \n\n HLTHPLN1  HADMAM \n Yes:198   1:198  \n No : 24   2: 22  \n           9:  2  \n\n\n\n\n\n\nThis data set full of categorical variables is now fully cleaned and ready to be analyzed!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#summary",
    "href": "dataprep.html#summary",
    "title": "4  Data Preparation",
    "section": "4.6 Summary",
    "text": "4.6 Summary\n\nIn this lesson, we worked through the basics on data cleaning. Data cleaning is so important and there are so many ways to do it. Provided are some examples using popular functions in dplyr (under tidyverse).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-categorical-variable",
    "href": "dataviz.html#graphs-for-a-single-categorical-variable",
    "title": "5  Data Visualization",
    "section": "5.2 Graphs for a Single Categorical Variable",
    "text": "5.2 Graphs for a Single Categorical Variable\n\nA categorical variable has categories that are either ordinal with a logical order or nominal with no logical order.\nCategorical variables need to be set as the factor data type in R to be able to be analyzed and visualized correctly.\nSome common graphing options for single categorical variable:\n\nBar graph\nPie chart\n\nIn any graph, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With categorical variables, this may require recoding the factor(s) of interest and possibly renaming it/them to something meaningful if needed.\n\n\n5.2.1 Bar Graph\n\nA bar graph depicts the frequency or the relative frequency for each category of the qualitative data as a bar rising vertically from the horizontal axis. A bar graph is also known as a bar chart and is often used to examine similarities and differences across categories of things; bars can represent frequencies, percentages, means, or other statistics.\nWe can learn a lot from a bar graph, like the marital status group with the highest and lowest frequencies according to the census.gov.\n\n\n\n\n\n\n\n\n\n\n\n5.2.1.1 geom_bar()\n\nCreate a Bar Graph using ggplot() Command\nLike histograms, ggplot has many more parameters available over base R to construct bar graphs.\nFirst, we load tidyerse to access ggplot() command and others. You can always do this at the start of all your code to keep all the libraries together that are being used.\n\n\nlibrary(\"tidyverse\")\n\n\nggplot() works in layers, so you will routinely see the + symbol to kick off a new layer with added functionality.\nUsing the ggplot, we always include the aes() command first inside the ggplot() command. The aes() command is a quoting function that describes the variables being used. From there, it depends on the plot.\n\nFirst layer: ggplot() and aes() which calls the dataset and variables used.\nSecond layer: Graph type: Bar graph: geom_bar().\nAdditional layers: labs - for labels including titles; themes; and geom_text. Recreate the example below adding one layer at a time to see how the visualization changes.\n\n\n\n\n5.2.1.2 Pertinent Parameters to the geom_bar()\n\nstat=“identity”: In ggplot2, the argument stat=“identity” is used inside the geom_bar when creating bar charts to specify that the actual values of the data should be plotted, rather than calculated summary statistics. By default, ggplot assumes that bar charts use stat=“count”, which counts the number of observations in each category. When you want to display the actual values in your dataset (e.g., sales numbers, average scores), you need to set stat=“identity” to ensure that the y-values are mapped directly from the data rather than being automatically counted.\nYou only need to specify stat=“identity” if you have pre-calculated values to use in the barplot. In the example below, we do have pre-calculated numbers going into make the chart, so stat=“identity” is used. Here, we are using exact values, but we can also compute a dataset before graphing using a group_by() and a variety of counting function.\nposition=“dodge”: If we have more than one categorical variable, we might set the position=“dodge” argument. This argument controls how the bars are positioned relative to each other when you are plotting multiple categories within a bar chart. By default, bars in a bar chart are stacked, and setting position=“dodge” ensures that they are placed side-by-side. This is particularly useful when you are comparing multiple groups or categories within the same plot, as it allows for a clear visual distinction between each group. In the example below, we only have one variable, so position=“dodge” argument is not needed.\n\nIn combination, using stat=“identity” and position=“dodge” is common when you want to compare different categories or groups with specific values in a side-by-side manner, ensuring the chart is easy to interpret. For example, if you are comparing sales figures across different products for multiple years, this approach would give you a bar for each product in each year, clearly separated.\nshow.legend = FALSE: In ggplot2, the argument show.legend = FALSE is used to hide the legend for a particular layer or the entire plot. By default, ggplot2 automatically adds a legend if you include aesthetics like color, size, or shape that distinguish groups in your data. If you don’t want the legend to appear, you can set show.legend = FALSE.\nscale_fill_manual(): When creating grouped bar charts or other plots with multiple categories, it’s important to ensure that the visual distinction between groups is clear. This is where manually setting the colors for each category comes into play. The scale_fill_manual() function in ggplot2 allows you to manually define the colors used for the filled areas, such as the bars in a bar chart or the shaded areas in an area chart. By using the values argument within scale_fill_manual(), you can specify a custom color palette that suits your design or presentation needs. For example, you might choose a palette of yellow and brown to represent different categories. A key aspect of using scale_fill_manual() effectively is knowing how many colors to provide. The number of colors you define in the values argument must match the number of levels or categories in your data. If you are comparing three product categories, for instance, you’ll need to provide exactly three colors—one for each category. Failing to match the number of colors to the number of categories can result in errors or misrepresentation in the plot. For example, if you have four levels (e.g., four different product categories or groups) and only provide two colors, ggplot may not know how to properly assign colors to the additional categories. Therefore, it’s crucial to know the number of distinct categories in your data and plan your color palette accordingly to maintain clarity and visual consistency in your chart.\n\n\n## inputting probabilities calculated from a 2023 multiple choice\n## question.  From what you learned about R so far, how do you expect\n## its market share to change?\nGoUp &lt;- 0.54285\nGoDown &lt;- 0.03809\nRemainStable &lt;- 0.34285\nNoOpinion &lt;- 0.07619\n# designing the data frame\ndata_frame &lt;- data.frame(Category = c(\"Go Up\", \"Go Down\", \"Remain Stable\",\n    \"No Opinion\"), Percentage = c(GoUp, GoDown, RemainStable, NoOpinion))\n# Making the graph\nMarketShare &lt;- ggplot(data_frame, aes(x = Category, y = Percentage, fill = Category)) +\n    geom_bar(stat = \"identity\", show.legend = FALSE) + labs(title = \"How do you expect R's market share to change?\",\n    x = \"Opinion Category\", y = \"Percentage (%)\") + theme_minimal() + geom_text(aes(label = Percentage),\n    vjust = -0.5, size = 4) + scale_fill_manual(values = c(\"red\", \"blue\",\n    \"purple\", \"green\"))\n\nMarketShare\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Bar Graph with Data Wrangling\n\nLet’s start an example from scratch so that we can see each parameter take effect. In doing so, lets use a dataset to make a bar graph instead of relying on pre-calculated data.\n\nLets examine the AUQ300 variable from the nhanes survey to run an example.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n\n\nNext, we need to check the import by looking at the summary or head of the data.\n\n\n# Results hidden to save space, but gives you the first 6 records in\n# the data set.\nhead(nhanes)\n\n\nWe can also check the summary of data of only the variable of interest, AUQ300, to get a sense of what we are evaluating.\n\n\nsummary(nhanes$AUQ300)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   2.000   1.656   2.000   7.000    4689 \n\n\n\nThe AUQ300 variable represents gun use. A screenshot of the codebook is copied below so that we can see what AUQ300 really refers to. It is available on https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm. This is always a necessary step because variable names can be convoluted and not representative of the variable definition.\n\n\n\n\nAUQ300\n\n\n\n5.2.2.1 Recode Variable if Needed\n\nLook to see if the AUQ300 needs recoding after looking at the codebook and making sense of the variable.\nAUQ300 needs to be a factor variable with 1 equaling a Yes and 2 equaling a No. We can use recode_factor to accomplish 2 things at once with the mutate function.\nrecode_factor() transforms the levels of a categorical variable (factor) into a new set of levels and is specific to categorical variables.\nrecode() is generic and can apply to numerical, categorical, or textual data, but still transforms data from one format or code to another.\n\n\nnhanes.clean &lt;- nhanes %&gt;%\n    select(AUQ300) %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\"))\n\n\nThen, we need to check the recode for accuracy. You should see the No’s and Yes’s alongside the rest being coded as NA’s.\n\n\nsummary(nhanes.clean)\n\n  AUQ300    \n Yes :1613  \n No  :3061  \n NA's:4690  \n\n\n\n\n5.2.2.2 Get Bar Roughly Plotted\n\nStart with the basic plot using the ggplot() and geom_bar() commands.\nBelow writes the statement with and without the piping operator.\n\nSince we are also going to use data preparation techniques, the piping operator is recommended.\n\n\n# Without piping operator\nggplot(nhanes.clean, aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n# With piping operator\nnhanes.clean %&gt;%\n    ggplot(aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n5.2.2.3 Add Functions to Clean Chart\n\nOmit the NA category from AUQ300 variable, which represents gun use. Then plot the graph below.\nThe drop_na() function is a good way to drop NA values from either the entire dataset or just one variable. It was introduced in the data prep lesson. Since we are only interested in dropping NA values from our one variable of interest that is to be graphed (AUQ300), we can put it in the parentheses so that we do not unintentionally drop lots of observations for no reason.\nAdd an axis labels under labs(x = …, y=…).\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(x = AUQ300)) + geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\")\n\n\n\n\n\n\n\n##Here, we really benefit from the piping operator because we are doing more than one thing.\n\n\nFrom the bar graph, we can see that almost double the amount of people have not fired a firearm for any reason than those that fired one.\n\n\n\n5.2.2.4 Adding Color\n\nThere are many ways to add color to a bar graph. Below, the color is filled in directly in the aes() command by choosing it to give a different color to each categorical value of AUQ300.\n\nWhen fill is mapped to a variable, the fill color of the geom will vary based on the values of that variable. This is useful for distinguishing different groups or categories within the data. In this case the fill=AUQ300 gives a distinct color pattern based on how many categories there are considering the fact we are using the default “scale.”\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(AUQ300, fill=AUQ300)) +\n  geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\", \n       subtitle = \"Filled inside the aes()\") \n\n\n\n\n\n\n\n\n\n\n\n5.2.2.5 Data Prep and Then Visualized\n\nIn the command below, we create a gss.2016.cleaned object to make a barplot. In doing so, we do the following:\nCreate a bar graph using the ggplot() command, which requires an aes() quoting function. This function says that we want to use the grass variable in our bar graph.\nDrop all NAs from the grass variable so that legal and not legal are the only categories.\nWe then create the bars and fill them with 2 colors, red and purple. Many color codes can be used here, and will be discussed in a later lesson.\nWe then add labels to our graph on both x and y axis.\nFinally, we print the new graph, which is saved under the legalize.bar object.\nBelow, I brought back over the code from the last part in Data Preparation. You should still have this in your Chapter1.R file. We are going to use that file to create a graphics in R.\n\n\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(x = grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    mutate(age = as.numeric(x = age)) %&gt;%\n    mutate(age.cat = cut(x = age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n        \"30 - 59\", \"60 - 74\", \"75+\")))\n\n\nOnce the data is prepped, we can graph the variable or variables.\n\n\nggplot(gss.2016.cleaned, aes(grass)) + geom_bar()  ##with no piping operator\n\n\n\n\n\n\n\ngss.2016.cleaned %&gt;%\n    ggplot(aes(grass)) + geom_bar()  ##with piping operator\n\n\n\n\n\n\n\n\n\n# Make a Bar Graph for Grass Variable\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(grass)) + geom_bar(fill = c(\"red\", \"blue\")) + labs(x = \"Should marijuana be legal\",\n    y = \"Frequency of Responses\")\n\n\n\n\n\n\n\n\n\n\n5.2.2.6 Edit The Graphic\n\nNext, we can edit these commands to include the age variable. The aes() quoting function has expanded to have the bars filled color using the grass variable, the age category has replaced the grass variable on the x axis.\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, fill = grass)) + geom_bar() + labs(x = \"Age Category\",\n    y = \"Frequency of responses\")\n\n\n\n\n\n\n\n\n\nWe can add the position set at “dodge” inside the geom_bar() layer to make the barchart unstacked (or grouped).\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, fill = grass)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Age Category\", y = \"Frequency of responses\")\n\n\n\n\n\n\n\n\n\nWe can edit further to include a new a formula on the y axis to sum and count.\nIn the formula you provided, after_stat(count) is used within the ggplot2 framework to refer to the computed statistic generated by the geom_bar() function. Specifically, in the context of bar plots, count refers to the number of observations (or frequency) for each category within the data.\nWhen you use after_stat(count), you are referencing the count that is computed after geom_bar() has processed the data and calculated how many observations fall into each group (in this case, within each age category and the “grass” variable, which likely refers to attitudes toward marijuana legalization).\nWe also gave this a theme and updated the labels.\n\n\ngss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, y = 100 * (after_stat(count))/sum(after_stat(count)),\n        fill = grass)) + geom_bar(position = \"dodge\") + theme_minimal() +\n    labs(x = \"Age Category\", y = \"Percent of responses\")\n\n\n\n\n\n\n\n\n\nEvaluate these graphs and see what information you can get from them.\n\n\n\n\n5.2.3 Pie Chart\n\nA pie chart is a segmented circle whose segments portray the relative frequencies of the categories of a qualitative variable.\nSlices of pie in different colors represent the parts.\nIn this example, the firearm is divided by type to show parts of a whole, where the total of the proportions must add to 1.0 and the total of the percentages must add to 100%.\n\n\n# Importing data from working directory\nfbi.deaths &lt;- read.csv(\"data/fbi_deaths.csv\", stringsAsFactors = TRUE)\n# Selecting rows of interest for pie chart\nfbi.deaths.small &lt;- fbi.deaths[c(3, 4, 5, 6, 7), ]\nfbi.deaths.small &lt;- fbi.deaths.small %&gt;%\n    rename(Weapon = X)\n# Checking summary of fbi deaths\nsummary(fbi.deaths.small)\n\n                       Weapon      X2012          X2013          X2014     \n Firearms, type not stated:1   Min.   : 116   Min.   : 123   Min.   :  93  \n Handguns                 :1   1st Qu.: 298   1st Qu.: 285   1st Qu.: 258  \n Other guns               :1   Median : 310   Median : 308   Median : 264  \n Rifles                   :1   Mean   :1779   Mean   :1691   Mean   :1662  \n Shotguns                 :1   3rd Qu.:1769   3rd Qu.:1956   3rd Qu.:2024  \n Asphyxiation             :0   Max.   :6404   Max.   :5782   Max.   :5673  \n (Other)                  :0                                               \n     X2015          X2016     \n Min.   : 177   Min.   : 186  \n 1st Qu.: 258   1st Qu.: 262  \n Median : 272   Median : 374  \n Mean   :1956   Mean   :2201  \n 3rd Qu.:2502   3rd Qu.:3077  \n Max.   :6569   Max.   :7105  \n                              \n\n\n\nAgain, ggplot works in layers, so in order to make a pie, you need a few layers and have a few optional ones.\n\nThe aes() command specifies the variable to create the pie, in this case x2016.\ngeom_col() sets the borders of the pie and makes it visible.\ncoord_polar() command makes the pie circular.\ntheme_void() command is optional and adjusts the theme of the pie. to remove axis, background, etc.\n\n\n\nggplot(fbi.deaths.small, aes(x=\"\", y=X2016, fill=Weapon)) +\n  geom_col() + \n  coord_polar(\"y\", start=0) + \n  theme_void() \n\n\n\n\n\n\n\n\n\nFrom the pie, we can see that the majority of weapons that caused fbi gun related deaths are handguns followed by a type of firearm that is not stated.\n\n\n\n5.2.4 Comparison of Charts\n\nRecommended graphs for single categorical or factor type variable:\n\nBar graph, for showing relative group sizes.\nPie charts are available in R but are not recommended because they tend to be less clear for comparing group sizes.\n\nPie charts are difficult to read since the relative size of pie pieces is often hard to determine.\nPie charts take up a lot of space to convey little information.\nPeople often use fancy formatting like 3D, which takes up more space and makes understanding relative size of pie pieces even more difficult.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-continuous-variable",
    "href": "dataviz.html#graphs-for-a-single-continuous-variable",
    "title": "5  Data Visualization",
    "section": "5.3 Graphs for a Single Continuous Variable",
    "text": "5.3 Graphs for a Single Continuous Variable\n\nA continuous variable refers to a variable that can take any value over a range of values.\nA continuous variable needs to be numeric, and could be integer type or numeric type in R. Just like with graphs that include categorical variables, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With continuous variables, this may require recoding the variable to coerce it to the appropriate data type and/or renaming it to something meaningful if needed.\nIt is also beneficial to make sure the numerical variable is indeed supposed to be numerical (as opposed to a factor). For instance, you commonly see numbers listed for categories like the Yes/No coded as a 1/2, such as with the AUQ300 variable.\n\nSome common graphing options for single continuous variable:\n\nHistograms (From Lesson 2)\nDensity plots\nBoxplots\nViolin plots\n\n\n5.3.1 Histograms\n\nA histogram is a useful plot to determine central tendency and spread.\nWe went over histograms in Lesson 2, so refer back for information on how to create a histogram using base R and ggplot.\nRemember that you can tell the distribution from a histogram, and that distribution can be normal or skewed (Right or Left).\n\nWith each chart based on quantitative data, you should be able to get a sense of the distribution.\nThe histogram below looks right skewed.\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Density Plots\n\nA density plot is similar to a histogram but more fluid in appearance because it does not have the separate bins.\n\nProbability density is not very useful for interpreting what is happening at any given value of the variable on the x-axis, but it is useful in computing the percentage of values that are within a range along the x-axis.\nThe area under the curve in a density plot could be interpreted as the probability of a single observation or a range of observations.\nWe can use random normal data to create the density plot like shown below with a sample of 1000, a mean of 10 and a standard deviation of 2. To do this, we need to make the vector and assign it to a data frame.\nIn R, set.seed() is a function used to set the seed for random number generation. By setting a seed using set.seed(), you ensure reproducibility of your code. If you run the same code with the same seed, you’ll get the same sequence of random numbers every time. This is particularly useful for debugging, testing, or when you want to ensure that your results are reproducible.\nWe use set.seed before any function with a random normal generator to ensure reproducibility.\nIf a dataset is provided, then you do not need to generate your own random data as shown in the step below.\n\n\n\nset.seed(1)\nx &lt;- rnorm(1000, mean = 10, sd = 2)\ndf &lt;- data.frame(x)\n\n\nNext, we can make the density plot using the ggplot2 package under tidyverse.\n\nLayer 2 includes the geom_density() command in addition to the standard Layer 1 ggplot() command to create the density plot.\n\n\n\nggplot(df, aes(x)) + geom_density()\n\n\n\n\n\n\n\n\n\nThere are a lot of arguments you can change. I selected a couple below. Be sure to look at the help file on the geom_density() layer to get the variety on what you can do.\n\ncolor = sets a line color\nlwd = makes the line thicker. Increase this number for thicker line.\nfill= colors the area under the curve.\nalpha= sets the transparency to the area under the curve.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x)) +\n  geom_density(color = \"darkblue\", \n    #fill = Fills color under the curve\n    fill = \"lightblue\",\n    #alpha = Sets a transparency to the area under the curve. \n    #.5 for 50% transparency. \n    alpha = .5) \n\n\n\n\n\n\n\n\n\nWe can even add a mean line, which we know in this case is 10 because we used random normal data with that mean set as a parameter.\ngeom_vline() is a function used to add vertical lines to a plot created with ggplot. This function is useful for visually indicating specific points or ranges on the x-axis.\nYou can do a line break in your R code after a comma (\\(,\\)) or after a plus sign (\\(+\\)). I find things easier to read on less lines, but it is personal preference how many lines you use given still following the rules in R.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", fill = \"lightblue\",\n    alpha = 0.5) + geom_vline(aes(xintercept = mean(x)), color = \"red\",\n    linetype = \"dashed\", lwd = 1)\n\n\n\n\n\n\n\n\n\nYou do not need the rnorm function if you are provided a dataset with a numerical variable. The following code uses the customers dataset to do 2 examples of density plots with 2 numerical variables.\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\n\nstr(customers)\n\n'data.frame':   200 obs. of  10 variables:\n $ CustID   : int  1530016 1531136 1532160 1532307 1532356 1532387 1533017 1533561 1533697 1533766 ...\n $ Sex      : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ Race     : chr  \"Black\" \"White\" \"Black\" \"White\" ...\n $ BirthDate: chr  \"12/16/1986\" \"5/9/1993\" \"5/22/1966\" \"9/16/1964\" ...\n $ College  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ HHSize   : int  5 5 2 4 5 2 3 5 3 2 ...\n $ Income   : int  53000 94000 64000 60000 47000 67000 84000 76000 42000 71000 ...\n $ Spending : int  241 843 719 582 845 452 153 1079 247 708 ...\n $ Orders   : int  3 12 9 13 7 9 2 23 3 4 ...\n $ Channel  : chr  \"SM\" \"TV\" \"TV\" \"SM\" ...\n\nggplot(customers, aes(Income)) + geom_density()\n\n\n\n\n\n\n\nggplot(customers, aes(Orders)) + geom_density(color = \"#745033\", fill = \"#740000\",\n    alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n5.3.3 Boxplot\n\nA boxplot is a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups.\nBoxplots are often used to compare the distribution of a continuous variable across several groups.\nA box plot allows you to:\n\nGraphically display the distribution of a data set.\nCompare two or more distributions.\nIdentify outliers in a data set.\n\n\n\n\n\nA Boxplot with Outliers on Left\n\n\n\nBoxplots include the following information:\n\nA line representing the median value.\nA box containing the middle 50% of values.\nWhiskers extending to 1.5 times the IQR.\nOutliers more than 1.5 times the IQR away from the median.\n\n\n\n\n\nA Boxplot with No Outliers\n\n\n\nThis boxplot above displays 5 summary values:\n\nS = smallest value.\nL = largest value.\nQ1 = first quantile = 25th percentile.\nQ2 = median = second quantile = 50th percentile.\nQ3 = third quantile = 75th percentile.\n\nFor example, use the GrowthFund Vector from the last lesson. It is executed again below.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\nGrowthFund &lt;- as.data.frame(GrowthFund)\n\n\nThe quantile() function returns the five point summary when no arguments are specified where the 25th percentile is Quarter 1, and the 75 percentile is Quarter 3. The 50th percentile is the median.\n\n\nQuanData &lt;- quantile(GrowthFund$GrowthFund)\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\n\n\n\n5.3.4 Detecting Outliers\n\nWe see an outlier visually, but without the tool available, we can detect them through the statistics. First we calculate the IQR, which is just quarter 3 minus quarter 1.\n\n\nsummary(GrowthFund$GrowthFund)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-38.320   3.875  13.015  10.088  16.942  36.290 \n\nIQRvalue &lt;- 16.9425 - 3.875\nIQRvalue\n\n[1] 13.0675\n\nIQRvalue &lt;- IQR(GrowthFund$GrowthFund)\nIQRvalue\n\n[1] 13.0675\n\n\n\nThen, multiply the IQR by 1.5.\n\n\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 19.60125\n\n\n\nFinally conduct 2 checks to determine if outliers are past the low whisker and/or high whisker.\n\nA TRUE value indicates that at least one outlier is present at the small end of the distribution.\nA FALSE value indicates that no outliers are at the high end of the distribution.\n\n\n\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n 25% \nTRUE \n\n# True indicating an outlier to the left\n3.875 - -38.32  #42.195\n\n[1] 42.195\n\n42.195 &gt; 19.60125  #TRUE\n\n[1] TRUE\n\nQuanData[5] - QuanData[4] &gt; OutlierValue\n\n 100% \nFALSE \n\n# False indicating no outlier to the right.\n36.29 - 16.9425  #19.3475\n\n[1] 19.3475\n\n19.3475 &gt; 19.60125  #FALSE \n\n[1] FALSE\n\n\n\nYou can also more formally test by using the following formulas.\n\n\\(Lower bound=Q1−1.5×IQR\\)\n\\(Upper bound=Q3+1.5×IQR\\)\nA data point \\(x\\) is an outlier if \\(x\\) less than lower bound or \\(x\\) is greater than the upper bound. Confirming what we found above, we determine one outlier is present to the left.\n\n\n\nLowerBound &lt;- 3.875 - 1.5 * IQRvalue\nLowerBound\n\n[1] -15.72625\n\nQ1 &lt;- QuanData[2]\nQ1  #3.875\n\n  25% \n3.875 \n\nLowerBound &lt;- Q1 - OutlierValue\nLowerBound  #-15.72625\n\n      25% \n-15.72625 \n\nUpperBound &lt;- 16.9425 + 1.5 * IQRvalue\nUpperBound\n\n[1] 36.54375\n\nQ3 &lt;- QuanData[4]\nQ3  #16.9425\n\n    75% \n16.9425 \n\nUpperBound &lt;- Q3 + OutlierValue\nUpperBound  #36.54375\n\n     75% \n36.54375 \n\n## Insert Lower bound and Upper bound in vector to determine if\n## outliers are present: (-38.32, LowerBound 1.71, 3.17, 5.99, 12.56,\n## 13.47, 16.89, 16.96, 32.16, 36.29, UpperBound)\n\n## one outlier to the left, -38.32.\n\n\n5.3.4.1 GrowthFund Boxplot\n\nWe can use ggplot to retrieve our graph and associated numbers.\nThe outlier is visually depicted on the graph as -38.32.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe can add a little color to the plot with the fill parameter, but then we also need to be sure to turn off the legends in the geom_boxplot.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot(fill = \"red\")\n\n\n\n\n\n\n\n\n\nYou can add parameters to make this visualization more professional, but this gets you started. Be sure to look at some examples in the R community or on ChatGPT.\n\n\n\n\n5.3.5 Violin Plots\n\nA visual display of data that combines features of density plots and boxplots to show the distribution of numeric variables, often across groups.\n\n\n5.3.5.1 GrowthFund Example\n\nWe can look at the GrowthFund example we had as a boxplot above as a violin plot. In order to make the change, we alter the second layer from geom_boxplot() to geom_violin().\n\n\nGrowthFund %&gt;% ggplot(aes(x=\"\", y=GrowthFund))+ \n  geom_violin() + theme_minimal() + coord_flip()\n\n\n\n\n\n\n\n\n\n\n5.3.5.2 2nd Example Checking for outliers with mtcars dataset mpg variable\n\nWe can also view mpg from the mtcars data set as a violin plot because it is a numerical variable. In the plot below, I graphed mpg as a violin plot. You can also embed other visual markers like mean and median or layer on another graph like a boxplot.\nYou can see that this violin plot is vertical. The coord_flip() command we used above flips the chart horizontal the same way it flips a boxplot.\n\n\nmtcars %&gt;%\n  ggplot(aes(x=\"\", y = mpg)) +\n  geom_violin(fill=\"lightgreen\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nWe could also merge both a violin and a boxplot. The code below shows two separate charts (and also flipped) using the following code and then merges them into one visualization.\nThe width parameter controls the width of the boxes in a boxplot or the width of the violins in a violin plot. You can vary this parameter to create better depth.\n\n\ndata(mtcars)\n\nmtcars %&gt;%\n    ggplot(aes(x = \"\", y = mpg)) + geom_boxplot(color = \"#986D00\", fill = \"#24585E\") +\n    theme_minimal() + coord_flip()\n\n\n\n\n\n\n\n\n\nLet’s get some summary statistics to check for outliers, skewness, and kurtosis to see how our visual aids help us in understanding those results.\n\n\nIQRvalue &lt;- IQR(mtcars$mpg)\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue  #11.0625\n\n[1] 11.0625\n\nQuanData &lt;- quantile(mtcars$mpg)\nQuanData\n\n    0%    25%    50%    75%   100% \n10.400 15.425 19.200 22.800 33.900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n  25% \nFALSE \n\n# Using the numbers from QuadData\n15.425 - 10.4 &gt; 11.0625\n\n[1] FALSE\n\n# False indicating no outlier to the left\n\n\nQuanData[5] - QuanData[4] &gt; OutlierValue\n\n100% \nTRUE \n\n33.9 - 22.8 &gt; 11.0625\n\n[1] TRUE\n\n# TRUE indicating an outlier to the right.\n\n\nWe can see more specific information on outliers if we calculate the lower bound and upper bound and insert the values into the vector.\n\n\nQ1 &lt;- QuanData[2]\nQ1\n\n   25% \n15.425 \n\nLowerBound &lt;- Q1 - OutlierValue\nLowerBound  #4.3625 \n\n   25% \n4.3625 \n\nQ3 &lt;- QuanData[4]\nQ3\n\n 75% \n22.8 \n\nUpperBound &lt;- Q3 + OutlierValue\nUpperBound  #33.8625 \n\n    75% \n33.8625 \n\nsort(mtcars$mpg)\n\n [1] 10.4 10.4 13.3 14.3 14.7 15.0 15.2 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7\n[16] 19.2 19.2 19.7 21.0 21.0 21.4 21.4 21.5 22.8 22.8 24.4 26.0 27.3 30.4 30.4\n[31] 32.4 33.9\n\n\n[LowerBound: 4.3625] 10.4 10.4 13.3 14.3 14.7 15.0 15.2 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.2 19.7 21.0 21.0 21.4 21.4 21.5 22.8 22.8 24.4 26.0 27.3 30.4 30.4 32.4 [UpperBound: 33.8625)] 33.9\n\nsemTools::skew(mtcars$mpg)  #normal\n\nskew (g1)        se         z         p \n0.6723771 0.4330127 1.5527885 0.1204737 \n\nsemTools::kurtosis(mtcars$mpg)  #mesokurtic\n\nExcess Kur (g2)              se               z               p \n    -0.02200629      0.86602540     -0.02541068      0.97972740 \n\n\n\nLooks like the mpg variable is quite normal with one potential outlier to the right, but no major signs of skewness or kurtosis.\nWe can also get a histogram of mpg and are able to make the same claims towards normality. You can see a slight pull to the right, but it is seemingly normal. A higher sample size could help here.\n\n\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 5, color = \"black\",\n    fill = \"green\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-two-variables-at-once",
    "href": "dataviz.html#graphs-for-two-variables-at-once",
    "title": "5  Data Visualization",
    "section": "5.4 Graphs for Two Variables At Once",
    "text": "5.4 Graphs for Two Variables At Once\n\nCombinations of 2 Variable Types for Graphing\n\nTwo categorical/ factor.\nOne categorical/ factor and one continuous/ numeric.\nTwo continuous/ numeric.\n\n\n\n5.4.1 Bar Graphs for Two Categorical Variables\n\nThere are two formats available for bar charts:\n\nGrouped\nStacked\n\n\n\n\n# A tibble: 6 × 3\n# Groups:   vs, gear [6]\n     vs  gear     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     0     3    12\n2     0     4     2\n3     0     5     4\n4     1     3     3\n5     1     4    10\n6     1     5     1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.1 Grouped Bar Graph\n\nGrouped bar graph allow comparison of multiple sets of data items, with a single color used to denote a specific series across all sets.\nFor example, we can look at both the vs and gear variables in the ggplot command.\n\nYou can do a little grouping and counting before you began to generate a new table with frequencies based on vs and gear variables. Once a new dataset object is made, you can make the graph with the geom_bar layer specifying the stat=“identity”.\nSince there are two variables, you can set the position to dodge to view the fill categorical variable side by side.\n(stat = “identity”) tells ggplot that the y values are already computed and should be used as-is for the heights of the bars. In this case, they are frequencies calculated in the countsDF dataset.\n\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(vs = as.factor(vs)) %&gt;%\n    mutate(gear = as.factor(gear))\n\n\ncountsDF &lt;- mtcars %&gt;%\n    group_by(vs, gear) %&gt;%\n    count()\n\nsummary(countsDF)\n\n vs    gear        n         \n 0:3   3:2   Min.   : 1.000  \n 1:3   4:2   1st Qu.: 2.250  \n       5:2   Median : 3.500  \n             Mean   : 5.333  \n             3rd Qu.: 8.500  \n             Max.   :12.000  \n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\") + labs(title = \"Grouped Car Distribution by Gears and VS\",\n    x = \"Number of Gears\", y = \"Count\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.4.1.2 Stacked Bar Graph\n\nA Stacked bar graph extends the standard bar graph from looking at numeric values across one categorical variable to two. Each bar in a standard bar graph is divided into a number of sub-bars stacked end to end, each one corresponding to a level of the second categorical variable.\nUsing ggplot, we can also stack these charts by removing the position = dodge statement.\n\n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Stacked Car Distribution\",\n       x = \"Number of Gears\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.4.1.3 Bar Graph for Continuous Across Groups\n\nIn comparison to a bar graph for a single categorical variable, a bar chart for a continuous variable across groups includes both a x and y axis. The continuous variable is put on the y axis, and the categorical (factor) variable is placed on the x axis showing the groups.\nTherefore, instead of counting data based on group, we can see another continuous variable based on group data.\nThe frequency data (i.e., counts) can be replaced with another numerical variable like mean.\nIn the below example, instead of counting observations per group, here, we took the average mpg (a continuous variable) based on groups of gear and vs and summarized the data into a variable avg_mpg. We then used that variable in a ggplot() command to create a unique chart to that above.\n\n\navg_mpg &lt;- mtcars %&gt;%\n    group_by(gear, vs) %&gt;%\n    summarise(mpg = mean(mpg, na.rm = TRUE))\n\n\nggplot(avg_mpg, aes(gear,\n  mpg, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  ggtitle(\"Average MPG by VS and Gear\")\n\n\n\n\n\n\n\n\n\nBelow, we can add color using the scale_fill_manual. Since there are two colors, we use the c() command to combine them together inside the layer.\n\n\nggplot(avg_mpg, aes(gear, mpg, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\", color = \"black\") + ggtitle(\"Average MPG by VS and Gear\") +\n    scale_fill_manual(values = c(\"yellow\", \"brown\"))\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Boxplot for Continuous Across Groups\n\nA boxplot requires one continuous variable (like we did above). When we include an additional grouping variable, we get multiple boxplots, one for each group. This allows us to directly compare distributions.\nThe categorical variable should correctly be a factor data type before you begin.\nIn the example below, mpg is the continuous variable, and gear is the categorical variable.\nWe see 3 boxplots for three values of gear (3, 4, 5). ggplot() and geom_boxplot() are required components of the command. The scale_fill_manual() and theme_minimal() layers are optional ways to change the style and color.\n\n\nmtcars %&gt;%\n  ggplot(aes(x = gear, y = mpg, fill = gear)) +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_manual(values = c(\"gray\", \"red\", \"blue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLets alter the functions above to depict mpg based on vs with categorical states 0 and 1.\n\n\nmtcars %&gt;%\n    ggplot(aes(x = vs, y = mpg, fill = vs)) + geom_boxplot(show.legend = FALSE) +\n    scale_fill_manual(values = c(\"gray\", \"red\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.4.3 Scatterplot for Two Continuous Variables\n\n5.4.3.1 Scatterplots\n\nA scatterplot is used to determine if two continuous variables are related.\n\nEach point is a pairing: \\((x_1, y_1),(x_2, y_2),\\) etc.\n\nOur goal with a scatterplot is to characterize the relationship by visual inspection. This includes determining if the relationship looks positive, negative, or not existent.\n\n\n\n\nScatterPlot Results\n\n\n\nSometimes, it is really clear how to characterize the relationship. Other times, additional statistical tests are needed to confirm the relationship (which we will go over in later lessons). This is true especially with big data, where the plot window can look like a giant blog of observations.\nLet’s work a clean example examining the relationship between income and the years of education one has had.\nThis plot has a clear positive trend, meaning that as one has more years of education, we see higher income. And similarly, as we see higher income, we also see more years of education. This means that a scatter can help characterize the relationship, and does not state that one variable is causing another to occur.\n\n\nEdu &lt;- read.csv(\"data/education.csv\")\nplot(Edu$Income ~ Edu$Education, ylab = \"Income\", xlab = \"Education\")\n\n\n\n\n\n\n\n\n\nWorking with ggplot instead of base R, we would use the following code.\n\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs (for labels) as shown below.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\")\n\n\n\n\n\n\n\n\ngeom_point() has some parameters you can change like shape = where you can add depth to your chart. Some common shapes of geom_point are as follows.\n\n\n# shape = 0, square shape = 1, circle shape = 2, triangle point up\n# shape = 3, plus shape = 4, cross shape = 5, diamond shape = 6,\n# triangle point down shape = 7, square cross shape = 8, star shape =\n# 9, diamond plus shape = 10, circle plus shape = 11, triangles up\n# and down shape = 12, square plus shape = 13, circle cross shape =\n# 14, square and triangle down shape = 15, filled square shape = 16,\n# filled circle shape = 17, filled triangle point-up shape = 18,\n# filled diamond\n\n\nFor instance, the code below changes the color, shape, and size of the geom_point().\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 10) + labs(y = \"Income\", x = \"Education\")\n\n\n\n\n\n\n\n\n\ngeom_line() in R’s ggplot2 package is used to create line plots, which connect data points with straight lines. It is commonly used to visualize trends over time or continuous relationships between variables.\nggplot allows us to add a geom_line, which is helpful in drawing a line through the data. Here, I am also resetting the color off of the default value. You see this a lot on time series models like stock charts.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_line(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn R’s ggplot2 package, geom_smooth() is used to add a trendline (also called a smoothing line or a regression line) to a plot. It fits a line or curve through your data points based on a smoothing method, allowing you to visualize the general relationship between the x and y variables. The trendline can help reveal patterns such as linear relationships or trends in noisy data. The trendline give us 2 parts:\n\nTrendline: This line represents the fitted relationship between the two variables, which could be linear, polynomial, or a more flexible curve depending on the smoothing method used.\nConfidence Interval: The shaded area (often gray by default) around the trendline shows the confidence interval, giving a sense of the uncertainty of the fit.\n\nWe can change our line to a geom_smooth line, which is considered a trendline to help us visualize the relationship between the variables.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_smooth(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nWe can change the type of trendline. The most common is the to develop the trendline using the lm method, which stands for the linear method that we are going to learn in the Regression lesson. For now, lets insert _method=“lm” into our geom_smooth() later to see the change.\nThe gray band on a scatter plot in R usually represents the confidence interval around a fitted line when you plot it using geom_smooth() in ggplot2. This gray area visualizes the uncertainty or variability of the estimated regression line, providing a sense of how confident we are about the predictions at different points along the line.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\") + geom_smooth(method = \"lm\", color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nUnlike geom_smooth(), which fits a trendline or smoothing line to data, geom_line() directly connects the raw data points in the order they appear (typically by the x-axis values). In business statistics class that teaches linear regression, we use more trendlines than geom_lines.\nLet’s look at a few more examples and see if the relationship is considered positive, negative, or not existent.\nBelow, we see a negative trend.\n\n\nggplot(mtcars, aes(x = disp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the hp variable, below, we see another negative trend.\n\n\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the qsec variable, below, we see a weak positive trend. This relationship would need to verified later on.\n\n\nggplot(mtcars, aes(x = qsec, y = mpg)) + geom_point() + geom_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nThe plot() command also works when you do not have 2 continuous variables, and instead have one categorical variable paired with one continuous variable. However, the plot is not as adequate as others are in inferring the relationship from the variables.\nFor example, the plot below would be better served as a boxplot.\n\n\nplot(mtcars$mpg ~ mtcars$cyl)\n\n\n\n\n\n\n\nboxplot(mtcars$mpg ~ mtcars$cyl)\n\n\n\n\n\n\n\n\n\nUsing ggplot, we would have the same issue. Since vs is a categorical variable, it does not look right when we use the geom_point() later.\n\n\nggplot(mtcars, aes(cyl, mpg)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nInstead, we would want the geom_boxplot() layer like shown below. The geom_smooth() is also not an applicable layer to a boxplot and would need to be removed.\n\nWhen you create multiple boxplots in ggplot2 and one or more variables are numeric but not converted to factors, only one boxplot may show up because ggplot2 treats numeric variables as continuous. Boxplots group data by categorical variables, so if your variable is continuous, ggplot interprets it as a single category and produces only one boxplot.\nBelow, we still have an error.\n\n\nggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nSomething still is not quite right here, so we would need to make sure cyl is correctly a factor before making the boxplot. Now, we should see multiple boxplots, one for each categorical level. To fix this, you need to convert the grouping variable to a factor using as.factor() in your data, so ggplot will recognize it as a categorical variable and display multiple boxplots accordingly.\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(cyl = as.factor(cyl))\nggplot(mtcars, aes(cyl, mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.4.3.2 Try to Recreate\n\nYou try examples of scatterplots using the UScrime data set that is part of the MASS package to examine a few relationships using ggplot2. A few examples are below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what information you can take away from the scatterplots above and create some more to practice.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#full-example-using-nhanes-dataset",
    "href": "dataviz.html#full-example-using-nhanes-dataset",
    "title": "5  Data Visualization",
    "section": "5.5 Full Example Using nhanes Dataset",
    "text": "5.5 Full Example Using nhanes Dataset\n\nA big example of where charts are helpful is to explore the nhanes dataset, which is actually a dataset related to auditory issues, in which the number of guns fired is a variable.\nSpecifically, AUQ060 refers to “Hear a whisper from across a quiet room?” AUQ070 refers to “Hear normal voice across a quiet room?” and AUQ080 refers to “Hear a shout from across a quiet room?” While AUQ300 refers to “Ever used firearms for any reason? AUQ310 refers to”How many total rounds ever fired?” and AUQ320 refers to “Wear hearing protection when shooting?” I got these references at the following website: https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm.\nIt would be interesting to determine whether there is a relationship between hearing loss and gun use. Below, I clean the data set and make some charts. All variables are categorical, and I only look at 2 categorical variables at a time - one auditory related (AUQ 060, 070, 080) and one gun related (AUQ 300, 310, 320).\nThe first step is data cleaning, and in particular recoding the variables of interest. We did some of these before. Let’s also filter out the unused variables using the select statement.\n\nThe select statement in dplyr has a conflict with the select statement in MASS. Since we used MASS earlier, we have to specify which package we want to use select from. We want to use select from dplyr, so we add dplyr:: before the function.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n# summary(nhanes)\n\nnhanes.clean &lt;- nhanes %&gt;%\n    dplyr::select(AUQ300, AUQ310, AUQ320, AUQ060, AUQ070, AUQ080) %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ310 = recode_factor(AUQ310, `1` = \"1 to less than 100\", `2` = \"100 to less than 1000\",\n        `3` = \"1000 to less than 10k\", `4` = \"10k to less than 50k\", `5` = \"50k or more\",\n        `7` = \"Refused\", `9` = \"Don't know\")) %&gt;%\n    mutate(AUQ060 = recode_factor(AUQ060, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ070 = recode_factor(AUQ070, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ080 = recode_factor(AUQ080, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ320 = recode_factor(AUQ320, `1` = \"Always\", `2` = \"Usually\",\n        `3` = \"About half the time\", `4` = \"Seldom\", `5` = \"Never\"))\nsummary(nhanes.clean)\n\n  AUQ300                       AUQ310                     AUQ320    \n Yes :1613   1 to less than 100   : 701   Always             : 583  \n No  :3061   100 to less than 1000: 423   Usually            : 152  \n NA's:4690   1000 to less than 10k: 291   About half the time: 123  \n             10k to less than 50k : 106   Seldom             : 110  \n             50k or more          :  66   Never              : 642  \n             Don't know           :  26   NA's               :7754  \n             NA's                 :7751                             \n  AUQ060      AUQ070      AUQ080    \n Yes :2128   Yes : 564   Yes : 159  \n No  : 745   No  : 210   No  :  53  \n NA's:6491   NA's:8590   NA's:9152  \n                                    \n                                    \n                                    \n                                    \n\n\n\nFrom here, NA’s are an issue, but we don’t want to broadly omit because it would slice down our dataset too much. I am going to leave them in and handle it on a chart by chart basis.\nThe most applicable chart to graph 2 categorical variables is a barchart. This requires calculating frequencies, and then graphing. Using ggplot, the frequencies are calculated automatically.\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Whisper Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Normal Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Shout Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n############# Wear hearing protection when shooting\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Whisper Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Normal Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Shout Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThat is a few of the charts. See if you can determine anything interesting from them and also try to run the other combinations. You will see that it is easier to see with less categories when you have multiple variables like this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#summary",
    "href": "dataviz.html#summary",
    "title": "5  Data Visualization",
    "section": "5.7 Summary",
    "text": "5.7 Summary\n\nPractice many more examples with the help of ChatGPT and work towards constructing high-quality charts and graphs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "probability.html#some-common-statistics-terms",
    "href": "probability.html#some-common-statistics-terms",
    "title": "6  Probability and Probability Distributions",
    "section": "6.1 Some Common Statistics Terms",
    "text": "6.1 Some Common Statistics Terms\n\nStatistical inference is one of the foundational ideas in statistics. Since it is often impossible to collect information on every single person or organization, scientists take samples of people or organizations and examine the observations in the sample. Inferential statistics are then used to take the information from the sample and use it to understand (or infer to) the population. In conducting probability calculations, we can make inferences to understand the probability associated with the population.\nResearchers often work with samples instead of populations, where samples are subsets of different populations. In the case of the state data on opioid policies that your book discusses, all states are included, so this is the entire population of states. Statisticians sample from the population to understand the probabilities associated with it.\nWhen selecting a sample, we hope to select a representative sample from the population, and use properties of the normal distribution to understand what is likely happening in the whole population. A normal distribution is one of the most fundamental distributions to use in calculating probabilities. We will look at both discrete and normal distributions, but also seek to understand how a normal distribution and the central limit theorem can help us shed light on many statistics we are inferring.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-distribution",
    "href": "probability.html#probability-distribution",
    "title": "6  Probability and Probability Distributions",
    "section": "6.2 Probability Distribution",
    "text": "6.2 Probability Distribution\n\nA probability distribution is the numeric or visual representation of the set of probabilities that each value or range of values of a variable occurs.\nTwo important characteristics:\n\nThe probability of each real value of some variable is non-negative. Instead, it is either 0 or positive. More specifically, the probability of each value x is a value between 0 and 1. Or equivalently. \\(0 &lt;= P(X=x) &lt;= 1\\).\nThe sum of the probabilities of all possible values of a variable is 1.\n\nConsider the probability distribution that reflects the number of credit cards that a bank’s customers carry.\n\n\n\n  NumberOfCards Percentage\n1             0       2.5%\n2             1       9.8%\n3             2      16.6%\n4             3      16.5%\n5     4 or more       54.6\n\n\n\nGiven the characteristics of a probability distribution, we can ask whether this is a valid probability distribution.\n\nYes, because \\(0 &lt;= P(X=x) &lt;= 1\\) and the sum of the percentages is 1.\n\n\n0.025 + 0.098 + 0.166 + 0.165 + 0.546\n\n[1] 1\n\n\nSecond, with the information in the table, we can calculate a number of things, like the probability that a reader carries no credit cards.\n\n\\(P(X=0)= .025\\)\n\nThe probability that a reader carries fewer than two credit cards.\n\n\\(P(X&lt;2)= P(X=0)+P(X=1)= .025 +.098= .123\\)\n\nThe probability that a reader carries at least two credit cards.\n\n\\(P(X&gt;=2) = P(X=2)+P(X=3)+P(X=4)  = .166+.165+.546= .877\\)\nOr \\(1-P(X&lt;2) = 1-.123 = .877\\)\n\nBecause of the 2 characteristics of a probability distribution, sometimes there are a couple ways to calculate the correct answer, like we did above with either calculating probabilities associated with above a value, or beneath and equal to a value knowing that the total sum of all probabilities is 1.\nWhen you produce a percentage, you multiple the calculated probability by 100, so instead of finding a value between 0 and 1, you find a percentage between 0% and 100%. This means that if you use the pnorm function to calculate a probability, you can multiple that probability by 100 to get the percentage.\nWhen you produce the value given a probability calculation, you multiply the probability calculated by the sample size (n).\n\n\n6.2.1 Random Variables\n\nA Random Variable is a function that assigns numerical values to the outcomes of a random experiment.\nDenoted by uppercase letters (e.g., \\(X\\) ).\nCorresponding values of the random variable: \\(x_1,x_2, x_3,...\\)\nRandom variables may be classified as:\n\nDiscrete - The random variable assumes a countable number of distinct values.\n\n\nDiscrete probability distributions show probabilities for variables that can only have certain values, which includes categorical variables and variables that must be measured in whole numbers like number of people texting during class.\nThe Binomial Distribution is a discrete distribution that evaluates the probability of a “yes” or “no” outcome occurring over a given number of trials\n\n\nContinuous - The random variable is characterized by (infinitely) uncountable values within any interval.\n\n\nContinuous probability distributions show probabilities for values, or ranges of values, of a continuous variable that can take any value in some range.\nThe Normal Distribution is a continuous distribution and is the most important of all probability distributions. Its graph is bell-shaped and this bell-shaped curve is used in almost all disciplines.\n\nFor example, consider an experiment in which two shirts are selected from the production line and each is either defective (D) or non-defective (N).\n\nSince only 2 shirts are selected, here is the sample space, which are all the available options: \\({(D,D), (D,N), (N,D), (N,N)}\\)\nThe random variable X is the number of defective shirts.\nThe possible number of defective shirts is the set \\(X={0,1,2}\\),\nSince these are the only countable number of possible outcomes, this is a discrete random variable.\n\n\n\n\n6.2.2 Useful Commands for Random Variables\n\nset.seed() command is useful when conducting random sampling since it will result in the same sample to be taken each time the code is run, which makes sampling more reproducible.\n\nWe briefly looked at this when making our density plot with random normal data using the rnorm() command.\n\nsample_n() command can be used to take a sample. The arguments for sample_n() are size = which is where to put the size of the sample to take and replace = which is where you choose whether or not you want R to sample with replacement (replacing each value into the population after selection, so that it could be selected again) or without replacement (leaving a value out of the sampling after selection).\nLet’s look at an example using the pdmp_2017.csv file.\n\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\nBelow, I am using the read.csv() command to read in the data set and use strings as factors = TRUE to ensure character variables are coerced as factors. This helps bypass the need to coerce later on. We do need to note that it will change all string variables to factors with the TRUE parameter.\n\n\nopioidpolicy  &lt;- read.csv(\"data/pdmp_2017.csv\", stringsAsFactors = TRUE) \n# Set a starting value for sampling\nset.seed(3)\n# Sample 25 states and save as sample and check summary\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nYou should have the same answers as I do above (8 No’s, and 17 Yes’s) because we set the same seed. If you don’t, I mention the reason below at the end of this short sampling experiment.\n\n\n# Sample another 25 states and check summary.\n# Note the different answer than above. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 14  11 \n\n\n\n# Sample another 25 states and check summary\n# Again, note the differences in numbers each time. \nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n 12  13 \n\n\n\n# Sample another 25 states and check summary using same set seed as our first run (3). \nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\nAgain, you should have the same numbers as I do above, and these numbers should be equivalent to our first run (8 No’s, and 17 Yes’s). If you don’t have the same numbers as me is possible that your random number generator is on a different setting. Post R version 3.6.0 or later, we should be on Rejection sample.kind. The next line sets the RNGkind().\n\n\nRNGkind(sample.kind = \"Rejection\")\n# Run the same code again as above for replication results\nset.seed(3)\nSample &lt;- sample_n(opioidpolicy, size=25, replace=FALSE) \nsummary(Sample$Required.Use.of.Prescription.Drug.Monitoring.Programs)\n\n No Yes \n  8  17 \n\n\n\n\n6.2.3 Summary Measures for a Random Variable\n\n6.2.3.1 Expected Value\n\nWe can calculate the expected value, or value we think is going to occur based on the type of distribution.\n\nExpected value is also known as the population mean \\(\\mu\\), and is the weighted average of all possible values of \\(X\\).\nMore specifically, \\(E(X)\\) is the long-run average value of the random variable over infinitely many independent repetitions of an experiment.\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the expected value of \\(X\\) is the probability weighted average of the values. In the case of one random variable, that means: \\(E(X) = \\mu = \\sum{x_iP(X=x_i)}\\)\n\n\n\n\n6.2.3.2 Variance\n\nVariance of a random variable is the average of the squared differences from the mean.\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the variance is defined as: \\(Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2*P(X=x_i)}\\)\n\n\n\n\n6.2.3.3 Standard Deviation\n\nStandard deviation is consistently the square root of the variance. \\(SD(X) = \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\)\n\n\n\n\n6.2.4 Example of Summary Measures for a Random Variable\n\ndefectdata &lt;- read.csv(\"data/defects.csv\")\nhead(defectdata)\n\n  SerialNumber NumDefects\n1            1          6\n2            2          6\n3            3          0\n4            4          1\n5            5          6\n6            6          6\n\nstr(defectdata)\n\n'data.frame':   500 obs. of  2 variables:\n $ SerialNumber: int  1 2 3 4 5 6 7 8 9 10 ...\n $ NumDefects  : int  6 6 0 1 6 6 7 0 3 2 ...\n\n# Calculate the probability of the defective pixels per monitor for\n# each member of the sample space [0, 1, 2, 3, 4, 5, 6, 7].\nsampleSpace &lt;- 0:7\nfrequency &lt;- table(defectdata$NumDefects)\nfrequency\n\n\n 0  1  2  3  4  5  6  7 \n49 47 62 83 60 65 66 68 \n\nproportions &lt;- prop.table(frequency)\nproportions\n\n\n    0     1     2     3     4     5     6     7 \n0.098 0.094 0.124 0.166 0.120 0.130 0.132 0.136 \n\ncumulativeproportions &lt;- cumsum(prop.table(proportions))\ncumulativeproportions\n\n    0     1     2     3     4     5     6     7 \n0.098 0.192 0.316 0.482 0.602 0.732 0.864 1.000 \n\n\n\nOnce we calculate vectors for the frequency table, we bind them together and transpose them into columns and combine them into a data frame.\n\n\nDefects &lt;- t(rbind(sampleSpace, frequency, proportions, cumulativeproportions))\nDefects &lt;- as.data.frame(Defects)\nstr(Defects)\n\n'data.frame':   8 obs. of  4 variables:\n $ sampleSpace          : num  0 1 2 3 4 5 6 7\n $ frequency            : num  49 47 62 83 60 65 66 68\n $ proportions          : num  0.098 0.094 0.124 0.166 0.12 0.13 0.132 0.136\n $ cumulativeproportions: num  0.098 0.192 0.316 0.482 0.602 0.732 0.864 1\n\n\n\nNext, we calculate summary statistics based on the formulas above.\n\n\n# How many defects should the manufacturer expect per monitor? E(X)?\nExDefects &lt;- sum(Defects$sampleSpace * Defects$proportions)\nExDefects  #3.714\n\n[1] 3.714\n\n# Variance of the number of defects per monitor?\ndeviations &lt;- (Defects$sampleSpace - ExDefects)^2 * Defects$proportions\ndeviations\n\n[1] 1.35179201 0.69238482 0.36428670 0.08462614 0.00981552 0.21499348 0.68980507\n[8] 1.46850026\n\nvarDefects &lt;- sum(deviations)\nvarDefects  #4.876204\n\n[1] 4.876204\n\n# Standard deviation of the number of defects per monitor?\nstDefects &lt;- sqrt(varDefects)\nstDefects  #2.208213\n\n[1] 2.208213",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-binomial-distribution",
    "href": "probability.html#the-binomial-distribution",
    "title": "6  Probability and Probability Distributions",
    "section": "6.3 The Binomial Distribution",
    "text": "6.3 The Binomial Distribution\n\nThe binomial distribution is a discrete probability distribution and applies to probability for binary categorical variables with specific characteristics.\nProperties of a binomial random variable:\n\nA variable is measured in the same way \\(n\\) times, signifying that \\(n\\) is the sample size.\nThere are only two possible values of the variable, often called “success” and “failure”\nEach observed value is independent of the others.\nThe probability of “success”, \\(p\\), and the probability of “failure”, \\(1-p\\), is the same for each observation, so each time the trial is repeated, the probabilities of success and failure remain the same.\nThe random variable is the number of successes in \\(n\\) measurements.\n\n\n\n6.3.1 Summary Measures for a Binomial Random Variable\n\nExpected value, variance, and standard deviation were introduced and defined in section 2. We can take derivatives of the formulas to simplify our calculations given knowing a \\(n\\) and \\(p\\).\nThe formula for the expected value of a binomial random variable expands from \\(\\sum{x_iP(X=x_i)}\\) to \\(= n*p\\).\nThe variance of a binomial random variable expands from \\(\\sum{(x_i-\\mu)^2*P(X=x_i)}\\) to \\(= n*p*(1-p)\\)\nThe standard deviation a binomial random variable expands from \\(\\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}\\) to \\(= \\sqrt{np*(1-p)}\\)\nExample summary statistics of a binomial random variable\n\nA current WM student has a career free-throw percentage of 89.4%. Suppose he shoots six free throws in tonight’s game. What is the expected number of free throws that he will make?\n\n\nex &lt;- 6 * 0.894\nex\n\n[1] 5.364\n\nvarx &lt;- 6 * 0.894 * (1 - 0.894)\nvarx\n\n[1] 0.568584\n\nsdx &lt;- sqrt(varx)\nsdx\n\n[1] 0.7540451\n\n\nIf the student shoots 6 free throws and typically makes 89.4% of them, we can multiply those two values together for the expected value, 5.364. We also find that this data has a variance of .568584 and standard deviation of .754051.\n\n\n\n6.3.2 The Probability Mass Function\n\nThe Probability Mass Function for a discrete random variable X is a list of the values of X with the associated probabilities, that is, the list of all possible pairs: \\((X, P(X=x))\\)\n\nA probability mass function computes the probability that an exact number of successes happens for a discrete random variable, given \\(n\\) and \\(p\\) defined above.\nDistribution of probabilities of different numbers of successes.\nA probability mass function is used to describe discrete random variables in a binomial distribution.\nEvery random variable is associated with a probability distribution that describes the variable completely.\nUses dbinom() command to calculate in R.\n\nExample using the probability mass function: Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that none of the workers is afraid that they will never be able to retire?\nAgain, we can use the dbinom() command to calculate this in R given x = none or 0, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n\n# P(X=0)\ndbinom(0, 10, 0.2)\n\n[1] 0.1073742\n\n\n\nThe answer suggests that there is a .107 or 10.737% chance that no workers think they won’t be able to retire.\n\n\n\n6.3.3 Cumulative Distribution Function\n\nAnother way to look at a probability distribution is to examine its cumulative probability distribution. Here, you can determine the probability of getting some range of values, which is often more useful than finding the probability of one specific number of successes.\nA cumulative distribution function may be used to describe either discrete or continuous random variables.\n\nThe cumulative distribution function for X is defined as: \\(P(X&lt;=x)\\)\nThe less than and equal to sign is the standard way to look at the cumulative distribution function. You can calculate &gt;, &gt;=, &lt; from \\(P(X&lt;=x)\\) given the two rules of probability discussed above.\n\n\\(0 &lt;= P(X=x) &lt;= 1\\)\nThe sum of the probabilities of all possible values of a variable is 1\n\nUses pbinom() command to calculate in R with the default value of lower.tail = TRUE is for n or fewer successes\nCan change lower.tail parameter to lower.tail = FALSE is computing higher than n rather than n or higher.\n\nExample using the cumulative distribution function, Approximately 20% of U.S. workers are afraid that they will never be able to retire. Suppose 10 workers are randomly selected. What is the probability that less than 3 of the workers are afraid that they will never be able to retire?\nWe can use the pbinom() command to calculate this in R given q = less than 3 or &lt;=2, size = 10 workers, or just 10, and prob = 20% or .2. We write this command as listed below.\n\n\n# P(X&lt;3) or P(X&lt;=2)\npbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\nOr likewise, we could use multiple dbinom() commands to get individual probabilities and add them up. This statement is much longer, but does give you the same answer. Examine the figure below to see why.\n\n\n\ndbinom(0, 10, 0.2) + dbinom(1, 10, 0.2) + dbinom(2, 10, 0.2)\n\n[1] 0.6777995\n\n\n\n\n\nVisual of Binomial Distribution Example\n\n\n\n\n6.3.4 Variations in binom() Command\n\nIn order to find \\(P(X = 70)\\) given 100 trials and .68 probability of success, we enter:\n\n\n# P(X = 70)\ndbinom(70, 100, 0.68)\n\n[1] 0.07907911\n\n\n\nIn order to find \\(P(X &lt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt;= 70)\npbinom(70, 100, 0.68)\n\n[1] 0.7006736\n\n\n\nIn order to find \\(P(X &lt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &lt; 70)\npbinom(69, 100, 0.68)\n\n[1] 0.6215945\n\n\n\nIn order to find \\(P(X &gt; 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt; 70)\npbinom(70, 100, 0.68, lower.tail = FALSE)\n\n[1] 0.2993264\n\n\n\nIn order to find \\(P(X &gt;= 70)\\), given 100 trials and .68 probability of success, we enter:\n\n\n# P(X &gt;= 70)\npbinom(69, 100, 0.68, lower.tail = FALSE)  #Or\n\n[1] 0.3784055\n\n1 - pbinom(69, 100, 0.68)\n\n[1] 0.3784055\n\n\n\nExamples of binomial distribution calculations in a word problems: A current WM student has a career free-throw percentage of 90.3%. Suppose he shoots five free throws in tonight’s game. What is the probability that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903)\n\n[1] 0.6003973\n\n\n\nWhat is the percentage that he makes all five free throws?\n\n\n# P(X=5)\ndbinom(5, 5, 0.903) * 100  #60.04%\n\n[1] 60.03973\n\n\n\nA current WM student has a career free-throw percentage of 80.5%. Suppose he shoots six free throws in tonight’s game. What is the probability that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE)\n\n[1] 0.6676464\n\n# Or\ndbinom(5, 6, 0.805) + dbinom(6, 6, 0.805)\n\n[1] 0.6676464\n\n\n\nWhat is the percentage that he makes five or more of his free throws?\n\n\n# P(X&gt;=5)\npbinom(4, 6, 0.805, lower.tail = FALSE) * 100  # 66.76%\n\n[1] 66.76464\n\n\n\nThirty five percent of consumers with credit cards carry balances from month to month. Six consumers with credit cards are randomly selected. What is the probability that fewer than two consumers carry a credit card balance?\n\n\n# P(X&lt;=2)\ndbinom(0, 6, 0.35) + dbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n# Or\npbinom(1, 6, 0.35)\n\n[1] 0.3190799\n\n\n\nWhat is the percentage that fewer than two consumers carry a credit card balance?\n\n\npbinom(1, 6, 0.35) * 100  #31.9%\n\n[1] 31.90799\n\n\n\n\n6.3.5 Follow Up Binomial Example\nFor a discrete binomial distribution with a sample size of 4 (i.e., the number of trials is 4), calculate the probability of each possible outcome (ranging from 0 to 4 successful outcomes) using a probability of success \\(p=0.3\\).\n\n\n\n\n\n\n\n\n\n\n## dbinom (X=x) given size at 4 and p=.3 P(X=0) = 0.2401\ndbinom(0, 4, 0.3)\n\n[1] 0.2401\n\n# P(X=1) = 0.4116\ndbinom(1, 4, 0.3)\n\n[1] 0.4116\n\n# P(X=2) = 0.2646\ndbinom(2, 4, 0.3)\n\n[1] 0.2646\n\n# P(X=3) = 0.0756\ndbinom(3, 4, 0.3)\n\n[1] 0.0756\n\n# P(X=4) = 0.0081\ndbinom(4, 4, 0.3)\n\n[1] 0.0081\n\n\n\nCalculate the probability when the value is less than or equal to 2. All calculations below give same answer.\nTo confirm accuracy, sum the individual probabilities you calculated using dbinom() above. This total should match the value obtained using the pbinom() function in R, which gives the cumulative probability up to a specified number of successes.\nLess than or equal to is the default state of the function in which we use the exact number given.\n\n\n### P(X&lt;=2) =0.9163\npbinom(2, 4, 0.3)\n\n[1] 0.9163\n\ndbinom(0, 4, 0.3) + dbinom(1, 4, 0.3) + dbinom(2, 4, 0.3)\n\n[1] 0.9163\n\n0.2401 + 0.4116 + 0.2646\n\n[1] 0.9163\n\n\n\nCalculate the probability when the value is less than 2. All calculations below give same answer.\nTo calculate less than instead of less than or equal to, we go down 1 unit (or 1 integer) in our pbinom() function, i.e., using 1 instead of 2.\n\n\n## P(X&lt;2) = 0.6517\npbinom(1, 4, 0.3)\n\n[1] 0.6517\n\ndbinom(0, 4, 0.3) + dbinom(1, 4, 0.3)\n\n[1] 0.6517\n\n0.2401 + 0.4116\n\n[1] 0.6517\n\n\n\nCalculate the probability when the value is greater than 2. All calculations below give same answer.\nWe use 2 inside the function because greater than is the opposite tail to less than or equal to. This means that the numbers should be the same as the numbers in a less than or equal to problem, but we need to include the lower.tail parameter at False.\n\n\n## P(X&gt;2) = 0.0837\n1 - pbinom(2, 4, 0.3)\n\n[1] 0.0837\n\npbinom(2, 4, 0.3, lower.tail = F)\n\n[1] 0.0837\n\ndbinom(3, 4, 0.3) + dbinom(4, 4, 0.3)\n\n[1] 0.0837\n\n0.0756 + 0.0081\n\n[1] 0.0837\n\n\n\nCalculate the probability when the value is greater than or equal to 2.\nWe use 1 inside the function because greater than or equal to is the opposite tail to less than. This means that the numbers should be the same as the numbers in a less than problem, but we need to include the lower.tail parameter at False.\n\n\n## P(X&gt;=2) = 0.3483\n1 - pbinom(1, 4, 0.3)\n\n[1] 0.3483\n\npbinom(1, 4, 0.3, lower.tail = FALSE)\n\n[1] 0.3483\n\ndbinom(2, 4, 0.3) + dbinom(3, 4, 0.3) + dbinom(4, 4, 0.3)\n\n[1] 0.3483\n\n0.2646 + 0.0756 + 0.0081\n\n[1] 0.3483",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-continuous-distribution",
    "href": "probability.html#the-continuous-distribution",
    "title": "6  Probability and Probability Distributions",
    "section": "6.4 The Continuous Distribution",
    "text": "6.4 The Continuous Distribution\n\nProperties of a Continuous Random Variable\n\nThe random variable is characterized by (infinitely) uncountable values within any interval.\nBecause of the definition infinite uncountable, When computing probabilities for a continuous random variable, \\(P(X=x)=0\\)\nTherefore, we cannot assign a nonzero probability to each infinitely uncountable value and still have the probabilities sum to one.\nThus, \\(P(X=a)\\) and \\(P(X=b)\\) both equal zero, and the following holds for continuous random variables: \\(P(a &lt;= X &lt;= b) = P(a &lt; X &lt; b) = P(a &lt;= X &lt; b) = P(a &lt; X &lt;= b)\\).\n\nThis is important to consider and compare to discrete probability.\n\n\n6.4.1 Density Functions for Continuous Distributions\n\n6.4.1.1 Probability Density Function\n\nThe Probability Density Function is used to describe continuous random variables.\nProbability Density Function \\(f(x)\\) of a continuous random variable X describes the relative likelihood that \\(X\\) assumes a value within a given interval (e.g., \\(P(a&lt;=X&lt;=b)\\), where \\(f(x)&gt;=0\\) for all possible values of \\(X\\) and the area under \\(f(x)\\) over all values of \\(x\\) equals one.\n\n\n\n6.4.1.2 Cumulative Density Function\n\nThe Cumulative Density Function \\(F(x)\\) of a continuous random variable X suggests that for any value x of the random variable X, the cumulative distribution function \\(F(x)\\) is computed as, \\(F(x) = P(X &lt;= x)\\) as a result, \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nThe goal of cumulative distributions is to find the area under the curve. The pnorm() function computes the probability at point q to find the area under the curve.\n\nThree arguments of the pnorm() command:\n\n\nq is the value of interest;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.\nWe can also work backwards to find a value given a probability. The qnorm() function computes the quantile value at p to find the value associated with a probability.\n\nThree arguments of the qnorm() command:\n\n\np is the cumulative probability;\nThe mean (mean);\nThe standard deviation (sd);\n\nAlso an optional lower.tail parameter, which is defaulted to TRUE signifying &lt; or &lt;=.\n\n\n\n\n6.4.2 The Normal Distribution\n\nThe normal distribution serves as the cornerstone of statistical inference.\nData is symmetric about its mean.\nMean=Median=Mode.\nThe distribution is bell-shaped.\nThe distribution is asymptotic, which means that the tails get closer and closer to the horizontal axis, but never touch it.\nClosely approximates the probability distribution of a wide range of random variables, such as the following:\n\nHeights and weights of newborn babies.\nScores on SAT.\nCumulative debt of college graduates.\n\nThe normal distribution is completely described by two parameters: population mean \\(\\mu\\), which describes the central location of the distribution,and population variance \\(\\sigma^2\\), which describes the dispersion of the distribution.\n\n\n\n\nNormal Distribution\n\n\n\nA special case of the normal distribution:\n\nMean is equal to zero (E(Z) = 0).\nStandard deviation is equal to one (SD(Z) = 1).\n\n\n\n# Solving the figure above using the pnorm() command in R.  P(Z&lt;=0)\npnorm(q = 0, mean = 0, sd = 1)\n\n[1] 0.5\n\n# Or the following because the default value of the mean and sd are 0\n# and 1.\npnorm(0)\n\n[1] 0.5\n\n\n\nIf we assume a mean of 0 and a standard deviation of 1, and we are looking for the probability when the mean is 0, we will get a .5 probability or 50 percent (.5*100). This is because the curve is normal with identical sides.\nWe can also solve this backwards. Below is the qnorm() command, which solves the figure above looking at probability instead of values.\n\n\n# P(Z &gt;= z) = .5\nqnorm(0.5, 0, 1)\n\n[1] 0\n\n# Or\nqnorm(0.5)\n\n[1] 0\n\n\n\n\n6.4.3 Empirical Rule\n\nChevyshev’s Theorem states that at least \\(1 - 1/z^2\\)% of the data lies between \\(z\\) standard deviations from the mean. This result does not depend on the shape of the distribution.\nWith a normal distribution, we can assume approximately the following under the empirical rule:\n\n68% of values within one SD of the mean;\n95% of values within two SD of the mean;\n99.7% of values within three SD of the mean.\n\n\n\n\n\nEmpirical Rule\n\n\n\n\n6.4.4 Calculating z-scores\n\nA z-score allows description and comparison of where an observation falls compared to the other observations for a normally distributed variable.\nA z-score is calculated as the number of standard deviations an observation is away from the mean.\nA normally distributed variable can be used to create z-scores.\nPurpose: This formula calculates the z-score of an individual data point.\nInterpretation: It tells us how many standard deviations a specific value \\(x\\) is from the mean of the dataset.\nThe \\(x_i\\) represents the value of variable \\(x\\) for a single observation, \\(\\mu_x\\) is the mean of the \\(x\\) variable, \\(\\sigma_x\\) is the standard deviation of the \\(x\\) variable. So, \\(z_i\\) is the difference between the observation value and the mean value for a variable and is converted by the denominator into standard deviations. The final z-score for an observation is the number of standard deviations it is from the mean. \\[z_i = (x_i - \\mu_x)/\\sigma_x\\].\nA z score or z value specifies by how many standard deviations the corresponding x value falls above (z &gt; 0) or below (z &lt; 0) the mean.\n\nA positive z indicates by how many standard deviations the corresponding x lies above mean.\nA zero z indicates that the corresponding x equals mean.\nA negative z indicates by how many standard deviations the corresponding x lies below mean.\n\nExample of a z-score calculation: Scores on a management aptitude exam are normally distributed with a mean of 72 and a standard deviation of 8.\n\nWhat is the probability that a randomly selected manager will score above 60?\nFirst, we could transform the random variable X to Z score using the transformation formula:\n\n\n\n(60 - 72)/8\n\n[1] -1.5\n\n\n\nThen, you can calculate the probability using the standard normal distribution, which has a mean of 0 and a standard deviation of 1.\n\n\n# P(Z &gt; -1.5)\npnorm(-1.5, 0, 1, lower.tail = FALSE)\n\n[1] 0.9331928\n\n# Or similarly, we could use the line below because of the default\n# values associated with the pnorm() command\npnorm(-1.5, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nAlso, because R handles the standard normal transformation on our behalf with its inclusion of parameters, we can use the pnorm() command with the mean and standard deviation provided above to calculate the probability in less steps.\n\n\n# Note the same answer as above.  P(X &gt; 60)\npnorm(60, 72, 8, lower.tail = FALSE)\n\n[1] 0.9331928\n\n\n\nTo answer the question, there is a 0.933 probability or a 93.3% chance that a randomly selected manager will score above a 60 on the managerial aptitude exam.\nIn order to get the percentage in R, we simply multiply the answer by 100.\n\n\npnorm(60, 72, 8, lower.tail = FALSE) * 100\n\n[1] 93.31928\n\n\n\n\n6.4.5 Finding Utility in Calculating Probability\n\nExample using pnorm() with Word Problems: Suppose the life of a particular brand of laptop battery is normally distributed with a mean of 6 hours and a standard deviation of 0.9 hours. Use R to calculate the probability that the battery will last more than 8 hours before running out of power and document that probability below.\n\n\n# P(X &gt; 8)\npnorm(8, 6, 0.9, lower.tail = FALSE)\n\n[1] 0.01313415\n\n\n\nThe time for a professor to grade a student’s homework in business statistics is normally distributed with a mean of 15 minutes and a standard deviation of 3.5 minutes. What is the probability that randomly selected homework will require less than 16 minutes to grade?\n\n\n# P(X &lt; 16)\npnorm(16, 15, 3.5)\n\n[1] 0.6124515\n\n\n\nWhat percentage of randomly selected homeworks will require less than 16 minutes to grade?\n\n\npnorm(16, 15, 3.5) * 100\n\n[1] 61.24515\n\n\n\n\n6.4.6 Finding Probability Between Two Values\n\nWe mentioned above that in order to find probability between 2 values a and b, we could use the following equation: \\(P(a&lt;=X&lt;=b) = F(b)- F(a)\\).\nTo use this formula, find P(−1.52 &lt;= Z &lt;= 1.96). This would equal P(Z&lt;=1.96)−P(Z&lt;=−1.52) given a standard normal random variable Z we get the commands below.\n\n\n# P(Z &lt;= 1.96) - P(Z &lt;= -1.52)\npnorm(1.96, 0, 1) - pnorm(-1.52, 0, 1)\n\n[1] 0.9107466\n\n\n\n\n6.4.7 Finding Value Given a Probability\n\nWe use R’s pnorm() and qnorm() commands to solve problems associated with the normal distribution.\nIf we want to find a particular x value for a given cumulative probability (p), then we enter “qnorm(p, μ, σ)”.\n\n\n# P(X &gt; x) = 0.10\nqnorm(0.9, 7.49, 6.41)\n\n[1] 15.70475\n\n# or\nqnorm(0.1, 7.49, 6.41, lower.tail = FALSE)\n\n[1] 15.70475\n\n\n\n\n6.4.8 Finding Utility in Calculating Values from Probability\n\nExample of qnorm() with Word Problems: The stock price of a particular asset has a mean and standard deviation of $58.50 and $8.25, respectively. What is the 95th percentile of this stock price?\n\n\n# P(X&lt;=x) =.95\nqnorm(0.95, 58.5, 8.25)\n\n[1] 72.07004\n\n\n\nThe salary of teachers in a particular school district is normally distributed with a mean of $50,000 and a standard deviation of $2,500. Due to budget limitations, it has been decided that the teachers who are in the top 2.5% of the salaries would not get a raise. What is the salary level that divides the teachers into one group that gets a raise and one that does not?\n\n\n# P(X&gt;=x) =.025\nqnorm(0.025, 50000, 2500, lower.tail = FALSE)\n\n[1] 54899.91\n\n\n\nYou are planning a May camping trip to a National Park in Alaska and want to make sure your sleeping bag is warm enough. The average low temperature in the park for May follows a normal distribution with a mean of 32°F and a standard deviation of 8°F. Above what temperature must the sleeping bag be suited such that the temperature will be too cold only 5% of the time?\n\n\n# P(X&lt;=x) =.05\nqnorm(0.05, 32, 8)\n\n[1] 18.84117",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#central-limit-theorem-clt",
    "href": "probability.html#central-limit-theorem-clt",
    "title": "6  Probability and Probability Distributions",
    "section": "6.5 Central Limit Theorem (CLT)",
    "text": "6.5 Central Limit Theorem (CLT)\n\nCLT refers to the fact that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. Therefore, CLT suggests that for any population X with expected value \\(\\mu\\) and standard deviation \\(\\sigma\\) the sampling distribution of \\(X\\) will be approximately normal if the sample size \\(n\\) is sufficiently large.\nThe CLT tells us that, regardless of the original distribution of a population, the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger. We use the CLT when we want to estimate population parameters (like the mean) from sample data and apply techniques that rely on normality, such as confidence intervals and hypothesis testing. This allows us to make inferences about the population using the normal distribution even when the population itself isn’t normally distributed.\nThe Central Limit Theorem (CLT) holds true for continuous variables, regardless of whether they are normally distributed or not. Generally, the normal distribution approximation is justified when the sample size is sufficiently large, typically \\(n≥30\\). If the sample means are approximately normal, we can transform them into a standard normal form. The standard deviation of the sample means (also called the standard error) can be estimated using the population standard deviation and the sample size that makes up the distribution.\nIf \\(\\bar{X}\\) is approximately normal, then we can transform it using an updated formula of the z-score formula with standard error \\(𝑍=(𝑋- \\mu)/(\\sigma/\\sqrt(n))\\)\nNext, we want to create an experiment where we simulate why the CLT holds true. The rnorm() command pulls random data from a normal distribution. However, even when the sample is small, it can appear not normal - even though it is from a normal distribution.\n\n\nset.seed(1)\n# Sample of n = 10\nhist(rnorm(10), xlim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nIf we increase the sample size, we should all see a nice normal bell shape distribution like the one below.\n\n\nset.seed(1)\n# Sample of n = 1000\nhist(rnorm(1000), xlim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nThe x limits are set from -3 to 3 because a normal distribution follows the empirical rule discussed above (almost all data is within 3 sd of the mean).\nRelating this to the CLT, the CLT states that the sum or mean of a large number of independent observations from the same underlying distribution has an approximate normal distribution. If we were to take a 6 sided dice, in any given roll, we would expect the average. This means that the expected value E(X) will be the mean as discussed above.\n\n\n# Creating a sample\nd &lt;- 1:6\n# Calculating the expected value E(X) which equals the population\n# mean\nmean(d)  #3.5\n\n[1] 3.5\n\n\n\nRolling a dice only one time would not give us enough data to assume a normal distribution under the CLT. However, if we were to roll a higher number of times, and repeat that experiment \\(n\\) number of times, we would expect that as \\(n\\) grows, we would approach a normal distribution.\n\n\n# First, let's roll the dice 1000 times.  We would expect the average\n# like shown below.\nset.seed(1)\nNumberofRolls &lt;- 1000\nx &lt;- sample(d, NumberofRolls, replace = TRUE)\n# The mean(x) is 3.514 and our mean of 1 through 6 is 3.5.  We\n# estimated about the average.\nmean(x)\n\n[1] 3.514\n\nhist(x)\n\n\n\n\n\n\n\n\n\nNext, if we repeat the dice roll experiment that we ran 1,000 times above, we can see the normal distribution start to take shape. The example below has a loop for simulation purposes. This loop rolls x with allowed values 1 to 6 - 1,000 times - and then does that 100 times. The more we do this, the closer we get to approximating the mean (3.5) as our histogram starts to get more narrow.\n\n\nset.seed(1)\nt &lt;- 0\nfor (i in 1:100) {\n    NumberofRolls &lt;- 1000\n    x &lt;- sample(d, NumberofRolls, replace = TRUE)\n    t[i] &lt;- mean(x)\n}\nhist(t, xlim = c(3, 4))\n\n\n\n\n\n\n\n\n\nImportant to note that for any sample size \\(n\\), the sampling distribution of \\(\\bar{x}\\) is normal if the population X from which the sample is drawn is normally distributed, meaning, there is no need to use CLT.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#standard-error-se",
    "href": "probability.html#standard-error-se",
    "title": "6  Probability and Probability Distributions",
    "section": "6.6 Standard Error (SE)",
    "text": "6.6 Standard Error (SE)\n\nThe \\(SE\\) is the standard deviation of the sampling distribution for all samples of size \\(n\\).\nIt is unusual to have the entire population for computing the population standard deviation, and it is also unusual to have a large number of samples from one population. A close approximation to this value is called the standard error of the mean. \\(SE=sd/\\sqrt(n)\\).\nStandard Deviation vs. Standard Error\n\nSD: Measure of variability in the sample.\nSE: Estimate of how closely the sample represents the population.\n\nPurpose: This is the standard error formula used to assess the distribution of sample means around the population mean.\nInterpretation: This measures how much the sample mean, \\(\\bar{X}\\) is expected to vary from the population mean given the sample size \\(n\\).\nThe standard deviation measures the spread of individual data points, while the standard error measures the spread of sample means.\nThe standard deviation and standard error are both measures of variability but are used in different contexts and serve different purposes. The standard deviation quantifies the spread of individual data points within a dataset. It is often used when analyzing the distribution of a single sample or population. On the other hand, the standard error is a measure of the precision of the sample mean in estimating the population mean. It tells us how much the sample mean is likely to vary from the true population mean if we were to take repeated samples. The standard error is derived from the standard deviation but is scaled by the square root of the sample size, meaning it decreases as the sample size increases. This makes it particularly useful in inferential statistics, where it helps assess the accuracy of estimates based on sample data.\nExample using SD: Given that \\(\\mu\\) = 16 inches and \\(\\sigma\\) = 0.8 inches, determine the following: What is the probability that a randomly selected pizza is less than 15.5 inches?\n\n\n# P(X &lt; 15.5)\npnorm(15.5, 16, 0.8)\n\n[1] 0.2659855\n\n\n\nExample using SE: What is the probability that 2 randomly selected pizzas average less than 15.5 inches?\n\n\\(P(\\bar{X} &lt; 15.5)\\)\n\n\npnorm(15.5, 16, 0.8/sqrt(2))\n\n[1] 0.1883796\n\n\nAdditional Examples using SE:\n\nAnne wants to determine if the marketing campaign has had a lingering effect on the amount of money customers spend on coffee. Before the campaign, \\(\\mu\\) = $4.18 and \\(\\sigma\\) = $0.84. Based on 50 customers sampled after the campaign, \\(\\mu\\) = $4.26.\n\\(P(\\bar{X} &gt; 4.26)\\)\n\n\npnorm(4.26, 4.18, 0.84/sqrt(50), lower.tail = FALSE)\n\n[1] 0.2503353\n\n\n\nOver the entire six years that students attend an Ohio elementary school, they are absent, on average, 28 days due to influenza. Assume that the standard deviation over this time period is \\(\\sigma\\) = 9 days. Upon graduation from elementary school, a random sample of 36 students is taken and asked how many days of school they missed due to influenza. What is the probability that the sample mean is between 25 and 30 school days?\n\n\\(P(\\bar{X} &lt; 30)-P(\\bar{X} &lt; 25)\\)\n\n\npnorm(30, 28, 9/sqrt(36)) - pnorm(25, 28, 9/sqrt(36))\n\n[1] 0.8860386\n\n\n\nAccording to the Bureau of Labor Statistics, it takes an average of 22 weeks for someone over 55 to find a new job. Assume that the probability distribution is normal and that the standard deviation is two weeks. What is the probability that eight workers over the age of 55 take an average of more than 20 weeks to find a job?\n\\(P(\\bar{X} &gt;20)\\)\n\n\npnorm(20, 22, 2/sqrt(8), lower.tail = FALSE)\n\n[1] 0.9976611",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#sampling-distribution-of-the-sample-proportion",
    "href": "probability.html#sampling-distribution-of-the-sample-proportion",
    "title": "6  Probability and Probability Distributions",
    "section": "6.7 Sampling Distribution of the Sample Proportion",
    "text": "6.7 Sampling Distribution of the Sample Proportion\n\nEstimator Sample proportion \\(\\bar{P}\\) is used to estimate the population parameter \\(p\\).\nEstimate: A particular value of the estimator \\(\\bar{p}\\).\nThe expected value of \\(\\bar{P}=E(\\bar{P})=𝑝\\).\nThe standard deviation of \\(\\bar{P}\\) is referred to as the standard error of the sample proportion, which equals \\(se(\\bar{P}) = \\sqrt(p*(1-p)/n\\).\nPurpose: This formula calculates the z-score for a sample proportion \\(\\hat{P}\\) by comparing it to a population proportion, \\(P\\).\nInterpretation: It measures how far the sample proportion deviates from the population proportion in terms of the standard deviation (for proportions).\nThe Central Limit Theorem for the Sample Proportion.\n\nFor any population proportion \\(p\\), the sampling \\(\\bar{P}\\) is approximately normal if the sample size \\(n\\) is sufficiently large.\nAs a general guideline, the normal distribution approximation is justified when \\(n*p&gt;=5\\) and \\(n((1-p)&gt;=5\\).\nIf \\(\\bar{P}\\) is normal, we can transform it into the standard normal random variable as \\(Z=(\\hat{P}-E(\\bar{P})/se(\\bar{P})\\) = \\((\\hat{P}-p)/(\\sqrt(p*(1-p)/n))\\).\nUsing the pnorm() function, this would translate to the following: \\(pnorm(\\hat{P}, E(\\bar{P}), se(\\bar{P})\\) , where \\(se(\\bar{P}) = \\sqrt(p*(1-p)/n)\\)\n\n\n\n6.7.1 Examples Using Proportions\n\nAnne wants to determine if the marketing campaign has had a lingering effect on the proportion of customers who are women and teenage girls. Before the campaign, p = 0.43 for women and p = 0.21 for teenage girls. Based on 50 women and 50 teenage girls sampled after the campaign, p = 0.46 and p = 0.34, respectively.\nTo calculate the probability that the marketing campaign had a lingering effect on women, we use \\(P(\\hat{P} &gt;= .46)\\).\n\n\npnorm(0.46, 0.43, sqrt(0.43 * (1 - 0.43)/50), lower.tail = FALSE)\n\n[1] 0.3341494\n\n\n\nThe probability that the observed proportion is 0.46 or higher, assuming the true proportion remains 0.43, is approximately 0.3341.\nTo calculate the probability that the marketing campaign had a lingering effect on teenage girls, we use \\(P(\\hat{P} &gt;= .34)\\).\n\n\npnorm(0.34, 0.21, sqrt(0.21 * (1 - 0.21)/50), lower.tail = FALSE)\n\n[1] 0.01200832\n\n\n\nThe probability that the observed proportion is 0.34 or higher, assuming the true proportion remains 0.21, is approximately 0.0120.\nThe result for teenage girls is statistically significant, while the result for women suggests no strong evidence of an effect.\nThe labor force participation rate is the number of people in the labor force divided by the number of people in the country who are of working age and not institutionalized. The BLS reported in February 2012 that the labor force participation rate in the United States was 63.7%. A marketing company asks 120 working-age people if they either have a job or are looking for a job, or, in other words, whether they are in the labor force. What is the probability that between 60% and 62.5% of those surveyed are members of the labor force?\nHere we find \\(P(\\hat{P} &lt;= .625)-P(\\hat{P} &lt;= .6)\\).\n\n\npnorm(0.625, 0.637, sqrt(0.637 * (1 - 0.637)/120)) - pnorm(0.6, 0.637,\n    sqrt(0.637 * (1 - 0.637)/120))\n\n[1] 0.192639\n\n\n\nThere is a 19.26% chance that 60% to 62.5% of those surveyed are members of the labor force.\nSometimes it is helpful to assign variables so that you can use the consistent functions. The example below does that.\n\n\n\n## Between .625 and .6 - given sample size of 120 and a p of .637\np &lt;- 0.637\nn &lt;- 120\nphat1 &lt;- 0.625\nphat2 &lt;- 0.6\nQ1 &lt;- pnorm(phat1, p, sqrt(p * (1 - p)/n)) - pnorm(phat2, p, sqrt(p * (1 -\n    p)/n))\nQ1  #0.192639\n\n[1] 0.192639",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#transformations-of-variables",
    "href": "probability.html#transformations-of-variables",
    "title": "6  Probability and Probability Distributions",
    "section": "6.8 Transformations of Variables",
    "text": "6.8 Transformations of Variables\n\nIf data is not normally distributed, we need to conduct a transformation. When we transform a variable, we hope to change the shape to normal so that we can continue to calculate under the rules of the normal distribution. For variables that are right skewed, a few transformations that could work to make the variable more normally distributed are: square root, cube root, reciprocal, and log.\nLet’s do an example with the opioid data set discussed earlier, this time using opioidFacility.csv.\nFirst, read in the opioid data set from so we can see a variable that is considered not normal.\n\n\n\n# Distance to substance abuse facility with medication-assisted\n# treatment\ndist.mat &lt;- read.csv(\"data/opioidFacility.csv\")\n# Review the data\nsummary(dist.mat)\n\n    STATEFP         COUNTYFP          YEAR       INDICATOR        \n Min.   : 1.00   Min.   :  1.0   Min.   :2017   Length:3214       \n 1st Qu.:19.00   1st Qu.: 35.0   1st Qu.:2017   Class :character  \n Median :30.00   Median : 79.0   Median :2017   Mode  :character  \n Mean   :31.25   Mean   :101.9   Mean   :2017                     \n 3rd Qu.:46.00   3rd Qu.:133.0   3rd Qu.:2017                     \n Max.   :72.00   Max.   :840.0   Max.   :2017                     \n     VALUE           STATE           STATEABBREVIATION     COUNTY         \n Min.   :  0.00   Length:3214        Length:3214        Length:3214       \n 1st Qu.:  9.25   Class :character   Class :character   Class :character  \n Median : 18.17   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 24.04                                                           \n 3rd Qu.: 31.00                                                           \n Max.   :414.86                                                           \n\n\n\n# Graph the distance variable which is called Value but represents\n# miles.  Note that this graph does not look normal - instead, it\n# looks right or positive skewed.\ndist.mat %&gt;%\n    ggplot(aes(VALUE)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Miles to nearest substance abuse facility\",\n    y = \"Number of counties\")\n\n\n\n\n\n\n\n\n\nNext, transform the variable to the 4 recommended transformations to see which one works best. We cannot see that result yet until we graph these results.\n\nThis requires 4 separate calculations using mutate() commands.\n\n\ndist.mat.cleaned &lt;- dist.mat %&gt;%\n    mutate(miles.cube.root = VALUE^(1/3)) %&gt;%\n    mutate(miles.log = log(x = VALUE)) %&gt;%\n    mutate(miles.inverse = 1/VALUE) %&gt;%\n    mutate(miles.sqrt = sqrt(x = VALUE))\n\nNow, graph the variable with the 4 recommended transformations to see which is most normal (bell shaped).\n\n\ncuberoot &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.cube.root)) + geom_histogram(fill = \"#7463AC\",\n    color = \"white\") + theme_minimal() + labs(x = \"Cube root of miles to nearest facility\",\n    y = \"Number of counties\")\n\nlogged &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.log)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Log of miles to nearest facility\", y = \"\")\n\ninversed &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.inverse)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + xlim(0, 1) + labs(x = \"Inverse of miles to nearest facility\",\n    y = \"Number of counties\")\n\nsquareroot &lt;- dist.mat.cleaned %&gt;%\n    ggplot(aes(x = miles.sqrt)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Square root of miles to nearest facility\",\n    y = \"\")\n\n\nWe can show all 4 graphs at one time to directly compare. Ensure your plot window is large enough to see this.\n\n\ngridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)\n\n\n\n\n\n\n\n\n\nFinally, determine if any of the transformations help. In this example, we determined that the cuberoot had the most normal transformation. The cube root graph contains a nice bell shape curve.\nLet’s use that new variable in the analysis. Start by summarizing the descriptive statistics, including retrieving the mean and standard deviation for cube root of miles, which are values that are required in the probability calculations.\n\n\ndist.mat.cleaned %&gt;%\n    drop_na(miles.cube.root) %&gt;%\n    summarize(mean.tran.dist = mean(x = miles.cube.root), sd.tran.dist = sd(x = miles.cube.root))\n\n  mean.tran.dist sd.tran.dist\n1       2.662915    0.7923114\n\n\n\n2.66 and .79 are the values we pulled for mean and standard deviation. We can use that information to calculate probabilities based on the functions we mentioned above.\nSo, what happens if the cuberoot of X &lt; 3 or less than 27 miles from the facility?\nWe estimate that about 66% of counties fall in the shaded area, having to travel less than 27 miles to nearest facility (27 = 3^3).\nThis means that (1- 0.6665403)*100 is the percentage of countries having to travel more than 27 miles to the nearest facility.\n\n\n27^(1/3)\n\n[1] 3\n\n3^3\n\n[1] 27\n\n# P(X&lt; cuberoot(27) = P(X &lt; 3)\npnorm(3, 2.66, 0.79)  ##about 66% likely\n\n[1] 0.6665403\n\n# P(X &gt; 3) #about 33% likely\npnorm(3, 2.66, 0.79, lower.tail = FALSE)\n\n[1] 0.3334597\n\n1 - pnorm(3, 2.66, 0.79)\n\n[1] 0.3334597\n\n\n\nWe estimate that about 20% of counties fall in the shaded area, having to travel &lt; 8 miles to nearest facility (8 = 2^3).\n\n\npnorm(2, 2.66, 0.79)\n\n[1] 0.2017342\n\n\n\nWe can use the equation to calculate the z-score for a county where you have to drive 15 miles to a facility.\n\n\n## z = (x-m)/sd since we are in cube root - we multiply x by ^1/3\n(15^(1/3) - 2.66)/0.79\n\n[1] -0.2453012\n\n\n\nThe transformed distance of a facility 15 miles away is .24 standard deviations LOWER than the mean transformed distance.\nNext, we can calculate z for a county with residents who have to travel 50 miles to the nearest facility. In the transformed miles variable, this would be the cube root of 50, or a value of 3.68.\n\n\n(50^(1/3) - 2.66)/0.79  #[1] 1.296242\n\n[1] 1.296242\n\n\n\nThis indicated that the transformed distance to a facility with MAT for this example county was 1.29 standard deviations above the mean transformed distance from a county to a facility with MAT.\n\n\n6.8.1 Transformation Second Example\n\nTaking a second example, let us look at the PHYSHLTH variable from the gender dataset (brfss.csv). We worked with this dataset in an earlier lesson. In doing so, we cleaned the data.\nI copied over that data preparation code in regards to the variable of interest (PHYSHLTH), and tidied it up for one example. To remind ourselves, the question being asked was the following, “Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?”\nIf ever you are using the MASS package and dplyr, the select function may have a conflict where R does not know which to use. If you get an error when using select, add dplyr:: in front of the statement to ensure you are using select from dplyr to select variables.\n\n\n#\ngender &lt;- read.csv(\"data/brfss.csv\")\n# Review the data\nsummary(gender)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n# PHYSHLTH example\ngender.clean &lt;- gender %&gt;%\n    dplyr::select(PHYSHLTH) %&gt;%\n    drop_na() %&gt;%\n    # Turn the 77 values to NA, since 77 meant don't know or not sure\n    # from the brss codebook\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA, since 99 meant Refuled from the brss\n    # codebook.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to 0 - since the number 88 meant 0 days of\n    # illness from the brss codebook.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\ntable(gender.clean$PHYSHLTH)\n\n\n     0      1      2      3      4      5      6      7      8      9     10 \n291696  19505  24890  14713   7644  12931   2140   8049   1478    325   9437 \n    11     12     13     14     15     16     17     18     19     20     21 \n   133    908     92   4558   8638    221    153    279     51   5554   1111 \n    22     23     24     25     26     27     28     29     30 \n   132     80     98   2270    149    204    831    390  35701 \n\nsummary(gender.clean)\n\n    PHYSHLTH     \n Min.   : 0.000  \n 1st Qu.: 0.000  \n Median : 0.000  \n Mean   : 4.224  \n 3rd Qu.: 3.000  \n Max.   :30.000  \n NA's   :10299   \n\nqnorm(0.95, mean = 500, sd = 10)\n\n[1] 516.4485\n\n\n\nOnce here, we graph PHYSHLTH.\n\n\n\ngender.clean %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram(fill = \"#7463AC\", color = \"white\") +\n    theme_minimal() + labs(x = \"Number of Days Sick\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe determined from the descriptive statistics lesson that this variable had severe skewness (positive). Most people had 0 days of illness.\nNext, we run the 4 calculations by mutating the variable and saving all 4 transformation under new variable names.\n\n\ngenderTransform &lt;- gender.clean %&gt;%\n    mutate(phy.cube.root = PHYSHLTH^(1/3)) %&gt;%\n    mutate(phy.log = log(x = PHYSHLTH)) %&gt;%\n    mutate(phy.inverse = 1/PHYSHLTH) %&gt;%\n    mutate(phy.sqrt = sqrt(x = PHYSHLTH))\n\n\nNext, we create the 4 graphs for each of the 4 transformations labelled above to see if one helps.\n\n\ncuberoot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.cube.root)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Cube root\", y = \"\")\nlogged &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.log)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 0.5) + theme_minimal() + labs(x = \"Log\", y = \"\")\ninversed &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.inverse)) + xlim(0, 1) + geom_histogram(fill = \"#7463AC\",\n    color = \"white\", binwidth = 0.05) + theme_minimal() + labs(x = \"Inverse\",\n    y = \"\")\nsquareroot &lt;- genderTransform %&gt;%\n    ggplot(aes(x = phy.sqrt)) + geom_histogram(fill = \"#7463AC\", color = \"white\",\n    binwidth = 1) + theme_minimal() + labs(x = \"Square root\", y = \"\")\n\n\nFinally, we plot the graphs using gridExtra so that we can see all 4.\n\n\ngridExtra::grid.arrange(cuberoot, logged, inversed, squareroot)\n\n\n\n\n\n\n\n\n\nIn this example, NOT ONE transformation helped. If this happens, something else would need to occur before correctly using the variable. Examples could be to run a non-linear model, or categorizing the data into bins, especially since there was a large frequency of people that were not ill.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#summary",
    "href": "probability.html#summary",
    "title": "6  Probability and Probability Distributions",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\nIn this lesson, we learned about the basic rules of probability alongside the binomial distribution and continuous distribution. We learned about the normal distribution and the limitations of using that distribution. We also learned how to transform variables that were not normal.\nIn a normal distribution, we used 3 main formulas.\nEach formula has a different role, but they all provide a way to assess variability relative to an average value, allowing for comparisons and inferences about the data or sample in question.\n\n\\[\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{Formula} & \\textbf{Context} & \\textbf{Interpretation} \\\\\n\\hline\n\\frac{x - \\text{mean}(x)}{\\text{sd}(x)} & \\text{Standardizing individual data points} & \\text{Z-score for individual values} \\\\\n\\hline\n\\frac{x - \\text{mean}(x)}{\\text{sd}(x) / \\sqrt{n}} & \\text{Standardizing the sample mean} & \\text{Used for hypothesis tests of means} \\\\\n\\hline\n\\frac{\\hat{p} - p}{\\sqrt{\\frac{p*(1 - p)}{n}}} & \\text{Standardizing a sample proportion} & \\text{Used for hypothesis tests of proportions} \\\\\n\\hline\n\\end{array}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "contingency.html#contingency-tables",
    "href": "contingency.html#contingency-tables",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.1 Contingency Tables",
    "text": "7.1 Contingency Tables\n\nA contingency table generally shows frequencies for two qualitative or categorical variables, x and y.\nEach cell represents a mutually exclusive combination of the pair of x and y values.\n\n\n\n\nContingency Table Example\n\n\n\nNote that each cell in the contingency table represents a frequency.\n\nIn the above table, 174 customers under the age of 35 purchased an Under Armour product.\n54 customers at least 35 years old purchased an Under Armour product.\n\nThe contingency table may be used to calculate probabilities using relative frequency.\n\nFirst obtain the row and column totals.\nSample size is equal to the total of the row totals or column totals. In this case, n = 600.\n\n\n\n7.1.1 Probability Rules\n\nThe addition rule refers to the probability that event A or B occurs, or that at least one of these events occurs, is the following:\n\\[P(A \\cup B) = (P(A) + P(B) - P(A \\cap B)\\]\nThe complement rule refers to the probability of the complement of an event, \\(P(A^c)\\) is equal to one minus the probability of the event.\n\\[P(A^c) = 1-P(A)\\]\nConditional probability refers to the probability of an event given that another event has already occurred.\n\nGiven two events A and B, each with a positive probability of occurring, the probability that A occurs given that B has occurred (A conditioned on B) is equals to \\(P(A|B) = (P(A \\cap B)/P(B)\\). Similarly, the probability that B occurs given that A has occurred (B conditioned on A) is equal to \\(P(B|A) = (P(B \\cap A)/P(A)\\).\n\nMarginal probability refers to the probability of an event occurring \\((P(A))\\), it may be thought of as an unconditional probability. It is not conditioned on another event.\nThe joint probability rule is determined by dividing each cell frequency by the grand total. Joint probability refers to the statistical measure that calculates the likelihood of two events occurring together and at the same point in time.\n\\[P(A \\cap B)\\]\n\nFor example, the probability that a randomly selected person is under 35 years of age and makes an Under Armour purchase is \\(P(U35 \\cap UA)=174/600= .29\\)\n\nFor example, this Venn Diagram illustrates the sample space for events A and B.\nThe union of two events (\\(A \\cup B\\)) is the event consisting of all simple events in A or B.\n\n The intersection of two events (\\(A \\cap B\\)) consists of all simple events in both A and B.\n\n\n\nVenn Diagram: Intersection and Union\n\n\n\nThe complement of event A (i.e., \\(A^c\\)) is the event consisting of all simple events in the sample space S that are not in A.\n\n\n\n\nVenn Diagram: Complement Rule\n\n\n\n\n7.1.2 Summary Measures for a Random Variable\n\nThe Summary Measures of a Random Variable are consistent with our discussion of Probability in the last lesson. They are copied back here for ease of reference.\n\n\n7.1.2.1 Expected Value\n\nWe can calculate the expected value, or value we think is going to occur based on the type of distribution.\n\nExpected value is also known as the population mean \\(\\mu\\), and is the weighted average of all possible values of \\(X\\).\nMore specifically, \\(E(X)\\) is the long-run average value of the random variable over infinitely many independent repetitions of an experiment.\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the expected value of \\(X\\) is the probability weighted average of the values. In the case of one random variable, that means: \\[E(X) = \\mu = \\sum{x_iP(X=x_i)} = n*p\\]\n\n\n\n\n7.1.2.2 Variance\n\nVariance of a random variable is the average of the squared differences from the mean.\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, x_3, ...\\) that occur with probabilities \\(P(X=x_i)\\), the variance is defined as: \\[Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2*P(X=x_i)} = n*p*(1-p)\\]\n\n\n\n\n7.1.2.3 Standard Deviation\n\nStandard deviation is consistently the square root of the variance. \\[SD(X) = \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\sum{(x_i-\\mu)^2*P(X=x_i)}}= \\sqrt{np*(1-p)}\\]\n\n\n\n\n7.1.3 Calculating Probability\n\nWe could calculate the statistics in this contingency table a couple of ways. First, we could manually calculate it like below.\n\n\n## Calculate Totals\nTotalUnder35 &lt;- 174 + 132 + 90\nTotalUnder35  ##Total under 35 age group\n\n[1] 396\n\nTotal35up &lt;- 54 + 72 + 78\nTotal35up  ## Total 35 and older\n\n[1] 204\n\nTotalUA &lt;- 174 + 54\nTotalUA  ## Total Under Armour\n\n[1] 228\n\nTotalNike &lt;- 132 + 72\nTotalNike  ## Total Nike\n\n[1] 204\n\nTotalAdidas &lt;- 90 + 78\nTotalAdidas  ## Total Adidas\n\n[1] 168\n\nTotal &lt;- 174 + 132 + 90 + 54 + 72 + 78\nTotal  ##Grand Total \n\n[1] 600\n\n\n\nAlternatively, we could make this a matrix and use the matrix command to help us calculate the answer. This gives us all the statistics with a couple of functions and allows us to interpret from there after selecting the right information from the output.\n\n\n# Put observed values in a vector by row.\nx &lt;- c(174, 132, 90, 54, 72, 78)\n# Turn the vector into a matrix. In this case, we had 2 rows, 3\n# columns, and entered the data by row.\no &lt;- matrix(x, 2, 3, byrow = TRUE)\no\n\n     [,1] [,2] [,3]\n[1,]  174  132   90\n[2,]   54   72   78\n\n# Next, we could add the names of the columns and rows values using\n# the dimnames() command.\ndimnames(o) &lt;- list(c(\"Under 35\", \"35 and Older\"), c(\"Under Armour\", \"Nike\",\n    \"Adidas\"))\no\n\n             Under Armour Nike Adidas\nUnder 35              174  132     90\n35 and Older           54   72     78\n\n# We can easily calculate the total from here.\ntotal &lt;- sum(o)\ntotal\n\n[1] 600\n\nrowsums &lt;- margin.table(o, 1)\nrowsums\n\n    Under 35 35 and Older \n         396          204 \n\ncolsums &lt;- margin.table(o, 2)\ncolsums\n\nUnder Armour         Nike       Adidas \n         228          204          168 \n\nprob &lt;- prop.table(o)\nprob  # each cell represents a joint probability\n\n             Under Armour Nike Adidas\nUnder 35             0.29 0.22   0.15\n35 and Older         0.09 0.12   0.13\n\nrowprob &lt;- margin.table(prob, 1)\nrowprob  #each value represents marginal probability\n\n    Under 35 35 and Older \n        0.66         0.34 \n\ncolprob &lt;- margin.table(prob, 2)\ncolprob  #each value represents marginal probability\n\nUnder Armour         Nike       Adidas \n        0.38         0.34         0.28 \n\n\n\nTake note that when subsetting from the matrix we created above, the dimnames do not serve as a column or row. so the first number is in row 1, column 1. This is distinct from data.frames, which typically have the 1st column designated for the labels.\n\n\nprob[1, 1]  #corresponds to the joint probability value associated with Row 1 (Under 35), Col 1 (Under Armour)\n\n[1] 0.29\n\n\n\nOnce we do our calculations our totals, we can ask probability questions.\nWhen looking up an answer to the question, it makes it easier to reference the tables to ensure you are subsetting correctly.\n\n\no\n\n             Under Armour Nike Adidas\nUnder 35              174  132     90\n35 and Older           54   72     78\n\nprob\n\n             Under Armour Nike Adidas\nUnder 35             0.29 0.22   0.15\n35 and Older         0.09 0.12   0.13\n\n\n\nIf a person is under 35, what is the probability they will select Under Armour? \\[P(UA|under35) = (P(UA \\cap under35)/P(under35))\\]\n\n\n# This is a conditional probability question.\n174/396  #or\n\n[1] 0.4393939\n\n0.29/0.66  #or\n\n[1] 0.4393939\n\nprob[1, 1]/rowprob[1]\n\n Under 35 \n0.4393939 \n\n\n\nWhat is the probability of a person 35 or older randomly selecting Nike? \\[P(N|older35) = (P(N \\cap older35)/P(older35))\\]\n\n\n# This is a conditional probability question.\n72/204  # or\n\n[1] 0.3529412\n\n0.12/0.34  # or\n\n[1] 0.3529412\n\nprob[2, 2]/rowprob[2]\n\n35 and Older \n   0.3529412 \n\n\n\nIf the chance the preference was the same regardless of condition, how many people would you expect to select Nike? \\[Ex(Nike)\\]\n\n\n# This is a expected value question.\nTotalNike  # or\n\n[1] 204\n\ncolsums[2]\n\nNike \n 204 \n\n\n\nWe can calculate summary measures based on a variable category.\n\n\n# E(X) of 35 Years or older (formula n*p)\nn &lt;- total\nn  #600\n\n[1] 600\n\np &lt;- rowprob[2]\np  #.34\n\n35 and Older \n        0.34 \n\nn * p  #Or simply the frequency for the column 35 and older\n\n35 and Older \n         204 \n\n# Var(X) (formula n*p*(1-p))\nvarx &lt;- n * p * (1 - p)\nvarx  #134.64\n\n35 and Older \n      134.64 \n\n# SD(X) (formula sqrt of variance)\nsdx &lt;- sqrt(varx)\nsdx  #11.60345 \n\n35 and Older \n    11.60345 \n\n\n\n\n7.1.4 Larger Contingency Table\n\nThe survey question asked participants in multiple groups the following question, “How likely are you to participate in the event in the future?”\nIn order to answer this question, first, lets bring in some data and calculate totals.\n\n\n\n\nContingency Table Numbers\n\n\n\nx &lt;- c(197, 388, 230, 103, 137, 98, 20, 18, 18, 13, 58, 45)\no &lt;- matrix(x, 4, 3, byrow = TRUE)\no\n\n     [,1] [,2] [,3]\n[1,]  197  388  230\n[2,]  103  137   98\n[3,]   20   18   18\n[4,]   13   58   45\n\ndimnames(o) &lt;- list(c(\"Students\", \"Faculty/Staff\", \"Alumni\", \"Town Residents\"),\n    c(\"Unlikely\", \"Moderately Likely\", \"Very Likely\"))\no\n\n               Unlikely Moderately Likely Very Likely\nStudents            197               388         230\nFaculty/Staff       103               137          98\nAlumni               20                18          18\nTown Residents       13                58          45\n\ntotal &lt;- sum(o)\ntotal\n\n[1] 1325\n\nrowsums &lt;- margin.table(o, 1)\nrowsums\n\n      Students  Faculty/Staff         Alumni Town Residents \n           815            338             56            116 \n\ncolsums &lt;- margin.table(o, 2)\ncolsums\n\n         Unlikely Moderately Likely       Very Likely \n              333               601               391 \n\n# Lets add the grand total to the colsums\ncolsums &lt;- c(colsums, total)\ncolsums\n\n         Unlikely Moderately Likely       Very Likely                   \n              333               601               391              1325 \n\n\n\nIn this example, we can bind the data together so that it is a little easier to see. Examine the final table and compare that to the figure above.\n\n\ntotals &lt;- cbind(o, rowsums)\ntotals\n\n               Unlikely Moderately Likely Very Likely rowsums\nStudents            197               388         230     815\nFaculty/Staff       103               137          98     338\nAlumni               20                18          18      56\nTown Residents       13                58          45     116\n\ntotals &lt;- rbind(totals, colsums)\ntotals\n\n               Unlikely Moderately Likely Very Likely rowsums\nStudents            197               388         230     815\nFaculty/Staff       103               137          98     338\nAlumni               20                18          18      56\nTown Residents       13                58          45     116\ncolsums             333               601         391    1325\n\n\n\n\n\nContingency Table Proportions\n\n\n\nNext, let’s calculate probabilities.\n\n\nprob &lt;- prop.table(o)\nprob\n\n                  Unlikely Moderately Likely Very Likely\nStudents       0.148679245        0.29283019  0.17358491\nFaculty/Staff  0.077735849        0.10339623  0.07396226\nAlumni         0.015094340        0.01358491  0.01358491\nTown Residents 0.009811321        0.04377358  0.03396226\n\nTotalRowProb &lt;- margin.table(prob, 1)\nTotalRowProb\n\n      Students  Faculty/Staff         Alumni Town Residents \n    0.61509434     0.25509434     0.04226415     0.08754717 \n\nTotalColProb &lt;- margin.table(prob, 2)\nTotalColProb\n\n         Unlikely Moderately Likely       Very Likely \n        0.2513208         0.4535849         0.2950943 \n\nTotalColProb &lt;- c(TotalColProb, sum(TotalColProb))\nTotalColProb\n\n         Unlikely Moderately Likely       Very Likely                   \n        0.2513208         0.4535849         0.2950943         1.0000000 \n\n\n\nIn this example, we can also bind the data together so that it is a little easier to see. Again, we are trying to get our R code in the best shape to be able to select the right information to calculate probability from it.\n\n\nproportions &lt;- cbind(prob, TotalRowProb)\nproportions\n\n                  Unlikely Moderately Likely Very Likely TotalRowProb\nStudents       0.148679245        0.29283019  0.17358491   0.61509434\nFaculty/Staff  0.077735849        0.10339623  0.07396226   0.25509434\nAlumni         0.015094340        0.01358491  0.01358491   0.04226415\nTown Residents 0.009811321        0.04377358  0.03396226   0.08754717\n\nproportions &lt;- rbind(proportions, TotalColProb)\nproportions\n\n                  Unlikely Moderately Likely Very Likely TotalRowProb\nStudents       0.148679245        0.29283019  0.17358491   0.61509434\nFaculty/Staff  0.077735849        0.10339623  0.07396226   0.25509434\nAlumni         0.015094340        0.01358491  0.01358491   0.04226415\nTown Residents 0.009811321        0.04377358  0.03396226   0.08754717\nTotalColProb   0.251320755        0.45358491  0.29509434   1.00000000\n\n\n\nNow that our tables have been tabulated, we can answer some probability questions.\nWhat proportion of Students participated in the survey?\n\\(P_S= 815/1325\\) or \\(0.61509434\\)\nWhat proportion of Faculty/Staff remarked that they were Unlikely to participate?\n\\(P(U|FS) = P(U \\cap FS) / P(FS) = 103/338 = 0.077735849/0.25509434 = 0.3047337\\)\nIf a person is a Town Resident, what is the probability that they will select Very Likely?\n\\(P(VL|TR) = P(VL \\cap TR) / P(TR) = 45/116 = 0.03396226/0.08754717 = 0.387931\\)\nWhat proportion of people would you expect to select Moderately Likely?\n\\(P_ML = 601/1325\\) or \\(0.4535849\\).\nWhat is the expected number of people would you expect to select Moderately Likely? \\(E(ML) = 601\\).\nWhat is the probability that a randomly selected person is a Faculty/Staff or selects Very Likely?\n\\(P_FS + P_VL - P_FS \\cup P_VL = 0.25509434 + 0.2950943 − 0.07396226 = 0.4762264\\)\n\n\n# E(X) of Moderately Likely (formula n*p)\nn = total\nn  #1325\n\n[1] 1325\n\np = TotalColProb[2]\np  #.45358\n\nModerately Likely \n        0.4535849 \n\nn * p  #Or simply the frequency for the column Moderately Likely\n\nModerately Likely \n              601 \n\n# Var(X) (formula n*p*(1-p))\nvarx &lt;- n * p * (1 - p)\nvarx  #328.3955\n\nModerately Likely \n         328.3955 \n\n# SD(X) (formula sqrt of variance)\nsdx &lt;- sqrt(varx)\nsdx  #18.12\n\nModerately Likely \n         18.12169",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#chi-squared-test-for-independence",
    "href": "contingency.html#chi-squared-test-for-independence",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.2 Chi-Squared Test for Independence",
    "text": "7.2 Chi-Squared Test for Independence\n\nBroadly, the test for independence tests whether or not two variables whose frequencies are represented in a contingency table have statistical independence from one another.\nA test of independence – also called a chi-square test of a contingency table – is used to analyze the relationship between two qualitative variables.\nBroadly, this test involves summing the squared differences between observed and expected values, combining the differences into an overall statistic, and then using that statistic to determine significance of the hypothesis test.\n\\(\\chi^2 = \\frac{(observed-expected)^2}{expected}\\)\nSquaring the differences before adding them up will result in a positive value that is larger when there are larger differences and smaller when there are smaller differences. This value captures the magnitude of the difference between observed and expected values.\nIn order to account for situations when the observed and expected values are very large, which could result in extremely large differences between observed and expected, the squared differences are divided by the expected value in each cell.\nVisualizing the Chi-squared Distribution Chi-squared distribution has a single parameter, the degrees of freedom or \\(df\\).\n\\(\\sqrt(2*df)\\) The degrees of freedom in a chi-squared test of independence are calculated as the Number of rows−1 multiplied by Number ofcolumns−1 or more formally: \\(df=(𝑟−1)∗(𝑐−1)\\) The larger the contingency table, the more degrees of freedom, the wider the distribution and more normal it becomes.\n\n\n\n\n\n\n\n\n\n\n\nThe Chi-Squared Distribution is noted by:\n\nRight-Skewed:For low degrees of freedom, the chi-squared distribution is highly right-skewed. As the degrees of freedom increase, the skewness decreases.\nNon-Negative Values: The chi-squared distribution is defined only for non-negative values because it is the sum of squared values.\nMean and Variance: The mean of a chi-squared distribution with k degrees of freedom is k. The variance is 2k.\n\nThe Chi-squared probability density function shows the probability of a value of chi-squared occurring when there is no relationship between the two variables contributing to the chi-squared, where a large chi-square indicates a relationship between variables.\nThe competing hypotheses can be expressed as:\n\n\\(H_0\\): The two classifications are independent.\n\\(H_A\\): The two classifications are dependent.\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.1 Steps to conduct Chi-Squared Test for Independence\n\nStep 1: Write the null and alternate hypotheses.\nStep 2: Compute the test statistic and calculate the probability.\nStep 3: State a conclusion. If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis OR If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\n\n\n\n7.2.2 Assumptions of the Chi-Squared Test of Independence\n\nAssumption 1: The variables must be nominal or ordinal.\nAssumption 2: The expected values should be 5 or higher in at least 80% of groups.\nAssumption 3: The observations must be independent.\nNote that there are a couple of ways observations can be non-independent.\n\nOne way to violate this assumption would be if the data included the same set of people before and after some intervention or treatment.\nAnother way to violate this assumption would be for the data to include siblings or parents and children or spouses or other people who are somehow linked to one another.\nSince people who are linked to each other often have similar characteristics, statistical tests need to be able to account for this and the chi-squared test does not.\n\n\n\n\n7.2.3 Example of a Test for Independence\n\n\n\nContingency Table Example\n\n\n\nStep 1: Set up the Hypothesis\n\n\\(H_0\\): Age group and Brand Name are independent.\n\\(H_A\\): Age group and Brand Name are dependent.\n\nFirst, we bring in the data manually from the table by column. We had 3 columns, so we need to make three vectors as shown below.\n\n\nUA &lt;- c(174, 54)\nN &lt;- c(132, 72)\nA &lt;- c(90, 78)\n\n\nThen we convert these variables into a data frame. If you already have a data frame, you don’t have to do this step.\n\n\nM &lt;- data.frame(UA, N, A)\nM\n\n   UA   N  A\n1 174 132 90\n2  54  72 78\n\n\n\nAlways make sure your data frame looks like the table.\nStep 2: Compute the test statistic and calculate the probability.\nHere, we run the chisq.test() command, which only includes the columns of interest.\n\n\nchisq.test(M)\n\n\n    Pearson's Chi-squared test\n\ndata:  M\nX-squared = 22.529, df = 2, p-value = 1.282e-05\n\n\n\nStep 3: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nHere, our p-value is 1.282e-05, so we reject the null, \\(H_0\\), and conclude that Age group and Brand Names are dependent.\n\n\\(df=(𝑟−1)∗(𝑐−1)\\) or \\((2−1)∗(3−1)=1∗2= 2df\\)\n\n7.2.3.1 Using a matrix instead of a data.frame\n\nYou can use either a matrix or a data.frame when solving a chi-squared problem.\nIn a Chi-squared test of independence, a matrix can be used to organize observed frequencies into categories along two dimensions. In the example, the matrix o contains data for different age groups (“Under 35” and “35 and older”) and their brand preferences (“Under Armour,” “Nike,” and “Adidas”). By setting up the matrix with these dimensions, the test evaluates whether there is a significant association between age group and brand preference. The test compares the observed counts with expected counts under the assumption that the two variables are independent.\n\n\nx &lt;- c(174, 132, 90, 54, 72, 78)\no &lt;- matrix(x, nrow = 2, ncol = 3, byrow = TRUE)\n## add some dimnames to o\ndimnames(o) &lt;- list(c(\"Under 35\", \"35 and older\"), c(\"Under Armour\", \"Nike\",\n    \"Adidas\"))\no\n\n             Under Armour Nike Adidas\nUnder 35              174  132     90\n35 and older           54   72     78\n\nchisq.test(o)\n\n\n    Pearson's Chi-squared test\n\ndata:  o\nX-squared = 22.529, df = 2, p-value = 1.282e-05\n\n# \\tPearson's Chi-squared test data: o X-squared = 22.529, df = 2,\n# p-value = 0.00001282\n\n\nIn either case, we should see the same result. The result includes the Chi-squared statistic (X-squared = 22.529), degrees of freedom (df = 2), and a very small p-value (0.00001282), indicating a strong association between age and brand preference.\n\n\n\n\n7.2.4 Example Using an Imported Dataset\n\nImport the STEM data into a data frame (table) into R.\nStep 1: Set up the Hypothesis:\n\n\\(H_0\\): Stem field and Gender are independent.\n\\(H_A\\): Stem field and Gender are dependent.\n\n\n\nSTEM &lt;- read.csv(\"data/stem.csv\")\nSTEM\n\n   STEM.field Female Male\n1    Sciences    120  100\n2  Technology     15   75\n3 Engineering     30  125\n4        Math     15   20\n\n\n\nStep 2: Compute the test statistic and calculate the probability.\nAgain, we use Rs chisq.test() command to calculate the value of the test statistic, as well as the p-value. Within the chisq.test() function, we indicate that the relevant data are in columns 2 and 3 of the data frame.\n\n\nchisq.test(STEM[, 2:3])\n\n\n    Pearson's Chi-squared test\n\ndata:  STEM[, 2:3]\nX-squared = 66.795, df = 3, p-value = 2.072e-14\n\n## test statistic: 66.795 df: 3 --- 4 rows and 2 columns\n(4 - 1) * (2 - 1)  #3 df\n\n[1] 3\n\n## p-value is almost 0\n\n\nStep 3: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nSince the p-value is less than 0.05, we can reject the null hypothesis. At the 5% significance level, we can conclude that one’s sex does influence field choice within the STEM major.\nWhen performing a Chi-squared test, the choice of including or excluding columns depends on which categories you want to analyze. In chisq.test(stem[, -1]), the first column is excluded, and the test is run on the remaining columns. This might be done if the first column represents data not relevant to the specific hypothesis you are testing (e.g., it could be an identifier or a control category).\nOn the other hand, chisq.test(stem[, 2:3]) only includes the second and third columns, focusing the analysis on specific categories of interest.\nThis is useful when you want to investigate the relationship between two particular variables, ignoring other columns that might be extraneous to the question you’re asking. Including or excluding columns is guided by the research hypothesis and the variables you want to test for independence.\n\n\nchisq.test(STEM[, -1])  ### when we subset we use object[row, columns] --- so including the numbers 2:3 means we include variables in the 2nd and 3rd columns. Including the number -1 after the comma means that we exclude the first column. \n\n\n    Pearson's Chi-squared test\n\ndata:  STEM[, -1]\nX-squared = 66.795, df = 3, p-value = 2.072e-14",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#chi-squared-goodness-of-fit-test",
    "href": "contingency.html#chi-squared-goodness-of-fit-test",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.3 Chi-Squared Goodness of Fit Test",
    "text": "7.3 Chi-Squared Goodness of Fit Test\n\nThere are a few other tests regarding a Chi-squared distribution, and one other is the goodness of fit test.\nThis test determines whether two or more population proportions equal each other or any predetermined set of values and like the test for independence, still involves the fit between observed and estimated values.\nA multinomial experiment consists of a series of n independent trials such that:\nOn each there are k possible outcomes.\nThe probability of falling into category \\(i\\) is the same on each trial.\nThe \\(k\\) probabilities sum to \\(1:p_1 + p_2 +⋯+p_k = 1\\)\nThe null hypothesis: the population proportions are equal to one another or they are each equal to a specific value.\nFor example, are four candidates in an election equally favored by voters?\nEqual Population proportions:\n\\(H_0: P_1=P_2=P_3=P_4=.25;\\)\n\\(H_A\\):Not all populations are equal to .25\nOr, do people rate food quality in a restaurant comparably to last year?\nUnequal Population Proportions:\n\\(H_0: P_1= .4, P_2=.3, P_3= .2, P_4=.1;\\)\n\\(H_A:\\) At least one \\(P_i\\) differs from its hypothesized value.\n\n\n7.3.1 Example Using Raw Numbers\n\nLast year the management at a restaurant surveyed its patrons to rate the quality of its food. The results were as follows: 15% Excellent; 30% Good; 45% Fair; and 10% Poor.\nBased on this and other survey results, management made changes to the menu.\nThis year, the management surveyed 250 patrons, asking the same questions about food quality. Here are the results: 46 Excellent; 83 Good; 105 Fair; and 16 Poor.\nWe want to know if the current results agree with those from last year, or if there has been a significant change.\nTo do this, we compute an expected frequency for each category and compare it to what we actually observe. Then, we compute the difference between what was observed and expected for each category. If the results this year are consistent with last year, these differences will be relatively small.\nThe steps are same as the Chi-Squared Test of Independence.\nStep 1: Set up the Hypothesis:\n\n\\(𝐻_0\\): Each proportion equals a hypothesized value.\n\\(𝐻_A\\): Not all proportions equal their hypothesized values.\n\nStep 2: Compute the test statistic and calculate the probability.\nWe can use R’s chisq.test() function to calculate the value of the test statistic and corresponding p-value. Within this function, we use the option p to indicate the location of the hypothesized proportions.\n\n\n\n# Chi Squared Test Goodness of Fit\no &lt;- c(46, 83, 105, 16)\np &lt;- c(0.15, 0.3, 0.45, 0.1)\n\n## quick check on p\nsum(p)  ##1: sum of all proportions should be 1\n\n[1] 1\n\n# run the test\nchisq.test(o, p = p)\n\n\n    Chi-squared test for given probabilities\n\ndata:  o\nX-squared = 6.52, df = 3, p-value = 0.08888\n\n# test statistic: 6.52 df: 3 ----- 4 proportions - 1 -- or\n4 - 1  ##3\n\n[1] 3\n\n# p-value is 0.08888 &gt; .05\n\n\nNote that p= is required here because it is not the second argument in the chisq.test() command.\nStep 3: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nAt alpha = .05, we do not reject \\(H_0\\) and cannot conclude that the proportions differ from the ones a year ago.\n\n\n\n7.3.2 Example Using an Imported Dataset\n\nThe steps are same as the Chi-Squared Test of Independence.\nStep 1: Set up the Hypothesis:\n\n\\(𝐻_0\\): Each proportion equals a hypothesized value.\n\\(𝐻_A\\): Not all proportions equal their hypothesized values.\n\nStep 2: Compute the test statistic and calculate the probability.\nFirst, we import the stemGOF.csv data into a data frame (table) in R.\nThen, again, we use Rs chisq.test function to calculate the value of the test statistic and the p-value. Within the chisq.test function, we use the option p to indicate the location of the hypothesized proportions.\n\n\nSTEMGOF &lt;- read.csv(\"data/stemGOF.csv\")\nSTEMGOF\n\n            Major X2010Proportions RecentNumbers\n1        Business             0.19            80\n2       Education             0.09            35\n3      Healthcare             0.12            85\n4 Social Sciences             0.22           105\n5            STEM             0.08            55\n6           Other             0.30           140\n\nchisq.test(STEMGOF$RecentNumbers, p = STEMGOF$X2010Proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  STEMGOF$RecentNumbers\nX-squared = 21.526, df = 5, p-value = 0.0006441\n\n# Chi-squared test for given probabilities data:\n# stemGOF$RecentNumbers X-squared = 21.526, df = 5, p-value =\n# 0.0006441 df: 6 options so 6-1 is 5 test statistic is back high:\n# 21. 526\n\n\nStep 3: If the probability that the null is true is very small, usually less than 5%, reject the null hypothesis versus If the probability that the null is true is not small, usually 5% or greater, retain the null hypothesis.\nSince our p-value is 0.0006441, we reject \\(H_0\\) and conclude that the proportions differ at the 5% significance level. In this case, that means that the recent stem numbers differ from the 2010 values.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "contingency.html#summary",
    "href": "contingency.html#summary",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.5 Summary",
    "text": "7.5 Summary\n\nIn this lesson, we learned about contingency tables in regards to calculating probability with two categorical variables. We also computed the chi-squared statistics for the test of independence and the goodness of fit.\nWe note that the “Goodness of fit” and “test of independence” are both chi-square tests, but they serve different purposes:\nGoodness of Fit test: Purpose: Assesses how well observed data match a specified distribution (often a theoretical or expected distribution). Example: Checking if the distribution of colors of M&M’s in a bag matches the expected company distribution (say, 30% blue, 20% red, etc.). Null Hypothesis: The observed frequencies match the expected frequencies.\nTest of Independence: Purpose: Determines whether there is a significant association between two categorical variables in a contingency table. Example: Evaluating if gender (male/female) is independent of preference for a type of product (e.g., A or B). Null Hypothesis: The two variables are independent (no association).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-testing",
    "href": "ttests.html#hypothesis-testing",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.1 Hypothesis Testing",
    "text": "8.1 Hypothesis Testing\n\nA hypothesis test resolves conflicts between two competing opinions (hypotheses).\nIn a hypothesis test, we define the null hypothesis as \\(H_0\\), which is the presumed default state of nature or status quo. We define the alternative hypothesis as \\(H_A\\), a contradiction of the default state of nature or status quo.\nIn statistics we use sample information to make inferences regarding the unknown population parameters of interest. We conduct hypothesis tests to determine if sample evidence contradicts \\(H_0\\).\n\nOn the basis of sample information, we either “Reject the null hypothesis” in which sample evidence is inconsistent with \\(H_0\\). This means that we show support for our alternative hypothesis, \\(H_A\\).\nOr we “Do not reject the null hypothesis.” in which the sample evidence is not inconsistent with \\(H_0\\); or we do not have enough evidence to “accept” \\(H_0\\). This means that there is no support for our alternative hypothesis, \\(H_A\\).\n\nGeneral guidelines:\n\nNull hypothesis, \\(H_0\\), states the status quo.\nAlternative hypothesis, \\(H_A\\), states whatever we wish to establish (i.e., contests the status quo).\n\nUse the following signs in hypothesis tests:\n\n\\(H_0\\): \\(=\\) or \\(&gt;=\\) or \\(&lt;=\\) to specify status quo.\n\\(H_A\\): \\(\\neq\\) or \\(&lt;\\) or \\(&gt;\\) to contradict \\(H_0\\).\n\nWhere \\(H_0\\) always contains the equality sign.\n\n\n8.1.1 Hypothesis Testing Covers a Variety of Tests\n\nWe will cover the following:\n\n\nOne-Sample t-test;\nIndependent samples t-test;\nDependent (or Paired) samples t-test.\n\n\n\n8.1.2 Using a t-Distribution\n\nAll of these hypothesis tests listed above use a t-distribution, which is like a z-distribution (i.e., z-score), but specifically accounts for some specific population information being unknown.\n\nThe z-distribution shows how many sample standard deviations (SD) some value is away from the mean.\nThe t-distribution shows how many standard errors (SE) some value is away from the mean.\n\nAll of these hypothesis tests in the assigned readings use the t.test() command to execute in R with different parameters set.\n\n\n\n8.1.3 Approaches to Hypothesis Testing\n\nAll approaches to scientific hypothesis testing enable us to determine whether the sample evidence is inconsistent with what is hypothesized under the null hypothesis, \\(H_0\\).\nBasic principle: First assume that \\(H_0\\) is true and then determine if sample evidence contradicts this assumption.\nThis means that we can never assume our claim (against status quo) is supported without rejecting the null statistically through the scientific process.\nThere are two general approaches to hypothesis testing:\n\nThe Critical Value Approach\nThe p-value Approach\n\n\n\n8.1.3.1 The Critical Value Approach: Not used here.\n\nIn this approach, we calculate the test statistic, which is also based on the type of test conducted. We then see whether the test statistic is past the critical value either looked up in a table or computed directly from a function in R.\n\n\n\n8.1.3.2 The p-Value Approach: What we are using here.\n\nIn this approach, we still calculate the test statistic, which is still based on the type of test conducted. However, in the p-value approach, this test statistic corresponds directly to a p-value, or probability value, that we use to make a decision on whether the hypothesis is supported or not.\n\nThe p-value is the likelihood, or probability, of observing a sample mean that is at least as extreme as the one derived from the given sample, under the assumption that the null hypothesis is true.\nThe calculation of the p-value depends on the specification of the alternative hypothesis.\nWe set a decision rule to reject \\(H_0\\) if p-value \\(&lt; \\alpha\\), which is set apriori. Specifically, the p-value approach sets an \\(\\alpha\\) value (e.g., .05, .01, .001) and then determines if the p-value calculated is less than the set alpha value. The smaller the \\(\\alpha\\), the more difficult it is to find significance in your alternative hypothesis, where you reject \\(H_0\\).\n\nWe typically set a \\(\\alpha &lt; .05\\) for a variety of tests.\nWe can go lower (e.g. .01, .001), but we typically do not go higher than .05.\n\n\n\n\n\n8.1.3.3 Why Use The p-Value Approach\n\nThe critical value approach has variance in the formulas and numbers looked up (based on distribution and sometimes degrees of freedom), while the p-value is used consistently, we reject the null if our p-value is less than an \\(\\alpha\\) set apriori.\n\n\n\n8.1.3.4 Three Step Procedure Using The p-value Approach\n\nSpecify null and alternative hypothesis.\nCompute the test statistic and calculate the p-value.\n\nInterpret the probability and state the conclusion.\n\n\n\n\n8.1.4 Setting up Hypotheses Tests\n\nThere are three types of hypotheses tests that you can analyze:\n\n\nTwo-tailed test: \\(H_0 =\\) vs \\(H_A \\neq\\)\nRight-tailed test: \\(H_0 &lt;=\\) vs \\(H_A &gt;\\)\nLeft-tailed test: \\(H_0 &gt;=\\) vs \\(H_A &lt;\\)\n\n\nIn a two-tailed test, the null is set to the equality (\\(H_0 =\\)) vs an alternate set to the inequality (\\(H_A \\neq\\)). In a right-tailed test, the null is set to less than or equal to (\\(H_0 &lt;=\\)), while the alternative is set to greater than (\\(H_A &gt;\\)). Finally, in a left-tailed test, the null is set to greater than or equal to (\\(H_0 &gt;=\\)), while the alternative is set to less than (\\(H_A &lt;\\)).\nIn the figure below, the green area is the Rejection region where we reject \\(H_0\\) and show support for our alternative hypothesis \\(H_A\\).\n\nThe rest of the area under the curve is the Failure to Reject \\(H_0\\) region in which we cannot show support for our alternative hypothesis \\(H_A\\).\n\n\n\n\n\nHypothesis Test\n\n\n\nIn a hypothesis test, you have to decide if a claim is true or not. Before you can figure out if you have a left-tailed test or right-tailed test, you have to make sure you have a single tail to begin with. A tail in hypothesis testing refers to the tail at either end of a distribution curve.\n\nStep 1: Write your null hypothesis statement and your alternate hypothesis statement.\nStep 2: Draw a normal distribution curve.\nStep 3: Shade in the related area under the normal distribution curve. The area under a curve represents 100%, so shade the area accordingly. The number line goes from left to right, so the first 25% is on the left and the 75% mark would be at the right tail. \n\n\n\n\n8.1.5 Hypothesis Testing: Single Population: A Right-Tailed Test\n\nA right-tailed test is where your hypothesis statement contains a greater than (&gt;) symbol. In other words, the inequality points to the right.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is greater than the original 90 hours, your hypothesis statements might be:\nNull hypothesis: No change or less than (\\(H_0\\) ≤ 90)\nAlternate hypothesis: Increased change (\\(H_A\\) &gt; 90) \n\n\n\n\n8.1.6 Hypothesis Testing: Single Population: A Left-Tailed Test\n\nA left-tailed test is where your hypothesis statement contains a less than (&lt;) symbol. In other words, the inequality points to the left.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is less than 70 hours, your hypothesis statements might be:\nNull hypothesis: No change or greater than (\\(H_0\\) ≥ 70)\nAlternate hypothesis: Decreased change (\\(H_A\\) &lt; 70)\n\n\n\n\n\nLeft-Tailed Hypothesis Test\n\n\n\n\n8.1.7 Hypothesis Testing: Single Population: A Two-Tailed Test\n\nThe two-tailed test is called a two-tailed test because the rejection region can be in either tail.\n\nFor example, consider comparing the life of batteries before and after a manufacturing change. If you want to know whether the battery life is different from the mean value, your hypothesis statements might be:\nNull hypothesis: Equals mean (\\(H_0 = \\mu\\)).\nAlternate hypothesis: Not equal to mean (\\(H_A \\neq \\mu\\)).\n\n\n\n\n\nTwo-Tailed Hypothesis Test\n\n\n\n\n8.1.8 Forming a Hypothesis\n\nWhen forming a hypothesis, we need to do the following first.\n\nIdentify the relevant population parameter of interest (e.g., \\(\\mu\\)).\nDetermine whether it is a one- or a two-tailed test.\nInclude some form of the equality sign in \\(H_0\\) and use \\(H_A\\) to establish a claim.\n\nFor an example in how to set up a hypothesis, we can look at a trade group that predicts that back-to-school spending will average $606.40 per family this year. The group uses this information to set an economic model in making predictions. A different economic model will be needed if the prediction is wrong.\n\nParameter of interest is \\(\\mu\\) since we are interested in the average back-to-school spending.\nSince we want to determine if the population mean differs from 606.4 (i.e, \\(\\neq\\)), it is a two-tail test.\nThe hypothesis test is as follows: \\(H_0: \\mu = 606.4\\) versus \\(H_A: \\mu \\neq 606.4\\)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#t.test-command",
    "href": "ttests.html#t.test-command",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.2 t.test() Command",
    "text": "8.2 t.test() Command\n\nThe t.test() command can be used in a variety of ways to test many different kinds of hypotheses. It is important to look up that command in your help and see all the different arguments you can change. The following screenshot shows a snippet of this.\n\n\n\n\nt.test Command\n\n\n\nWe use the default S3 method in conducting the t.test:\n\nDefault S3 method: t.test(x, y = NULL,alternative = c(“two.sided”, “less”, “greater”), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, …).\n\nWe change arguments as needed to satisfy the type of test that we are conducting.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#one-sample-t-test",
    "href": "ttests.html#one-sample-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.3 One-Sample t-Test",
    "text": "8.3 One-Sample t-Test\n\nA one-sample t-test compares a mean to a population or hypothesized value.\nA one-sample t-test requires a normal distribution; however, if underlying distribution is not normal, then by the rules outlined in central limit theorem (CLT), the sampling distribution is the approximately normal only if \\(n &gt;= 30\\).\n\n\n8.3.1 t.test() Command for One-Sample t-Test\n\nIn base R, the t.test() command discussed above is useful for getting the t for a one-sample t-test. The command arguments include the name of the variable and the hypothesized or population value (\\(\\mu\\)) to compare it to. We can also include the alternative statement to signify a right-tailed, left-tailed, or two-tailed test.\n\n\n\n8.3.2 Step 1: Specify Null and Alternative Hypothesis\n\nIn order to set up the appropriate null and alternative hypothesis, we need to determine the type of test: two-tailed, right-tailed, or left-tailed.\n\n\n8.3.2.1 Two-Tailed Test\n\nReject \\(H_0\\) on either side of the hypothesized value of the population parameter.\n\n\\(H_0: \\mu =\\mu_0\\) versus \\(H_A\\): \\(\\mu \\neq \\mu_0\\).\n\nThe \\(\\neq\\) symbol in \\(H_A\\) indicates that both tail areas in the distribution will be used to make the decision regarding the rejection of \\(H_0\\).\nA not equal to sign in the alternative signifies a two-tailed test.\n\nAn example of a two-tailed problem: A Dean is interested if the hours of study time per week is different than 12 hours.\n\\(H_0\\): Study hours is not different than 12 hours (\\(=\\)).\n\\(H_A\\): Study hours is different than 12 hours. (\\(\\neq\\)).\n\nExample of command in R:\n\ntwo.sided for two-tailed. However, this is the default value so it could be left out of the statement for the same results.\n\\(\\mu\\) of interest is 12.\n\n\nStudyHours &lt;- read.csv(\"data/StudyHours.csv\")\nsummary(StudyHours)\n\n     Hours      \n Min.   : 4.00  \n 1st Qu.:11.00  \n Median :16.00  \n Mean   :16.37  \n 3rd Qu.:20.50  \n Max.   :35.00  \n\nt.test(StudyHours$Hours, mu = 12, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = 3.5842, df = 34, p-value = 0.001047\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 13.89281 18.85005\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .01 level as indicated by the\n# p-value of .001047.\n\n\n\n\n8.3.2.2 One-Tailed Test\n\nReject \\(H_0\\) only on one side of the hypothesized value of the population parameter.\n\n\\(H_0\\): \\(\\mu &lt;=\\mu_0\\) versus \\(H_A\\): \\(\\mu &gt; \\mu_0\\) (right-tailed test).\n\\(H_0\\): \\(\\mu &gt;=\\mu_0\\) versus \\(H_A\\): \\(\\mu &lt; \\mu_0\\) (left-tailed test).\n\nNote that the inequality in the alternative \\(H_A\\) determines which tail area will be used to make the decision regarding the rejection of \\(H_0\\), right (&gt;) or left (&lt;).\nA greater than sign in the alternative signifies a right-tailed test.\n\nAn example of a right-tailed problem: A Dean is interested if the hours of study time per week is greater than 10 hours.\n\\(H_0\\): Study hours are less than or equal to than 10 hours a week. (\\(&lt;=\\)) \\(H_A\\): Study hours are greater than 10 hours a week. (\\(&gt;\\))\nExample of right-tailed command in R:\n\ngreater for right-tailed.\n\\(\\mu\\) of interest is 10 for 10 hours a week.\n\n\nt.test(StudyHours$Hours, mu = 10, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = 5.224, df = 34, p-value = 4.398e-06\nalternative hypothesis: true mean is greater than 10\n95 percent confidence interval:\n 14.3091     Inf\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 4.398e-06.\n\n\nA less than sign in the alternative signifies a left-tailed test.\n\nExample of a left-tailed problem: A Dean is interested if the hours of study time per week is less than 24 hours.\n\\(H_0\\): Study hours are greater than or equal to than 24 hours a week. (\\(&gt;=\\))\n\\(H_A\\): Study hours are less than 24 hours a week. (\\(&lt;\\))\nExample of left-tailed command in R:\n\nless for left-tailed.\n\\(\\mu\\) of interest is 24 for 24 hours a week.\n\n\nt.test(StudyHours$Hours, mu = 24, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  StudyHours$Hours\nt = -6.2547, df = 34, p-value = 2.015e-07\nalternative hypothesis: true mean is less than 24\n95 percent confidence interval:\n     -Inf 18.43376\nsample estimates:\nmean of x \n 16.37143 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 2.015e-07.\n\n\n\n\n\n\n8.3.3 Step 2. Compute the Test Statistic and calculate the p-value\n\nWhen the population standard deviation \\(\\sigma\\) is unknown, the test statistic for testing the population mean \\(\\mu\\) is assumed to follow the \\(t\\) distribution with a computed degrees of freedom (\\(df\\)) based on whether population variances are assumed to be equal or not.\nFormula for the Test Statistic: \\(t = (m_x-\\mu_x)/(s_x/\\sqrt(n_x))\\)\n\nIn the One-sample t-test, \\(m_x\\) represents the mean of the variable \\(x\\), the variable to be tested, \\(\\mu_x\\) is the population mean or hypothesized value of the variable, \\(s_x\\) is the sample standard deviation of \\(s\\), and \\(n\\) is the sample size.\n\nR will compute the \\(df\\) based on the parameters set in the t.test() command along with the test statistic, so we can rely on it to calculate this for us.\n\n\n\n8.3.4 Step 3. Interpret probability and results.\n\nSet alpha (\\(\\alpha\\)) to a common level, like .05, and compare it to the calculated p-value from your output in R.\nReject \\(H_0\\) if p-value is less than \\(\\alpha\\) value. This means we show support for our alternative hypothesis \\(H_A\\), which is against status quo.\nInterpret the results in plain English.\n\n\n\n8.3.5 Example of a One-Sample t-Test in R\n\nTo conduct a one-sample t-Test in R, first, let’s read in a dataset nhanes2016.csv from your book files and go ahead and load the tidyverse. Tidyverse is loaded so that we can subset the dataset in the next example.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\n\n\nSecond, lets select a variable to test. We are going to compare the mean of variable BPXSY1 to 120, which makes it a two-tailed test (\\(= vs \\neq\\)) with the \\(\\mu\\) set to 120.\nOnce we have our problem idea, we can go through the steps listed above.\nStep 1: Write null and alternative.\n\nH0: There is no difference between mean systolic BP and the cutoff for normal BP, 120 mmHG.\nHA: There is a difference between mean systolic BP and the cutoff for normal BP, 120 mmHG.\n\nStep 2: Calculating test-statistic and p-value.\n\nAgain, the t.test() command does both, the statement and output are listed below.\n\n\nt.test(x = nhanes.2016$BPXSY1, mu = 120, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  nhanes.2016$BPXSY1\nt = 2.4491, df = 7144, p-value = 0.01435\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 120.1077 120.9711\nsample estimates:\nmean of x \n 120.5394 \n\n\nStep 3: Interpret probability and results.\n\nWe found a test statistic is 2.4491, and a corresponding p-value is .01435. If our alpha was set to .05, .01435 is smaller than that (p-value &lt; alpha) This indicates that we should reject the null (\\(H_0\\)) and support the alternative (\\(H_A\\)) that the true mean is not equal to 120. This is in regards to blood pressure.\nTherefore, the sample mean systolic BP not equal to 120, or more formally the mean systolic blood pressure in a sample of 7,145 people was 120.54 (sd = 18.62). And our one-sample t-test found this mean to be statistically significantly different from the hypothesized mean of 120 [t(7144) = 2.449; p = 0.01435]. This indicates that the sample likely came from a population with a mean systolic blood pressure not equal to 120, signifying that the true population is likely not equal to 120.\n\n\n\n\n8.3.6 Example of a One-Sample Using a Subset\n\nLet’s do another example using a subset of the data. In this example, let’s create a subset of people 65+ years old to run the same two-tailed test. This allows us to see if the people 65 years and older had a different blood pressure than 120.\nIn the command below, we use tidyverse to set up a new data frame object named nhanes.2016.65plus to save the filtered data.\n\n\n\nnhanes.2016.65plus &lt;- nhanes.2016 %&gt;%\n    filter(RIDAGEYR &gt;= 65)\n\n\nStep 1: Write null and alternative.\n\nH0: There is no difference between mean systolic BP and the cutoff for normal BP, 120 mmHG when people are 65 or over.\nHA: There is a difference between mean systolic BP and the cutoff for normal BP, 120 mmHG when people are 65 or over.\n\nStep 2: Calculating test-statistic and p-value.\nAgain, we can use one t.test() command to handle steps 2 and 3.\n\n\nt.test(x = nhanes.2016.65plus$BPXSY1, mu = 120)\n\n\n    One Sample t-test\n\ndata:  nhanes.2016.65plus$BPXSY1\nt = 29.238, df = 1232, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 120\n95 percent confidence interval:\n 135.4832 137.7106\nsample estimates:\nmean of x \n 136.5969 \n\n\n\nStep 3: Interpret probability and results.\n\nBased on our output, we found a test statistic is 29.238, and a corresponding p-value is .000. If our alpha was set to .05, .000 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the true mean is not equal to 120 for people 65 and older. This again is in regards to blood pressure.\nThe mean systolic blood pressure in a sample of 1233 NHANES participants who were age 65 and above was 136.60 (sd = 19.93). The mean systolic blood pressure was found to be statistically significantly different from the hypothesized mean of 120 via a one-sample t-test (t(1232) = 29.238, p &lt; 0.001). The true mean systolic blood pressure in the population of adults 65 and older is likely not equal to 120.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#independent-samples-t-test",
    "href": "ttests.html#independent-samples-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.4 Independent Samples t-Test",
    "text": "8.4 Independent Samples t-Test\n\nIndependent samples t-test compares the means of two unrelated groups, so instead of just one population, we have 2 mutually exclusive groups.\nMore formally, two (or more) random samples are considered independent if the process that generates one sample is completely separate from the process that generates the other sample.\n\nThe samples are clearly delineated.\n\\(\\mu_1\\) is the mean of the first population.\n\\(\\mu_2\\) is the mean of the second population.\n\nThe sampling distribution is assumed to be normal and the linear combination of normally distributed random variables is also normally distributed.\nIf underlying distribution is not normal, then by the CLT, the sampling distribution is approximately normal only if both \\(n_1 &gt;= 30\\) and \\(n_2 &gt;= 30\\).\n\n\n8.4.1 t.test() Command for Independent Samples t-Test\n\nIn base R, the t.test() command is useful for getting the t for the Independent samples t-test. This is the same t.test() command used above.\nThe command needs to be altered to handle the second group. In the command, the arguments include a formula which is formatted as t.test(continuous_variable ~ grouping_variable) or t.test(population1, population2). Deciding on the correct format of the argument depends on the shape of the data discussed later on.\n\n\n\n\nTilde or Comma\n\n\n\nUsing the ~ symbol between variables: This method allows you to run the t-test without having to split the data manually if your data is not already split. You specify the numerical variable (your continuous measurement) on the left and the categorical variable (which identifies the two groups) on the right of the ~ symbol.\n\n\n\n8.4.2 Step 1: Specify Null and Alternative Hypothesis\n\nWhen conducting hypothesis tests concerning \\(\\mu_1  - \\mu_2\\) , the competing hypotheses will take one of the following forms:\n\nTwo-tailed Test: \\(H_0: \\mu_1  - \\mu_2 =  d_0\\) versus \\(H_A: \\mu_1  - \\mu_2 \\neq  d_0\\)\nRight-tailed Test: \\(H_0: \\mu_1  - \\mu_2 &lt;=  d_0\\) versus \\(H_A: \\mu_1  - \\mu_2 &gt;  d_0\\)\nLeft-tailed Test: \\(H_0: \\mu_1  - \\mu_2 &gt;=  d_0\\) versus \\(H_A: \\mu_1  - \\mu_2 &lt;  d_0\\)\n\nWhere \\(d_0\\) is the hypothesized difference between \\(\\mu_1  - \\mu_2\\).\n\n\n\n\nIndependent Samples Table\n\n\n\nA separate (\\(d_0\\)) value can be set here under the (\\(\\mu\\)) parameter if the value is not equal to 0, however 0 is the most common which tests for differences between populations (i.e., is the mean of population 1 different than mean of population 2). The alternative statement can be set at “two.sided”, “greater”, or “less”.\nExample of a two-tailed test:\nDoes spending differ between men and women?\n\n\\(H_0\\): No difference in spending between men and women. (\\(=\\)).\n\\(H_A\\): There is a difference in spending between men and women (\\(\\neq\\)).\nExample of a two-tailed command in R:\n\nThe statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. Therefore, the format t.test(var1, var2) should be used.\n\n\nSpend &lt;- read.csv(\"data/Spend.csv\")\nsummary(Spend)\n\n    MenSpend        WomenSpend    \n Min.   : 49.00   Min.   : 13.00  \n 1st Qu.: 89.25   1st Qu.: 62.50  \n Median : 99.00   Median : 84.00  \n Mean   :100.90   Mean   : 87.23  \n 3rd Qu.:117.75   3rd Qu.:100.50  \n Max.   :140.00   Max.   :220.00  \n\n\n\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"two.sided\", mu = 0)\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.1259\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.984055 31.317388\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.1259\n# which is greater than .05.\n\n\nThe alternative \\(H_A\\) and \\(\\mu\\) are not changed off their default values, so the statement above could be simplified from what is provided to the statement below.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend)\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.1259\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.984055 31.317388\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n\n\nExample of a right-tailed test: Is men’s spending (\\(\\bar{x}_1\\)) greater than that of women’s spending (\\(\\bar{x}_2\\))?\n\n\\(H_0\\): Men’s spending is less than or equal to women’s spending (\\(&lt;=\\)).\n\\(H_A\\): Men’s spending is greater than women’s spending (\\(&gt;\\)).\nExample of a right-tailed command in R:\n\nAgain, the statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. The alternative is changed off the default value, so the statement parameter is required.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.06296\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.0521     Inf\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.06296\n# which is greater than .05.\n\n\nExample of a left-tailed test: Is men’s spending (\\(\\bar{x}_1\\)) less than that of women’s spending (\\(\\bar{x}_2\\))?\n\n\\(H_0\\): Men’s spending is greater than or equal to women’s spending (\\(&gt;=\\)).\n\\(H_A\\): Men’s spending is less than women’s spending (\\(&lt;\\)).\nExample of a left-tailed command in R:\n\nAgain, the statement assumes that men’s spending and women’s spending are in 2 different columns in the data set. The alternative is changed off the default value, so the statement parameter is required.\n\n\nt.test(Spend$MenSpend, Spend$WomenSpend, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  Spend$MenSpend and Spend$WomenSpend\nt = 1.559, df = 45.469, p-value = 0.937\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 28.38543\nsample estimates:\nmean of x mean of y \n100.90000  87.23333 \n\n# This test is not significant as indicated by a p-value = 0.937\n# which is greater than .05.\n\n\n\n8.4.3 Step 2: Compute the Test Statistic and Calcuate the p-value\nFormula for the Test Statistic: \\(t = (m_1 - m_2)/\\sqrt((s^2_1/n_1)+(s^2_2/n_2))\\)\n\nIn the independent samples t-test formula, \\(m_1\\) is the mean of one group and \\(m_2\\) is the mean of the other group; the difference between the means makes up the numerator. The larger the difference between the group means, the larger the numerator will be and the larger the t-statistic will be!\nThe denominator includes the variances for the first group, \\(s^2_1\\), and the second group, \\(s^2_2\\) along with the sample sizes for each group, \\(n_1\\) and \\(n_2\\).\nDegrees of freedom are computed as n – k.\nThere is a 95% confidence interval around the difference between the groups.\nCalculate the p-value and Compare it to a Predetermined Alpha level\n\n\n\n8.4.4 Step 3: Interpret the Probability and Write a Conclusion\n\n\n8.4.5 Example of an Independent Samples t-Test in R\n\nUsing the nhanes data set, let’s bring over some code from the last section so that we can work this example. This includes reading in the data set and creating the subset for the ages 65 and over.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\nnhanes.2016.65plus &lt;- nhanes.2016 %&gt;%\n    filter(RIDAGEYR &gt;= 65)\n\n\nWhen conducting independent samples, grouping variables are common, so you can see if there is a difference between groups. In this example, we can test for a difference in blood pressure by sex (Male vs Female). This is coded as 1 and 2 in the data set, so we need to recode it before we begin the official test.\n\n\nnhanes.2016.cleaned &lt;- nhanes.2016 %&gt;%\n    mutate(RIAGENDR = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(sex = RIAGENDR) %&gt;%\n    rename(systolic = BPXSY1)\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): There is no difference between systolic blood pressure for males and females (\\(=\\)).\n\\(H_A\\): There is a difference between systolic blood pressure for males and females (\\(\\neq\\)).\nThis is a two tailed test.\n\nStep 2: Calculating test-statistic and p-value.\n\nThe statement below assumes that the grouping variable (sex) is in one column, and the continuous variable (systolic) is in another column. In this case, instead of a comma between the two columns of sex (like above), we write the statement as t.test(continuousVar ~ groupingVar) like below.\n\n\n# All other parameters can be left at default values\nt.test(nhanes.2016.cleaned$systolic ~ nhanes.2016.cleaned$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  nhanes.2016.cleaned$systolic by nhanes.2016.cleaned$sex\nt = 7.3135, df = 7143, p-value = 2.886e-13\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n 2.347882 4.067432\nsample estimates:\n  mean in group Male mean in group Female \n            122.1767             118.9690 \n\n\nStep 3: Interpret probability and results.\n\nWe found a test statistic is 7.3135, and a corresponding p-value is .000. If our alpha was set to .05, .000 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the men and women had different systolic blood pressure. Thus, there is a statistically significant difference in the mean blood pressure of males and females in the 2016 data set (t(7143) = 7.3135, p = .000).\n\n\n\n\n8.4.6 Example Independent t-test Using a Subset\n\nCreate a subset of the data frame of people 65+ years old to run the same test on blood pressure (two-tailed test).\n\n\nnhanes.2016.65plus.clean &lt;- nhanes.2016.65plus %&gt;%\n    drop_na(BPXSY1) %&gt;%\n    mutate(sex = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(systolic = BPXSY1)\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): There is no difference between systolic blood pressure for males and females 65 years or older\n\\(H_A\\): There is a difference between systolic blood pressure for males and females 65 years or older\nThis is a two tailed test.\n\nStep 2: Calculating test-statistic and p-value.\n\n\nt.test(nhanes.2016.65plus.clean$systolic ~ nhanes.2016.65plus.clean$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  nhanes.2016.65plus.clean$systolic by nhanes.2016.65plus.clean$sex\nt = -2.0141, df = 1231, p-value = 0.04422\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -4.5080955 -0.0591939\nsample estimates:\n  mean in group Male mean in group Female \n            135.4486             137.7323 \n\n\n\nStep 3: Interpret probability and results.\n\nWe found a test statistic is -2.0141, and a corresponding p-value is 0.04422. If our alpha was set to .05, 0.04422 is smaller than that (p-value &lt; alpha). This indicates that we should reject the null and support the alternative that the men and women 65 or older had different systolic blood pressure. Thus, there is a statistically significant difference in the mean blood pressure of males and females over the age of 65 participating in the 2016 NHANES (t(1231) = -2.01, p = 0.044).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#dependent-samples-t-test",
    "href": "ttests.html#dependent-samples-t-test",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.5 Dependent Samples t-Test",
    "text": "8.5 Dependent Samples t-Test\n\nDependent samples or a Paired samples t-test compares the means of two related groups.\nThe Dependent means there is a deliberate Pairing, or more specifically Matched Pairing that generally come from 2 methods:\n\n“Before” and “after” studies characterized by a measurement, some type of intervention, and another measurement, all on the same subject.\n\nExample: Measuring the weight of clients before and after a diet plan.\n\nA pairing of observations, where it is not on the same subject that gets sampled twice.\n\nExample: Matching 20 adjacent plots of land using a nonorganic fertilizer on one half of the plot and an organic fertilizer on the other.\n\n\nThe parameter of interest is the mean difference D where \\(D = x_1 - x_2\\) and the random variable \\(x_1\\) and \\(x_2\\) are a matched pair.\nThis works under the assumption regarding CLT that both \\(x_1\\) and \\(x_2\\) are normally distributed or \\(n &gt;= 30\\).\n\n\n8.5.1 t.test() Command for Dependent Samples t-Test\n\nOption 1: Requires the paired = TRUE parameter: The default for the command is paired=FALSE, which is an independent samples t-test.\nOption 2: Can also calculate a formula instead of turning the paired argument to true and place in the first parameter.\nSummary: In a paired t-test, you’re comparing two related sets of observations, such as measurements taken from the same subjects under different conditions. The key is that each observation in one group has a corresponding observation in the other group. In R, you can perform a paired t-test by either specifying paired = TRUE in the t.test function or by directly calculating the differences between the paired observations and running the t-test on those differences. If you choose the second approach, you subtract one set of values from the other and then use that result as the first argument in the t.test function (essentially calculating the \\(\\mu_d\\). This directly tests whether the mean of the differences is significantly different from zero, giving the same result as a paired t-test but with the differences calculated explicitly beforehand. Either method is fine.\n\n\n\n8.5.2 Step 1: Write Null and Alternative Hypothesis\n\nWhen conducting hypothesis tests concerning \\(\\mu_d\\) , the competing hypotheses will take one of the following forms:\n\nTwo-tailed Test: \\(H_0: \\mu_d  = d_0\\) versus \\(H_A: \\mu_d \\neq  d_0\\)\nRight-tailed Test: \\(H_0: \\mu_d &lt;= d_0\\) versus \\(H_A: \\mu_d&gt;  d_0\\)\nLeft-tailed Test: \\(H_0: \\mu_d &gt;= d_0\\) versus \\(H_A: \\mu_d&lt; d_0\\)\n\nWhere \\(d_0\\) typically is equal to 0, but not always. If \\(d_0\\) is something other than 0, then you change the mu parameter in R in the t.test() command.\n\n\n\n\nDependent Samples Table\n\n\n\nExample of a two-tailed test:\nIs the weight before and after quitting smoking different from each other?\n\nThe before and after signifies a paired test.\n\\(H_0\\): No difference in weight before and after smoking (\\(=\\)).\n\\(H_A\\): There is a difference in weight before and after smoking (\\(\\neq\\)).\nExample of a two-tailed command in R for a paired test:\n\n\nSmoke &lt;- read.csv(\"data/Smoke.csv\")\n# Other default values are left off this statement. Assumes a mu at\n# 0.\nt.test(Smoke$After, Smoke$Before, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 2.695e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 4.870942 9.129058\nsample estimates:\nmean difference \n              7 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 2.695e-08.\n\nExample of a right-tailed test:\nIs the weight after quitting smoking (\\(\\mu_1\\)) greater than the weight before (\\(\\mu_2\\))?\n\n\\(H_0\\): Weight after smoking is less than or equal to the weight before (\\(&lt;=\\)).\n\\(H_A\\): Weight after smoking is greater the weight to before (\\(&gt;\\)).\nExample of a right-tailed command in R for a paired test:\n\n\n# The paired statement is true (due to the before after) If we bring\n# WeightBeforeQuitting to the right side of the equation we get\n# WeightAfterQuitting &gt; WeightBeforeQuitting, which is what our\n# hypothesis wants.  Therefore, in the statement below, after comes\n# first as m1 followed by a comma, followed by the before as m2.  The\n# alternative parameter is then set to 'greater' Other default values\n# are left off this statement\nt.test(Smoke$After, Smoke$Before, paired = TRUE, alternative = \"greater\")\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 1.347e-08\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 5.223767      Inf\nsample estimates:\nmean difference \n              7 \n\n# This test is significant at the .001 level as indicated by the\n# p-value of 1.347e-08.\n\nExample of a left-tailed test:\nIs the weight after quitting smoking (\\(\\mu_1\\)) less than the weight before (\\(mu_2\\))?\n\n\\(H_0\\): Weight after smoking is greater than or equal to the weight before (\\(&gt;=\\)).\n\\(H_A\\): Weight after smoking is less to the weight before (\\(&lt;\\)).\nExample of a left-tailed command in R for a paired test:\n\n\n# The alternative parameter switches to 'less' leaving all else the\n# same as above.  Again, other default values are left off this\n# statement\nt.test(Smoke$After, Smoke$Before, paired = TRUE, alternative = \"less\")\n\n\n    Paired t-test\n\ndata:  Smoke$After and Smoke$Before\nt = 6.6072, df = 49, p-value = 1\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n     -Inf 8.776233\nsample estimates:\nmean difference \n              7 \n\n# This test is not significant as indicated by a p-value of 1, which\n# is greater than .05.\n\n\n\n\n8.5.3 Step 2: Compute the Test Statistic and Calculate the p-value\nFormula for the Test Statistic: \\(t = (m_d-d_0)/\\sqrt((s^2_d/n_d)\\)\n\nDependent samples t-test formula: The \\(m_d\\) is the mean of the differences between the related measures, the \\(s^2_d\\) is the variance of the mean difference between the measures, and \\(n_d\\) is the sample size. The dependent samples t-test worked a little differently from the independent samples t-test. In this case, the formula uses the mean of the differences between the two related measures.\nCalculate the p-value and Compare it to a Predetermined Alpha level\n\n\n\n8.5.4 Step 3: Interpret the Probability and Write a Conclusion\n\nThe steps involved with setting up the null and alternative hypotheses and calculating the test statistic are based on different rules and formulas for all three t-tests, but the rest of the steps are the same.\n\n\n\n8.5.5 Example of an Dependent Samples t-Test in R\n\nGiven the nhanes.2016 dataset, let’s ask the following research question: Is there a difference between measure 1 and 2 for systolic BP?\nFirst, bring back in the data set from the last section.\n\n\nnhanes.2016 &lt;- read.csv(file = \"data/nhanes2016.csv\")\nlibrary(\"tidyverse\")\n\n\nStep 1: Write null and alternative.\n\n\\(H_0\\): No difference between measures 1 and 2 for systolic BP. (\\(=\\))\n\\(H_A\\): There is a difference between measures 1 and 2 for systolic BP. (\\(\\neq\\)).\nThis is a two tailed test.\n\nUse the nhanes.2016.cleaned from above but be sure to rename BPXSY2 to systolic2 (two-tailed test).\n\n\nnhanes.2016.cleaned &lt;- nhanes.2016 %&gt;%\n    mutate(RIAGENDR = recode_factor(.x = RIAGENDR, `1` = \"Male\", `2` = \"Female\")) %&gt;%\n    rename(sex = RIAGENDR) %&gt;%\n    rename(systolic = BPXSY1) %&gt;%\n    rename(systolic2 = BPXSY2) %&gt;%\n    mutate(diff.syst = systolic - systolic2)\n\n\nStep 2: Calculate test-statistic and p-value.\n\n\nt.test(nhanes.2016.cleaned$systolic, nhanes.2016.cleaned$systolic2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  nhanes.2016.cleaned$systolic and nhanes.2016.cleaned$systolic2\nt = 9.3762, df = 7100, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.4310514 0.6589360\nsample estimates:\nmean difference \n      0.5449937 \n\n\n\nStep 3: Interpret probability and results\n\nWe found a test statistic is 9.3762, and a corresponding p-value is 0.000. If our alpha was set to .05, 0.000 is smaller than that (p-value &lt; alpha). This indicates that there is a difference in systolic blood pressure 1 and 2. Thus, there is a statistically significant difference in the mean blood pressure of reading 1 and 2 in the 2016 NHANES (t(7100) = 9.3762, p = 0.000).\n\n\n\n\n8.5.6 reshape function\n\nThe reshape function allows you to restructure a data frame by specifying a timevar (The variable in the data that defines the different time points or conditions), and idvar (The variable that uniquely identifies each subject or observational unit), and a direction (“wide” or “long”)\nThe syntax is as follows: reshape(data, timevar, idvar, direction, …)\nThe direction of reshaping, either “wide” to spread the data across columns or “long” to gather the data into rows.\n\n\ndata(\"sleep\")\nsummary(sleep)\n\n     extra        group        ID   \n Min.   :-1.600   1:10   1      :2  \n 1st Qu.:-0.025   2:10   2      :2  \n Median : 0.950          3      :2  \n Mean   : 1.540          4      :2  \n 3rd Qu.: 3.400          5      :2  \n Max.   : 5.500          6      :2  \n                         (Other):8  \n\nsleep2 &lt;- reshape(sleep, timevar = \"group\", idvar = \"ID\", direction = \"wide\")\nsummary(sleep2)\n\n       ID       extra.1          extra.2      \n 1      :1   Min.   :-1.600   Min.   :-0.100  \n 2      :1   1st Qu.:-0.175   1st Qu.: 0.875  \n 3      :1   Median : 0.350   Median : 1.750  \n 4      :1   Mean   : 0.750   Mean   : 2.330  \n 5      :1   3rd Qu.: 1.700   3rd Qu.: 4.150  \n 6      :1   Max.   : 3.700   Max.   : 5.500  \n (Other):4                                    \n\nt.test(sleep2$extra.1, sleep2$extra.2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sleep2$extra.1 and sleep2$extra.2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\n\nUsing tidyverse, we could get the same effect using the filter, select, and rename functions which are a part of dplyr. This method involves dividing the dataset into two separate groups based on a categorical variable. You then run the t-test on these two groups. In the code below, we use filter to divide by group and then after selecting and renaming the columns we need, we join back together using a full_join.\nUsing full_join ensures that all IDs are included in the resulting dataset, even if they only appear in one of the groups. This can be useful if your data might have missing group entries for some IDs.\n\n\ndata(\"sleep\")\nsummary(sleep)\n\n     extra        group        ID   \n Min.   :-1.600   1:10   1      :2  \n 1st Qu.:-0.025   2:10   2      :2  \n Median : 0.950          3      :2  \n Mean   : 1.540          4      :2  \n 3rd Qu.: 3.400          5      :2  \n Max.   : 5.500          6      :2  \n                         (Other):8  \n\nlibrary(tidyverse)\ngroup1 &lt;- sleep %&gt;%\n    filter(group == 1) %&gt;%\n    select(ID, extra) %&gt;%\n    rename(extra.1 = extra)\n\ngroup2 &lt;- sleep %&gt;%\n    filter(group == 2) %&gt;%\n    select(ID, extra) %&gt;%\n    rename(extra.2 = extra)\n\nsleep2 &lt;- group1 %&gt;%\n    full_join(group2, by = \"ID\")\nt.test(sleep2$extra.1, sleep2$extra.2, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  sleep2$extra.1 and sleep2$extra.2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\n\nIn either case, we reject the null hypotheses and find that the type of drug does affect the sleep in patient",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "ttests.html#summary",
    "href": "ttests.html#summary",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.7 Summary",
    "text": "8.7 Summary\n\nIn summary, we learned about the basic rules of hypothesis testing and how to set up a null and alternative hypothesis. We learned about two-tailed, right-tailed, and left-tailed hypotheses. We learned about 3 t.tests: the one-sample t-test, the independent samples t-test, and the dependent (or paired) samples t-test. Finally, we also learned how to interpret the results using the p-value approach.\nTake note that this lesson does not include the assumptions for t-tests. Assumptions are extremely important, but I will leave the discussion of them to another class.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "anova.html#analysis-of-variance-anova",
    "href": "anova.html#analysis-of-variance-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.1 Analysis of Variance (ANOVA)",
    "text": "9.1 Analysis of Variance (ANOVA)\n\nAn ANOVA is used to determine if there are differences among three or more groups. If there were only two groups, an independent samples t-test should be used.\nIn conducting an ANOVA, we utilize a completely randomized design, comparing sample means computed for each treatment to test whether the population means differ.\nANOVA has underlying assumptions to be met, and there are alternative methods to use when the assumptions are not met. The assumptions are extensions of those we use when comparing just two populations in a t.test:\n\nThe populations are normally distributed.\nThe population standard deviations are unknown but assumed equal.\nSamples are selected independently from each population.\n\nHere we compare a total of \\(k\\) populations, rather than just two. Therefore, the competing hypotheses for the one-way ANOVA:\n\n\\(H_0: \\mu_1 = \\mu_2 = \\cdots= \\mu_k\\)\n\\(H_A:\\) Not all population means are equal\n\n\n\n9.1.1 Omnibus test\n\nA significant result indicates the omnibus test is significant and that there is a difference between the means. This only suggests that there is at least one group difference somewhere between groups. This is not useful in determining which means are different from each other. Therefore, for the alternative hypothesis to be supported, at least one group must be different from the rest.\nThen, if we find a significant omnibus test, we continue our analysis with planned contrasts and post-hoc tests, which determine which means are statistically significantly different from one another.\n\n\n\n9.1.2 ANOVA Methodology\n\nWe first compute the amount of variability between the sample means. This is known as the between-treatments estimate, which compares the sample means to the overall mean, sometimes called the grand mean, or the average pf all the values from the data set.\nThen, we measure how much variability there is within each sample. This is known as the within-treatments estimate, which is essentially a measure of error.\nA ratio of the first quantity to the second forms our test statistic which follows the \\(F_{df1,df2}\\) distribution, where the degrees of freedom are calculated from the number of groups - 1 (\\(df_1: k-1\\)) and the total number of observations minus the number of groups (\\(df_2: n_t-k\\)).\n\n\nNote an F distribution behaves differently than a z- or a t- distribution.\n\nThe z-distribution shows how many sample standard deviations (SD) some value is away from the mean.\nThe t-distribution shows how many standard errors (SE) away from the mean.\nThe F-distribution is used to compare 2 populations’ variances. \n\nThe F distribution is noted by its:\n\nRight-Skewness: The distribution is positively skewed, meaning the right tail is longer than the left tail.\nNon-Negative Values: The F-distribution is only defined for positive values.\nShape and Degrees of Freedom: The exact shape of the F-distribution depends on the degrees of freedom. As the degrees of freedom increase, the distribution becomes less skewed and more closely approximates a normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#type-i-and-type-ii-errors",
    "href": "anova.html#type-i-and-type-ii-errors",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.2 Type I and Type II Errors",
    "text": "9.2 Type I and Type II Errors\n\nIn any hypothesis test, including an ANOVA, it is important to acknowledge that there is error, and that it is possible to come up with the wrong conclusion. An important note is that we are calculating results and making conclusions based on probability calculated given the sample we have, the timing it was collected, and any biases that might have been used in designing and securing the data set. We could also be violating assumptions that make the findings less accurate if still accurate at all.\n\nTherefore, it is really important to understand the limitations of what we are doing, and that there is always error! Our main goal as analysts is to minimize error as much as possible in selecting the right parameters, and having a data set that is as unbiased as possible so that the interpretation is as accurate as possible and consistent with the population it is inferring for.\nThere are 2 types of error:\n\nType I Error: Committed when we reject \\(H_0\\) when \\(H_0\\) is actually true. False Positive.\n\nOccurs with probability \\(\\alpha\\). \\(\\alpha\\) is chosen apriori.\n\nType II Error: Committed when we do not reject \\(H_0\\) and \\(H_0\\) is actually false. False Negative.\n\nOccurs with probability \\(\\beta\\). Power of the Test = 1−\\(\\beta\\).”For a given sample size \\(n\\), a decrease in \\(\\alpha\\) will increase \\(\\beta\\) and vice versa.”\n\n\nBoth \\(\\alpha\\) and \\(\\beta\\) decrease as \\(n\\) increases. Therefore, an increase in sample size decreases these two types of error.\nThe two types of error can be mapped onto a hypothesis decision chart that shows the two decisions, reject \\(H_0\\) or fail to reject \\(H_0\\), alongside what is actually happening in reality.\n\n\n\n\nType I and Type II Error\n\n\n\nConsider the following example of competing hypotheses that relate to the court of law.\n\n\\(H_0\\): An accused person is innocent.\n\\(H_A\\): An accused person is guilty.\n\nNow, think through the consequences of making either a Type I and Type II error:\n\nType I error (False Positive): Conclude that the accused is guilty when in reality, they are innocent.\nType II error (False Negative): Conclude that the accused is innocent when in reality, they are guilty.\n\n\n - Both types of error are extremely bad!\n\nIn taking another example, let’s look at some sample results from a polygraph. A polygraph (lie detector) is an instrument used to determine if an individual is telling the truth. These tests are considered to be 89% reliable. In other words, if an individual lies, there is a 0.89 probability that the test will detect a lie. Let there also be a 0.10 probability that the test erroneously detects a lie even when the individual is actually telling the truth. Consider the null hypothesis, “the individual is telling the truth,” and look at all 4 options.\n\n\n\n\nType I and Type II Error\n\n\n\nWith the error conclusions from Type I and Type II error in this example, we either predicted that someone was being honest when they were telling a lie, or we called someone a liar that was telling the truth! Again, both are bad.\nNow for a more recent example. Let’s look at the vaccine designed to prevent the spread of COVID-19. Again, we cannot assume the vaccine works without a significant test, so the alternative hypothesis \\(H_A\\) is framed as against status quo, or that the vaccine does help.\n\n\\(H_0\\): Vaccine does not help prevent spread of COVID-19.\n\\(H_A\\): Vaccine does help prevent the spread of COVID-19.\n\n\nNow, think through the consequences of making either a Type I and Type II error and come to the fill out the 4 boxes like I did below.\n\n\n\n\nType I and Type II Error\n\n\n\nIn this scenario, if we made a Type I error, we would make people get vaccines that were not helpful. If we made a Type II error, we would end up not administering the vaccine when in fact it actually could help prevent the spread of COVID-19. In making the decision to give the vaccine, these types of errors were weighed (along with others) and the vaccine ended up being administered because the Type II error was considered more problematic than the Type I.\nSome tests are designed to minimize Type I error, while others are designed to minimize Type II error. Selecting alpha levels that are smaller will help reduce Type I error, but at the cost of Type II error. And again, increasing sample size reduces both error.\n\n\n9.2.1 Some Reasons For Error\n\nMeasurement error refers to the difference between a measured quantity and its true value which could be due to random error or systematic error.\n\nRandom error refers to naturally occurring errors that are to be expected.\nSystematic error refers to miss-calibrated instruments causing error in measurement.\n\nBias — the tendency of a sample statistic to systematically over- or underestimate a population parameter.\n\nSelection bias refers to a systematic exclusion of certain groups from consideration for the sample.\nNon-response bias refers to a systematic difference in preferences between respondents and non-respondents to a survey or a poll.\nSocial Desirability bias refers to a bias that refers to the systematic difference between a group’s “socially acceptable” responses to a survey or poll.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#one-way-anova",
    "href": "anova.html#one-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.3 One-Way ANOVA",
    "text": "9.3 One-Way ANOVA\n\nOne-way ANOVA compares population means based on one categorical variable.\n\n\n9.3.1 Steps for Conducting a One-Way Anova\n\nWrite the null and alternate hypotheses.\nCompute the F-test statistic.\nCompute the probability for the test statistic (p-value).\nInterpret the probability.\nWrite a conclusion.\nIf model is significant, run post-hoc tests.\n\n\n\n9.3.2 Statistics in a One-Way ANOVA Table\n\nThere are a number of statistics being calculated with an aov() command, with the goal of producing the F-test statistic, which corresponds to a p-value that we can interpret the same way as we did in the earlier lessons (p-value &lt; alpha = significant result - reject the \\(H_0\\)).\n\n\n\n\nOne Way ANOVA Formulas\n\n\n\n9.3.2.1 Explained Variance\n\nIn a one-way ANOVA, first, we compute the explained variance, which in a one-way ANOVA is the sum of squares due to treatments (SSTR), where the treatment is our grouping variable. The explained variance suggests that the variation in outcome can be explained by a model, or in our case, a grouping variable. In computing the SSTR, we square the deviation between each group mean (\\(\\bar{y}_j\\)) and the grand mean \\(\\bar{y}\\) and multiply it by the sample size (\\(n_j\\)) and sum up all the values \\(\\sum\\). This leads to the following formula:\n\n\\(SSTR = \\sum{n_j*(\\bar{y}_j-\\bar{y}) ^2}\\)\n\nDegrees of freedom for the SSTR are computed by the number of groups \\((k-1)\\).\n\n\\(df_{sstr} = k-1\\)\n\nThe mean square due to treatment (MSTR) takes the value for the SSTR and divides by the treatment’s degrees of freedom \\((k-1)\\).\n\n\\(MSTR = SSTR/(k-1)\\)\n\nAbove in the overall methodology, the MSTR corresponds to the between-treatments estimate.\n\n\n\n9.3.2.2 Unexplained Variance\n\nThere is also a measure of unexplained variance, which we term error. In a one-way ANOVA, this unexplained variance refers to variability in the outcome that is not explained by the grouping variable. In the table, this error is called the sum of squares error (SSE). SSE is calculated by first multiplying the sample size of group and subtracting one (\\(n_j-1\\)), and then multiplying that number by its group variance (\\(\\sigma^2_i\\)). Once all groups are calculated, we add it up (\\(\\sum\\)). This leads to the following formula:\n\n\\(SSE = \\sum^k_{j=1}{(n_j-1)*\\sigma^2_j}\\)\n\nThis formula above considers the formula and definition of variance (\\(\\sigma^2\\)) and simplifies the formula, which you can also use:\n\n\\(SSE = \\sum^k_{j=1}\\sum^n_{i=1}(y_{ij}-\\bar{y_j})^2/(n-k)\\)\n\nDegrees of freedom for the SSE are computed by taking the total number of observations (\\(n_t\\)) and subtracting the number of groups \\((k)\\).\n\n\\(df_{sse} = n_t-k\\)\n\nThe mean square due to error (MSE) takes the SSE and divides by the appropriate degrees of freedom (\\(n_t-k\\)).\n\n\\(MSE = SSE/(n_t-k)\\)\n\nAbove in the overall methodology, the MSE corresponds to the within-treatments estimate.\nThere is a total sum of squares (SST) being calculated in an ANOVA, which refers to the total variance (explained and unexplained). This value is calculated as the total number of observations minus 1 (\\(n_t-1\\)). We don’t see it explicitly in our R output, but it is needed to help understand the full model.\n\n\n\n9.3.2.3 Calculating F\n\nFinally, to calculate the F-test statistic, we take the MSTR (explained variability) and divide by the MSE (unexplained variability) considering the appropriate degrees of freedom (\\(k-1\\) and \\(n_t-k\\)).\nA p-value is computed from that statistic mathematically by using the correct command in R.\n\n\\(F_{k-1,n_t-k} = MSTR/MSE\\)\n\n\n\n\n\n9.3.3 Example of a One-Way ANOVA\n\nLike the t-test, a one-way ANOVA follows 5 steps, and then includes an optional 6th step after significance is found and confirmed in step 5.\n\n\n9.3.3.1 Step 1: Set Up Null and Alternative Hypothesis\n\nThe competing hypotheses for the one-way ANOVA:\n\n\\(H_0: \\mu_{Atlanta} = \\mu_{Houston}= \\mu_{LosAngeles} = \\mu_{SanFran} = \\mu_{DC}\\)\n\\(H_A:\\) Not all population means of congestion levels are equal among the cities\n\n\n\n\n9.3.3.2 Steps 2 and 3: Compute the F-test Statistic and p-Value\n\nFirst, we read in the data set and give a look at what is included.\n\nBecause we are using read.csv() command, we can set stringsAsFactors argument to TRUE so that the categorical variable will be coded appropriately upon download.\nThe summary() command suggests that there is one categorical variable, “City” and one continuous variable, “CongestionRating.”\n\n\ncongestionData &lt;- read.csv(\"data/congestion.csv\", stringsAsFactors = TRUE)\nsummary(congestionData)\n\n             City    CongestionRating\n Atlanta       :25   Min.   :44.00   \n Houston       :25   1st Qu.:56.00   \n Los Angeles   :25   Median :59.00   \n San Francisco :25   Mean   :60.06   \n Washington, DC:25   3rd Qu.:64.00   \n                     Max.   :77.00   \n\n\nThe oneway.test() command is a common command for a one-way ANOVA. The oneway.test() function in R is used to perform a one-way analysis of variance (ANOVA) while allowing for the possibility of unequal variances (heteroscedasticity) among groups. It is based on Welch’s ANOVA, which is a version of ANOVA that does not assume equal variances across groups although you can set it to be true like we did below.\n\n\noneway.test(CongestionRating ~ City, congestionData, var.equal = TRUE)\n\n\n    One-way analysis of means\n\ndata:  CongestionRating and City\nF = 37.251, num df = 4, denom df = 120, p-value &lt; 2.2e-16\n\n\n\nThe aov() function in R is also used for performing ANOVA, but it assumes that the variances of the groups are equal (homoscedasticity). This function is suitable when the assumption of equal variances is met.\n\n\n# A common set of commands that work for both a one-way and two-way\n# ANOVA: aov() command with output in a followup command: either\n# anova() or summary() command.\nAnova1way &lt;- aov(CongestionRating ~ City, data = congestionData)\n\n\nIn either command, our formula is ContinuousVariable ~ CategoricalVariable - this will allow us to see if there are group differences in CongestionRating (continuous) based on city (categorical).\n\nTake a good look at the output from these commands to compare the results.\n\n\n\n# anova() command works exactly the same as summary here.  This\n# command is very prevalent in various textbooks and online\n# tutorials.\nsummary(Anova1way)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nCity          4   3251   812.7   37.25 &lt;2e-16 ***\nResiduals   120   2618    21.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(Anova1way)\n\nAnalysis of Variance Table\n\nResponse: CongestionRating\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nCity        4 3250.7  812.67  37.251 &lt; 2.2e-16 ***\nResiduals 120 2617.9   21.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nUsing the anova() or summary() commands, we can see the results from the ANOVA table, including the degrees of freedom (Df), sum of squares (Sum Sq), mean square (Mean Sq), F-test statistic (F value) and p-value (Pr(&gt;F)). It also includes asterisks (*) symbols if there are group differences due to our categorical variable. The more asterisks you see, the smaller the significant code. There is a table for these codes within the output marked Signif. codes.\nReading the codes:\n\nIf there are three asterisks, you would say that that your p-value is less than .001. If there are 2, your p-value is less than .01. 1 asterisk indicates a p-value &lt; .05.\nWe never say our p-value is 0, but instead say it is less than ___.\nWe do not use .1 as a significance level, although some statisticians mark a p-value less than .1 as “marginally significant”. There is a big debate about this, but I agree with the general consensus, so our threshold for this class is less than .05. Anything greater than or equal to .05 is not significant.\n\n\n\n\n9.3.3.3 Steps 4 and 5: Interpret the Probability and Write a conclusion.\n\nThe F-test statistic is 37.251 and the p-value is &lt; 2.2e-16 ***. This p-value is &lt; all typical alpha values like .05 or .001. This value is very close to 0. We would state that our p-value is less than .001 by looking at the table and noting the three asterisks.\n\nThe p-value result suggests that we support the alternative \\(H_A\\): Not all population means are equal.\nMore specifically we reject the null hypothesis \\(H_0\\) in support of \\(H_A\\) and conclude that not all congestion levels are equal among the cities.\n\n\n\n9.3.3.4 Step 6: If Model is Significant, Run Post-Hoc Tests\n\nPairwise comparisons: A statistical strategy for comparing different groups, often used after a statistically significant ANOVA to test hypotheses about which group means are statistically significantly different from one another.\n\nBonferroni Method;\nTukeyHSD Method\n\nThere are many tests to compute pairwise comparisons. The tests tend to vary in whether they minimize Type I or Type II error, and by how much. Many of the tests change the distribution in the calculation to alter the final results.\nPlanned Comparisons: also known as a priori comparisons or planned comparisons, are a statistical technique used in the analysis of variance (ANOVA) to compare specific means based on hypotheses formulated before examining the data. Unlike post hoc tests, which compare all possible pairs of means and are conducted after data collection, planned contrasts are designed to test specific hypotheses derived from theoretical considerations or previous research.\n\n\n9.3.3.4.1 Bonferroni Method\n\nThe Bonferroni method is a pairwise post-hoc test that is used after finding a statistically significant ANOVA. This method conducts a t-test for each pair of means and adjusts the threshold for statistical significance to ensure that there is a small enough risk of Type I error; it is generally considered a very conservative post hoc test that only identifies the largest differences between means as statistically significant.\nThe function has several arguments, such as \\(x =\\) for the continuous variable (listed first); \\(g =\\) for the grouping or categorical variable (listed 2nd); and the p-value adjustment, \\(p.adj =\\), which can be set as __bonf_ for Bonferroni (listed 3rd).\nThe output is a matrix of p-values, testing each pair for group differences. A value &lt; alpha signifies significant group differences.\n\nBased on the output, there are no differences in congestion levels between Houston and Atlanta, San Francisco and Atlanta, and Houston and San Francisco.\nThere are group differences in congestion levels between all other remaining groups (p-value &lt; alpha).\n\n\n\npairwise.t.test(congestionData$CongestionRating, congestionData$City, p.adj = \"bonf\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  congestionData$CongestionRating and congestionData$City \n\n               Atlanta Houston Los Angeles San Francisco\nHouston        1.00000 -       -           -            \nLos Angeles    9.7e-15 1.3e-15 -           -            \nSan Francisco  1.00000 1.00000 &lt; 2e-16     -            \nWashington, DC 0.00269 0.00071 2.1e-06     3.8e-05      \n\nP value adjustment method: bonferroni \n\n\n\n\n9.3.3.4.2 Tukey HSD Method\n\nThe Tukey HSD method is another pairwise post-hoc test that is used after finding a statistically significant ANOVA.\nThis method is used to determine which means are statistically significantly different from each other by comparing each pair of means.\nThis method is less conservative than the Bonferroni post hoc test, which means generally less Type II error (False Negative).\nThis test is modified from the Bonferroni test using a q-distribution instead of a t-distribution to calculate the answers.\nUsing the TukeyHSD() command, we insert the Anova1way object as a parameter that we made with the aov() command above. This aov() command is required before or while running the TukeyHSD() command.\n\n\nTukeyHSD(Anova1way)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = CongestionRating ~ City, data = congestionData)\n\n$City\n                               diff        lwr        upr     p adj\nHouston-Atlanta               -0.48  -4.139015   3.179015 0.9962320\nLos Angeles-Atlanta           12.24   8.580985  15.899015 0.0000000\nSan Francisco-Atlanta         -1.44  -5.099015   2.219015 0.8113836\nWashington, DC-Atlanta         4.96   1.300985   8.619015 0.0024588\nLos Angeles-Houston           12.72   9.060985  16.379015 0.0000000\nSan Francisco-Houston         -0.96  -4.619015   2.699015 0.9499289\nWashington, DC-Houston         5.44   1.780985   9.099015 0.0006646\nSan Francisco-Los Angeles    -13.68 -17.339015 -10.020985 0.0000000\nWashington, DC-Los Angeles    -7.28 -10.939015  -3.620985 0.0000021\nWashington, DC-San Francisco   6.40   2.740985  10.059015 0.0000373\n\n\n\nBased on the output, we find similar results to the Bonferroni test. There are no differences in congestion levels between Houston and Atlanta, San Francisco and Atlanta, and Houston and San Francisco. There are group differences in congestion levels between all other remaining groups (p-value &lt; alpha).\nWith the TukeyHSD() command, we can also determine which group mean is higher than the other by looking at the “diff” score. If you see a negative score than the second group listed in the output is higher than the first group listed. Ignoring the insignificant p-values &gt; .05, we find that both San Francisco and Washington has significantly less congestion than Los Angeles. We find that Los Angeles and Washington have a significantly higher group mean than Atlanta. Los Angeles and Washington DC also have a significantly higher group mean than Houston. Finally, Washington DC has a significantly higher group mean than San Francisco.\n\n\n\n9.3.3.4.3 Planned Contrasts\n\nPlanned contrasts are computed by developing contrasts that specify which means to compare to which other means.\nA contrast refers to the set of numbers used to specify which means or groups of means to compare to each other, usually to identify statistically significant differences among means after a statistically significant analysis of variance.\nRules to consider:\n\nA contrast is a group of numbers used to group categories. The categories grouped together should all be represented by the same number in the contrast.\nThe numbers in the contrast should all add to zero.\nAny category not included in the contrast should be represented by a zero.\n\n\n\n# Look at the levels of the categorical variable\nlevels(congestionData$City)\n\n[1] \"Atlanta\"        \"Houston\"        \"Los Angeles\"    \"San Francisco\" \n[5] \"Washington, DC\"\n\n# Note order from levels Atlanta, Houston, LA, San Fran, DC Contrasts\n# need to sum to zero.  In the code below, I am making Atlanta one\n# level.  Zeroing out Houston and DC.  Combining LA and San Fran into\n# another level. This will allow me to compare Atlanta to the West\n# Coast.\ncontrast1 &lt;- c(-2, 0, 1, 1, 0)\n\n\nLink the contrast to the categorical variable using contrasts()\n\n\ncontrasts(congestionData$City) &lt;- contrast1\n# View the structure of the City variable with contrast\nstr(congestionData$City)\n\n Factor w/ 5 levels \"Atlanta\",\"Houston\",..: 3 3 3 3 3 3 3 3 3 3 ...\n - attr(*, \"contrasts\")= num [1:5, 1:4] -2 0 1 1 0 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"Atlanta\" \"Houston\" \"Los Angeles\" \"San Francisco\" ...\n  .. ..$ : NULL\n\n\n\nRe-run the model using aov() command\n\n\nAnova1way &lt;- aov(CongestionRating ~ City, data = congestionData)\nsummary.aov(Anova1way, split = list(City = list(`Atl vs. West Coast` = 1)))\n\n                            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nCity                         4   3251   812.7   37.25  &lt; 2e-16 ***\n  City: Atl vs. West Coast   1    486   486.0   22.28 6.45e-06 ***\nResiduals                  120   2618    21.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe output suggests that there is a difference in ATL vs West Coast cities (Los Angeles and San Francisco) in regard to congestion levels. We can tell that because of the p-value of 6.45e-06 *** which is less than .01.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#two-way-anova",
    "href": "anova.html#two-way-anova",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.4 Two-Way ANOVA",
    "text": "9.4 Two-Way ANOVA\n\nA two-way ANOVA is called a randomized block design.\nThe term “block” refers to a matched set of observations across the treatments.\nIn a two-way ANOVA, there are now three sources of variation:\n\nRow variability (due to blocks or Factor B or a first grouping variable),\nColumn variability (due to treatments or Factor A or a second grouping variable), and,\nVariability due to chance or SSE.\n\nIn altering the methodology above, the row and column variability take place of the between-treatment estimate listed above, which still corresponds to explained variance. We still have one within-treatment estimate as before, or a measure of unexplained variance.\n\n\n9.4.1 Steps for Conducting a Two-Way ANOVA\n\nWrite the null and alternate hypotheses for each grouping variable.\nCompute the F-test statistics for each grouping variable.\nCompute the probability for the test statistics (2 p-values for a 2-way ANOVA).\nInterpret the probabilities.\nWrite a conclusion.\nIf model is significant, run post-hoc tests.\n\n\nThe steps are the same as a one-way ANOVA listed above, with the exception that now we have 2 grouping variables, so we need 2 F-stats and 2 p-values.\n\n\n\n9.4.2 Statistics in a Two-Way ANOVA Table\n\nWe add another row to our ANOVA table when we move from a one-way ANOVA to a two-way ANOVA. The goal is the same, which is to produce a F-test statistic. In a two-way ANOVA, we will have two F-test statistics (one for each grouping variable). Each F-test statistic corresponds to a separate p-value that we can interpret the same way as we did in the last lessons (p-value &lt; alpha = significant result - reject the \\(H_0\\)). So we can find one group significant and not the other, neither significant, or both significant with one test.\n\n\n\n\nTwo Way ANOVA Formulas\n\n\n\n9.4.2.1 Explained Variance\n\nIn a two-way ANOVA, we still start by computing the explained variance. Since we have an additional grouping variable, or factor, we will have two sum of squares measures, a sum of squares for factor A (SSA) and a sum of squares for factor B (SSB). Factor B is typically called a row (r) factor, while Factor A is typically called a column (c) factor. However, the order of the output will depend on the order of your formula in R.\nThe explained variance still suggests that the variation in outcome can be explained by a model, but now it will be due to the two grouping variables. In computing the SSB, we take the number of columns (\\(c\\)) and we multiply by the sum of the squared deviation between each group B mean (\\(\\bar{y_j}\\)) and the grand mean \\(\\bar{y}\\).\n\n\\(SSB = c*\\sum^r_{j=1}{(\\bar{y_j}-\\bar{y}) ^2}\\)\n\nIn computing SSA, we take the number of rows and we multiply that number by the sum of the squared deviation between each group A mean and the grand mean\n\n\\(SSA =r*\\sum^c_{i=1}{(\\bar{y_i}-\\bar{y}) ^2}\\)\n\nDegrees of freedom for the SSB are computed by the number of rows - 1 \\((r-1)\\), and the degrees of freedom for SSA are computed as the number of columns - 1 \\((c-1)\\).\n\n\\(df_{ssb} = r-1\\)\n\\(df_{ssa} = c-1\\)\n\nThe mean square due to Factor B (MSB) and Factor A (MSA) both take the value from the appropriate sum of squares (SSB and SSA) and divides that factors degrees of freedom \\((r-1)\\) or \\((c-1)\\).\n\n\\(MSB = SSB/(r-1)\\)\n\\(MSA = SSA/(c-1)\\)\n\n\n\n\n9.4.2.2 Unexplained Variance\n\nIn a two-way ANOVA, there is still only one measure of unexplained variance, which we term residual error.\nThe SSE is calculated after deriving the total variance (SST = both explained and unexplained), and subtracting the explained variance (SSB and SSA).\n\n\\(SSE = SST - (SSA + SSB)\\)\n\nDegrees of freedom for the SSE are computed by taking the total number of observations (\\(n_t\\)) and subtracting the number of rows (r) and the number of columns (c) and adding 1 \\((n_t-c-r+1)\\).\n\n\\(df_{sse} = n_t-c-r+1\\)\n\nThe mean square due to error (MSE) takes the SSE and divides by the appropriate degrees of freedom \\((n_t-c-r+1)\\).\n\n\\(MSE = SSE/(n_t-c-r+1)\\)\n\nThere is a total sum of squares (SST) being calculated mentioned above, which again refers to the total variance (explained and unexplained). To get this calculation, we take the squared deviation between each observation and the grand mean and we sum up all of our values.\nThere is also a total degrees of freedom in a two-way ANOVA, which is the total number of observations minus 1 (\\(n_t-1\\)). We don’t see it explicitly in our R output, but it is needed to help understand the full model.\n\n\n\n9.4.2.3 Calculating F-test Statistics\n\nFinally, to calculate the 2 F-test statistics, given a two-way ANOVA, we take both the MSB (explained variability for factor B) and the MSA (explained variability for factor A) and divide both by the MSE (unexplained variability) considering the appropriate degrees of freedom (\\(c-1\\) and \\(r-1\\) and \\(n_t-c-r+1\\)).\nThis gives us two scores to evaluate. A p-value is computed from each F-test statistic mathematically by using the correct command in R.\n\n\\(F1_{c-1,n_t-c-r+1 }= MSB/MSE\\)\n\\(F2_{r-1,n_t-c-r+1 }= MSA/MSE\\)\n\n\n\n\n\n9.4.3 Example of a Two-way ANOVA\n\nLet’s run an example to compare SAT scores to see if they are comparable from different instructors {Instructor 1, 2, and 3} and across 4 races {Asian.American, Black, Mexican.American, and White}.\n\n\n9.4.3.1 Step 1: Set Up Null and Alternative Hypothesis\n\nThe competing hypotheses for the two-way ANOVA are two-fold:\nHypothesis 1:\n\n\\(H_0: \\mu_{Instructor1} = \\mu_{Instructor2}= \\mu_{Instructor3}\\)\n\\(H_A:\\) Not all population means of SAT scores are equal among the instructors\n\nHypothesis 2:\n\n\\(H_0: \\mu_{Asian.American} = \\mu_{Black} = \\mu_{Mexical.American} =  \\mu_{White}\\)\n\\(H_A:\\) Not all population means of SAT scores are equal among races\n\n\n\n\n9.4.3.2 Steps 2 and 3: Compute the F-test Statistics and p-Values\n\nLike a one-way ANOVA, in a two-way ANOVA, we start with an appropriate data set.\n\nAgain, because we are using read.csv() command, we can set stringsAsFactors argument to TRUE so that the categorical variable will be coded appropriately upon download.\nThe summary() command suggests that there are two categorical variables, “Instructor” and “Race”, and one continuous variable, “SAT.”\n\n\nSATdata &lt;- read.csv(\"data/SAT.csv\", stringsAsFactors = TRUE)\nsummary(SATdata)\n\n        Instructor               Race         SAT      \n Instructor A:40   Asian.American  :30   Min.   :1190  \n Instructor B:40   Black           :30   1st Qu.:1328  \n Instructor C:40   Mexican.American:30   Median :1492  \n                   White           :30   Mean   :1468  \n                                         3rd Qu.:1592  \n                                         Max.   :1772  \n\n\n\n\n\n9.4.3.3 Steps 4 and 5: Interpret the Probabilities and Write a Conclusion\n\nAnova2way &lt;- aov(SAT ~ Instructor + Race, data = SATdata)\nsummary(Anova2way)\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nInstructor    2    5932    2966   1.084  0.342    \nRace          3 2353300  784433 286.624 &lt;2e-16 ***\nResiduals   114  311995    2737                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nIn the ANOVA results, we find that there was no difference in SAT score based on the Instructor of record. Specifically, we find a F-test statistic of 1.084 and an associated p-value at .342. That p-value is &gt; alpha (.05), and therefore, we fail to reject the null hypothesis \\((H_0:  \\mu_{Instructor1} = \\mu_{Instructor2}= \\mu_{Instructor3})\\).\nWe also find that Race is significant, with a F-test statistic of 286.624 and an associated p-value of &lt;2e-16 ***. This means that we can reject the null hypothesis and support the alternative hypothesis (\\(H_A:\\) Not all population means of SAT scores are equal among races).\nThis means that we can conduct post-hoc tests on our Race variable. If we conduct any post-hoc tests on Instructor, we should find no differences in groups (which we already found in the ANOVA results above). This means additional testing the Instructor variable is an unnecessary step.\n\n\n\n9.4.3.4 Step 6: If Model is Significant, Run Post-Hoc Tests\n\nUsing the pairwise.t.test() command, we can run a Bonferroni test on just race by using the command below. This result suggest that there are group differences between all groups. We can tell that because all p-values are less than alpha at .05.\n\n\n\npairwise.t.test(SATdata$SAT, SATdata$Race, p.adj = \"bonf\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  SATdata$SAT and SATdata$Race \n\n                 Asian.American Black   Mexican.American\nBlack            &lt; 2e-16        -       -               \nMexican.American &lt; 2e-16        4.9e-09 -               \nWhite            0.0044         &lt; 2e-16 &lt; 2e-16         \n\nP value adjustment method: bonferroni \n\n\n\nUsing the TukeyHSD() Method, we do receive output for both Race and Instructor. As expected, all p-values under the instructor grouping are well above an alpha of .05, meaning not significant (Fail to reject \\(H_0\\)).\nReceiving similar results to the Bonferroni test with regards to Race, we find all group differences significant. Specifically, Black Americans, Mexican Americans, and White Americans scored lower on their SAT than Asian Americans. Mexican Americans and White Americans scored higher on their SAT than Black Americans. White Americans scored higher than Mexican Americans. Using this command, we can tell which group scored lower than the other by looking at the difference score “diff”.\n\n\nTukeyHSD(Anova2way)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = SAT ~ Instructor + Race, data = SATdata)\n\n$Instructor\n                             diff       lwr      upr     p adj\nInstructor B-Instructor A  -0.275 -28.05409 27.50409 0.9996954\nInstructor C-Instructor A -15.050 -42.82909 12.72909 0.4056475\nInstructor C-Instructor B -14.775 -42.55409 13.00409 0.4189723\n\n$Race\n                                      diff        lwr        upr     p adj\nBlack-Asian.American            -339.33333 -374.55195 -304.11472 0.0000000\nMexican.American-Asian.American -248.86667 -284.08528 -213.64805 0.0000000\nWhite-Asian.American             -46.90000  -82.11862  -11.68138 0.0040188\nMexican.American-Black            90.46667   55.24805  125.68528 0.0000000\nWhite-Black                      292.43333  257.21472  327.65195 0.0000000\nWhite-Mexican.American           201.96667  166.74805  237.18528 0.0000000",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#choose-command",
    "href": "anova.html#choose-command",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.5 choose() command",
    "text": "9.5 choose() command\n\nThe Combination function counts the number of ways to choose objects from a total of objects. The order in which the objects are listed does not matter.\nSince repetition is not allowed, we use \\(C(n,x) = (n)!/((n-x!)*x!)\\)\nIn post hoc tests, x is always 2.\nyou can calculate the factorial of a number using the factorial function in base R.\n\n\nn &lt;- 4\n# C(4,2) = 4!/2!*(4-2)! manually\n(4 * 3 * 2 * 1)/((2 * 1) * (2 * 1))\n\n[1] 6\n\n# with the factorial function\nfactorial(4)/(factorial(4 - 2) * factorial(2))\n\n[1] 6\n\n\n\nA quick check on how many pairs we can expect in our post-hoc output, we can use the choose() command. We always only check 2 groups at a time, so the 2nd argument should remain a 2.\nIf we have 4 races, we would use 4,2 inside the command as shown below. This means we should see 6 p-value statistics for 6 pairwise tests.\n\n\n\n# with choose function\nchoose(4, 2)\n\n[1] 6\n\n\n\nWe had 3 instructors, so we would use 3,2 inside the command as shown below. If this variable was significant, we should see 3 p-value statistics for 3 pairwise tests.\n\n\nchoose(3, 2)\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "anova.html#summary",
    "href": "anova.html#summary",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.7 Summary",
    "text": "9.7 Summary\n\nIn this section, we learned about Type I and Type II error and the importance of reducing both. We also learned about a one-way ANOVA and a two-way ANOVA, including how to conduct them, and when to evaluate post-hoc tests.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "correlation.html#interpreting-the-direction-of-the-correlation",
    "href": "correlation.html#interpreting-the-direction-of-the-correlation",
    "title": "10  Correlation Analysis",
    "section": "10.3 Interpreting the Direction of the Correlation",
    "text": "10.3 Interpreting the Direction of the Correlation\n\nNegative correlations occur when one variable goes up and the other goes down.\nNo correlation happens when there is no discernible pattern in how two variables vary.\nPositive correlations occur when one variable goes up, and the other one also goes up (or when one goes down, the other one does too); both variables move together in the same direction.\n\n\n\n\nCorrelation Scatterplots\n\n\n\n10.3.1 Scatterplots to Visualize Relationship\n\nLet’s do an example to first visualize the data, and then to calculate the correlation coefficient.\nFirst, read in a .csv called DebtPayments.csv. This data set has 26 observations and 4 variables:\n\nA character variable with a bunch of metropolitan areas listed;\nAn integer numeric debt;\nA numeric variable Income;\nA numeric variable Unemployment.\n\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nstr(Debt_Payments)\n\n'data.frame':   26 obs. of  4 variables:\n $ Metropolitan.area: chr  \"Washington, D.C.\" \"Seattle\" \"Baltimore\" \"Boston\" ...\n $ Debt             : int  1285 1135 1133 1133 1104 1098 1076 1045 1024 1017 ...\n $ Income           : num  103.5 81.7 82.2 89.5 75.9 ...\n $ Unemployment     : num  6.3 8.5 8.1 7.6 8.1 9.3 10.6 12.4 12.9 9.7 ...\n\n\n\nNext, plot the relationship between 2 continuous variables.\n\nThere are a few ways to write the plot command using ggplot. We went over these in the Data Visualization lesson. Again we said:\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs() command (for labels) or stat_smooth() command to generate a regression line.\n\n\nDebt_Payments %&gt;%\n    ggplot(aes(Income, Debt)) + geom_point(color = \"#183028\", shape = 2) +\n    stat_smooth(method = \"lm\", color = \"#789F90\") + theme_minimal()\n\n\n\n\n\n\n\n\nIn the above plot, there is a strong positive relationship (upward trend) that should be confirmed with a correlation test.\nIn a second example below, we look at Unemployment as the X variable. This scatterplot is much more difficult to use in determining whether the correlation will be significant. It looks negative, but there is not a strong linear trend to the data. This will also need to be confirmed with a correlation test.\n\n\nDebt_Payments %&gt;%\n    ggplot(aes(Unemployment, Debt)) + geom_point(color = \"#183028\", shape = 2) +\n    stat_smooth(method = \"lm\", color = \"#789F90\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nIn many scatterplots using big data, the observations are too numerous to see a good relationship. In that case, the statistical test can trump this visual aid. However, in a lot of cases the scatterplot does help visualize the relationship between 2 continuous variables.\n\n\n\n10.3.2 Interpreting the Strength of the Correlation\n\nStatisticians differ on what is called a strong correlation versus weak correlation, and it depends on the context. A .9 may be required for a strong correlation in one field, and a .5 in another. Generally speaking in business, the absolute value of a correlation .8 or above is considered strong, between .5 and .8 is considered moderate, and between a .2 and .5 is considered weak.\nThe following is consistent with what is most generally used:\n\n\n\n\nCorrelation Strength",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#hypothesis-testing-with-correlations",
    "href": "correlation.html#hypothesis-testing-with-correlations",
    "title": "10  Correlation Analysis",
    "section": "10.4 Hypothesis Testing with Correlations",
    "text": "10.4 Hypothesis Testing with Correlations\n\nCorrelation values should be tested alongside a p-value to confirm whether or not there is a correlation. The null is tested using a t-distribution specifically testing whether \\(r = 0\\) or not, like the one-sample t-test section from the lesson 6.\nThe null and alternative are listed below.\n\n\\(H_0\\): There is no relationship between the two variables (\\(r = 0\\)).\n\\(H_A\\): There is a relationship between the two variables (\\(r \\neq 0\\)).\n\n\n\n10.4.1 cor.test() Command\n\nThe cor() command gives you just the correlation coefficient. This command can be useful if you are testing many correlations at one time. In the below statement, I can use \\(cor(Variable1, Variable2)\\) to see the correlation between 2 continuous variables.\n\n\ncor(Debt_Payments$Income, Debt_Payments$Debt)\n\n[1] 0.8675115\n\n\n\nThe cor.test() command tests the hypothesis whether \\(r=0\\) or not. This command comes with a p-value and t-test statistic (along with the correlation coefficient).\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Debt)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Debt\nt = 8.544, df = 24, p-value = 9.66e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7231671 0.9392464\nsample estimates:\n      cor \n0.8675115 \n\n\n\nThis test shows a strong positive correlation of .8675 (&gt;.8) which is significant. Our p-value is 9.66e-09 or &lt; .001 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between .723 and .939.\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Unemployment)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Unemployment\nt = -3.0965, df = 24, p-value = 0.004928\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7636089 -0.1852883\nsample estimates:\n       cor \n-0.5342931 \n\n\n\nThis test shows a moderate negative correlation of -.534 (&lt;.5 and .8) which is significant. Our p-value is 0.004928 or &lt; .01 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between -.765 and -.185. This confidence interval is wider than the one listed above. This is due to the noise in the relationship we noted in the scatterplot - the correlation is weaker, the relationship does not look as linear, the confidence decreases. Even though this is true, we must note that we still found a significant correlation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#limitations-of-correlation-analysis",
    "href": "correlation.html#limitations-of-correlation-analysis",
    "title": "10  Correlation Analysis",
    "section": "10.6 Limitations of Correlation Analysis",
    "text": "10.6 Limitations of Correlation Analysis\n\nTo determine a causal model, you need the following:\n\n\nSignificant Correlation: Statistically significant relationship between the variables.\nTemporal Precedence: Causal variable occurred prior to the other variable.\nEliminate Alternate Variables: No other factors can account for the cause.\n\n\nSome limitations are as follows:\n\nThe correlation coefficient captures only a linear relationship.\nThe correlation coefficient may not be a reliable measure in the presence of outliers.\nEven if two variables are highly correlated, one does not necessarily cause the other.\n\nNote that a correlation is the first step in understanding causality.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#summary",
    "href": "correlation.html#summary",
    "title": "10  Correlation Analysis",
    "section": "10.8 Summary",
    "text": "10.8 Summary\n\nIn this lesson, we learned about correlation coefficient and how to evaluate a strong, moderate, or weak/positive or negative correlation. We also learned how to visualize a relationship using a scatterplot. We also learned how to test for a correlation being significant or not.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#the-linear-regression-model",
    "href": "regression.html#the-linear-regression-model",
    "title": "11  Linear Regression Analysis",
    "section": "11.1 The Linear Regression Model",
    "text": "11.1 The Linear Regression Model\n\n11.1.1 Overview of the Linear Regression\n\nLinear regression analysis is a predictive modelling technique used for building mathematical and statistical models that characterize relationships between a dependent (continuous) variable and one or more independent, or explanatory variables (continuous or categorical), all of which are numerical.\nThis technique is useful in forecasting, time series modelling, and finding the causal effect between variables.\nSimple linear regression involves one explanatory variable and one response variable.\n\nExplanatory variable: The variable used to explain the dependent variable, usually denoted by X. Also known as an independent variable or a predictor variable.\nResponse variable: The variable we wish to explain, usually denoted by Y. Also known as a dependent variable or outcome variable.\n\nMultiple linear regression involves two or more explanatory variables, while still only one response variable.\n\n\n\n11.1.2 Estimating Using a Simple Linear Regression Model\n\nWhile the correlation coefficient may establish a linear relationship, it does not suggest that one variable causes the other.\nWith regression analysis, we explicitly assume that the response variable is influenced by other explanatory variables.\nUsing regression analysis, we may predict the response variable given values for our explanatory variables.\n\n\n\n11.1.3 Inexact Relationships\n\nIf the value of the response variable is uniquely determined by the values of the explanatory variables, we say that the relationship is deterministic.\n\nBut if, as we find in most fields of research, that the relationship is inexact due to omission of relevant factors, we say that the relationship is inexact.\nIn regression analysis, we include a stochastic error term, that acknowledges that the actual relationship between the response and explanatory variables is not deterministic.\n\n\n\n\n11.1.4 Regression as ANOVA\n\nANOVA conducts an F - test to determine whether variation in Y is due to varying levels of X.\nANOVA is used to test for significance of regression:\n\n\\(H_0\\): population slope coefficient \\(\\beta_1\\) \\(=\\) 0\n\\(H_A\\): population slope coefficient \\(\\beta_1\\) \\(\\neq\\) 0\n\nR reports the p-value (Significant F).\nRejecting \\(H_0\\) indicates that X explains variation in Y.\n\n\n\n\nLinear Regression Models\n\n\n\n\n11.1.5 The Simple Linear Regression Model\n\nThe simple linear regression model is defined as \\(y= \\beta_0+\\beta_1 𝑥+\\varepsilon_1\\), where \\(y\\) and \\(x\\) are the response and explanatory variables, respectively and \\(\\varepsilon_1\\) is the random error term. \\(\\beta_0\\) and \\(\\beta_1\\) are the unknown parameters to be estimated.\nSometimes, this equation can be represented using different variable names like \\(y=mx+b\\) This is the same equation as above, but different notation.\nBy fitting our data to the model, we obtain the equation \\(\\hat{y} = b_0 + b_1*x\\), where \\(\\hat{y}\\) is the estimated response variable, \\(b_0\\) is the estimate of \\(\\beta_0\\) (Intercept) and \\(b_1\\) is the estimate of \\(\\beta_1\\) (Slope).\nSince the predictions cannot be totally accurate, the difference between the predicted and actual value represents the residual \\(e=y-\\hat{y}\\).\n\n\n\n\nRegression Equation\n\n\n\n\n11.1.6 The Least Squares Estimates\n\nThe two parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated by minimizing the sum of squared residuals.\n\nThe slope coefficient \\(\\beta_1\\) is estimated as \\(b_1 = \\sum(x_i-\\bar{x}*y_i-\\bar{y})/\\sum(x_i-\\bar{x})^2\\)\nThe intercept parameter \\(\\beta_0\\) is estimated as \\(b_0 = \\hat{y}-b_1*\\bar{x}\\)\nAnd we use this information to make the regression equation given the formula above: \\(\\hat{y} = b_0 + b_1*x\\),\n\n\n\n\n11.1.7 Goodness of Fit\n\nGoodness of fit refers to how well the data fit the regression line. I will introduce three measures to judge how well the sample regression fits the data.\n\nStandard Error of the Estimate\nThe Coefficient of Determination (\\(R^2\\))\nThe Adjusted \\(R^2\\)\n\nIn order to make sense of the goodness of fit measures, we need to go back to the idea of explained and unexplained variance we learned in Chapter 7. Variance can also be known as a difference.\n\nUnexplained Variance = SSE or Sum of Squares Error: This equals the sum of squared difference between A and B, or between the sum of squares of the difference between observation (\\(y_i\\)) and our predicted value of y (\\(\\hat{y}\\)).\n\n\\(SSE = \\sum^n_{i=1}(y_i - \\hat{y})^2\\)\n\nExplained Variance = SSR or Sum of Squares Regression: This equals the sum of squared difference between B and C, or between the sum of squares of the difference between our predicted value (\\(\\hat{y}\\)) and the mean of y (\\(\\bar{y}\\)).\n\n\\(SSR = \\sum^n_{i=1}(\\hat{y} - \\bar{y})^2\\)\n\nTotal Variance = SST or Total Sum of Squares: This equals the sum of squared difference between A and C, or between the sum of squares of the difference between observation (\\(y_i\\)) and the mean of y (\\(\\bar{y}\\)).\n\nThe SST can be broken down into two components: the variation explained by the regression equation (the regression sum of squares or SSR) and the unexplained variation (the error sum of squares or SSE).\n\\(SST = \\sum^n_{i=1}(y_i - \\bar{y})^2\\)\n\n\n\n\n\nVisualization of Sum of Squares (SS)\n\n\n\n\n11.1.8 The Standard Error of the Estimate\n\nThe Standard Error of the Estimate, also known as the Residual Standard Error (RSE) (and labelled such in R), is the variability between observed (\\(y_i\\)) and predicted (\\(\\hat{y}\\)) values, targeting the unexplained variance in the figure above. This measure is a lack of fit of the model to the data.\nTo compute the standard error of the estimate, we first compute the SSE and the MSE.\n\nSum of Squares Error: \\(SSE = \\sum^n_{i=1}e^2_i = \\sum^n_{i=1}(y_i - \\hat{y})^2\\)\nDividing SSE by the appropriate degrees of freedom, n – k – 1, yields the mean squared error:\n\nMean Squared Error: \\(MSE = SSE/(n-k-1)\\)\n\nThe square root of the MSE is the standard error of the estimate, se.\n\nStandard Error of Estimate: \\(se = \\sqrt(MSE) = \\sqrt(SSE/(n-k-1))\\)\n\n\n\n\n11.1.8.1 R Output\n\nWe do not see the SSE or the SST in our R output, but we will see the Standard Error of the Estimate labelled Residual Standard Error. We will look at all these measures together in R output later on in the lecture.\n\n\n\n\n11.1.9 The Coefficient of Determination (\\(R^2\\))\n\nCoefficient of determination refers to the percentage of variance in one variable that is accounted for by another variable or by a group of variables.\nThis measure of R-squared is a measure of the “fit” of the line to the data.\n\\(R^2\\) = 1-SSE/SST where \\(SSE = \\sum^n_{i=1}(y_i - \\hat{y})^2\\) and \\(SST = \\sum^n_{i=1}(y_i - \\bar{y})^2\\)\n\n\\(R^2\\) can also be computed by SSR/SST, which provides the same answer as above.\nThis measure of fit targets both the unexplained and the explained variance.\n\\(R^2\\) can also be calculated using the formula for Pearson’s r and squaring it. This gives us the same answer.\n\nPearson’s \\(r\\) is a statistic that indicates the strength and direction of the relationship between two numeric variables that meet certain assumptions.\n\n(Pearson’s \\(r\\))\\(^2\\) = \\(r^2_{xy} = (cov_{xy}/(s_x*s_y))^2\\).\n\nYou need to see how all these formulas relate to see why all these formulas give you the same answer.\n\n\n11.1.9.1 Interpreting (\\(R^2\\))\n\nThis measure is easier to interpret over standard error. In particular, the (\\(R^2\\)) quantifies the fraction of variation in the response variable that is explained by changes in the explanatory variables.\nThe (\\(R^2\\)) gives you a score between 0 and 1. A score of 1 indicates that your Y variable is perfectly explained by your X variable or variables (where we only have one in simple regression).\nThe \\(R^2\\) measure has strength of determination like the correlation coefficient, only all measures are from 0 to 1.\n\nThe closer you get to one, the more explained variance we achieve.\nA score of 0 indicates that no variance is explained by the X variable or variables. In this case, the variable would not be a significant predictor variable of Y.\nAgain, the strength of the relationship varies by field, but generally, a .2 to a .5 is considered weak, a .5 to a .8 is considered moderate, and above a .8 is considered strong.\n\n\n\n\n11.1.9.2 R Output\n\nWe do see the \\(R^2\\) in our R output labelled Multiple R-squared.\n\n\n\n11.1.9.3 Limitations of The Coefficient of Determination (\\(R^2\\))\n\nMore explanatory variables always result in a higher \\(R^2\\).\nBut some of these variables may be unimportant and should not be in the model.\nThis is only applicable with multiple regression, discussed below.\n\n\n\n\n11.1.10 The Adjusted \\(R^2\\)\n\nThe Adjusted \\(R^2\\) tries to balance the raw explanatory power against the desire to include only important predictors.\n\nAdjusted \\(R^2 = 1-(1-R^2)*((n-1)/(n-k-1))\\).\nThe Adjusted \\(R^2\\) penalizes the \\(R^2\\) for adding additional explanatory variables.\n\n\n\n11.1.10.1 R Output\n\nWith our other goodness-of-fit measures, we typically allow the computer to compute the Adjusted \\(R^2\\) using commands in R.\n\nTherefore, we do see the Adjusted \\(R^2\\) in our R output labelled Adjusted R-squared.\n\n\n\n\n11.1.11 Example of Simple Linear Regression in R\n\nThe broad steps are the same as we used in Chapter 6 and 7 when setting up the t-tests and ANOVA hypothesis: 1. set up the hypothesis 2. compute the Test Statistic, 3. calculate probability 4. interpret and 5. write a conclusion.\nStep 1: Set Up the Null and Alternative Hypothesis\n\\(H_0\\): The slope of the line is equal to zero.\n\\(H_A\\): The slope of the line is not equal to zero.\nStep 2 and 3: Compute the Test Statistic and Calculate Probability\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nSimple &lt;- lm(Debt ~ Income, data = Debt_Payments)\nsummary(Simple)\n\n\nCall:\nlm(formula = Debt ~ Income, data = Debt_Payments)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-107.087  -38.767   -5.828   50.137  101.619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  210.298     91.339   2.302   0.0303 *  \nIncome        10.441      1.222   8.544 9.66e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 63.26 on 24 degrees of freedom\nMultiple R-squared:  0.7526,    Adjusted R-squared:  0.7423 \nF-statistic:    73 on 1 and 24 DF,  p-value: 9.66e-09\n\n\n\nSteps 4 and 5: Interpret the Probability and Write a Conclusion\nWe interpret the probability and write a conclusion after looking at the following:\n\n\nGoodness of Fit Measures\nThe F-test statistic\nThe p-value from the hypothesis\n\n\nWe then can interpret the hypothesis and make the regression equation if needed to make predictions.\n\n\n11.1.11.1 Examining the Goodness of Fit Measures\n\nLike the ANOVA, we need the summary() command to see the regression output.\nThis output includes the following:\n\nStandard Error of Estimate at 63.26\na \\(R^2\\) of 0.7526.\nan Adjusted \\(R^2\\) of 0.7423\n\nWe do not really see a difference in \\(R^2\\) and the Adjusted \\(R^2\\) because we only have a simple linear regression, which includes one X. The other thing that could create a difference between these measures is having a small sample size. A small sample size could adjust the \\(R^2\\) down (like it did a little here \\(n=28\\)). In both cases, a score around .75 indicates that we are explaining about 75% of the variance in Debt Payments, which is specifically stated as 75.26% in the \\(R^2\\) value.\nThe Standard Error of Estimate - known in R as the RSE or Residual Standard Error at 63.26. Before we interpret this, we need to know the scale, and income is in 1000s. Therefore, for every 1000 dollars difference in Income we could be off on our debt payments by 63.26 dollars.\nAgain, most statisticians interpret the \\(R^2\\) and the adjusted \\(R^2\\) over the RSE.\n\n\n\n11.1.11.2 Examining the F-Test Statistic\n\nThe F-statistic is a ratio of explained information (in the numerator) to unexplained information (in the denominator). If a model explains more than it leaves unexplained, the numerator is larger and the F-statistic is greater than 1. F-statistics that are much greater than 1 are explaining much more of the variation in the outcome than they leave unexplained. Large F-statistics are more likely to be statistically significant.\nWe see a F-statistic in our output with a p-value of 9.66e-09, which is less than .05. This indicates that our overall model is significant, which in simple regression means that our one X predictor variable is also significant. If this F-statistic is significant, we can interpret the hypothesis.\n\n\n\n11.1.11.3 Examining the Hypothesis\n\nWe see a p-value on the Income row of 9.66e-09 from a t-value. This is also significant at .05 level. This p-value is the same as the F-test statistic because we only have one X variable.\nThis significance means our predictor variable does influence the Y variable and that we can reject the null hypothesis and show support for our alternative hypothesis.\n\n\n\n11.1.11.4 Interpreting the Hypothesis\n\nA \\(b_1\\) estimate of 10.441 indicates that for 1000 dollars of Income (again our data is in 1000s), the payment of dept will increase by 10.441.\nA \\(b_0\\) estimate of 210.298 indicates where the regression line will start given a Y at 0.\n\nThis gives us a regression equation at \\(\\hat{y} = 210.298 + 10.441*Income\\)\nWe can graph this regression line using the abline() command on our plot we did earlier.\nAs you can see from the chart, there was no data at the Y intercept, but if I extend the chart out, it does hit the y axis at 210 like stated.\nAlso from the chart, we note that for each unit of income we go to the right, we go up by 10.441 units in dept payments.\n\n\n\nplot(Debt ~ Income, data = Debt_Payments, xlim = c(0, 120), ylim = c(0,\n    1350))\nabline(Simple)\n\n\n\n\n\n\n\n\n\n\n\n11.1.12 Predictions\n\nWe can calculate this using the regression equation or use the predict() command in order to calculate different values of y given values of x based on our regression equation we got in the step above.\nThe coef() function returns the model’s coefficients which are needed to make the regression equation.\n\n\n# What would be your debt payments if Income was 100 ( for 100,000)\ncoef(Simple)\n\n(Intercept)      Income \n  210.29768    10.44111 \n\n210.298 + 10.441 * (100)\n\n[1] 1254.398\n\npredict(Simple, data.frame(Income = 100))\n\n       1 \n1254.408",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#multiple-regression",
    "href": "regression.html#multiple-regression",
    "title": "11  Linear Regression Analysis",
    "section": "11.2 Multiple Regression",
    "text": "11.2 Multiple Regression\n\nMultiple regression allows us to explore how several explanatory variables influence the response variable.\nSuppose there are \\(k\\) explanatory variables. The multiple linear regression model is defined as: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ ⋯ + \\beta_kx_k +\\varepsilon\\)\nWhere:\n\n\\(y\\) is the dependent variable,\n\\(x_1, ⋯, x_k\\) are independent explanatory variables,\n\\(\\beta_0\\) is the intercept term,\n\\(\\beta_1 ⋯, \\beta_k\\) are the regression coefficients for the independent variables,\n\\(varepsilon\\) is the random error term\n\nWe estimate the regression coefficients—called partial regression coefficients — \\(b_0, b_1, b_2,… b_k\\), then use the model: \\(\\hat{y} = b_0 + b_1x_1 + b_2x_2 + \\ ... + b_kx_k\\) .\nThe partial regression coefficients represent the expected change in the dependent variable when the associated independent variable is increased by one unit while the values of all other independent variables are held constant.\nFor example, if there are \\(k = 3\\) explanatory variables, the value \\(b_1\\) estimates how a change in \\(x_1\\) will influence \\(y\\) assuming \\(x_2\\) and \\(x_3\\) are held constant.\n\n\n11.2.1 Example of Multiple Linear Regression in R\n\nThe steps to multiple linear regression are the same as simple linear regression except we should have more than one hypothesis.\n\n\nStep 1: Set up the null and alternative hypotheses\n\n\nHypothesis 1: Income affects Debt Payments.\n\n\\(H_0\\): The slope of the line in regards to Income is equal to zero.\n\\(H_A\\): The slope of the line in regards to Income is not equal to zero.\n\nHypothesis 2: Unemployment affects Debt Payments.\n\n\\(H_0\\): The slope of the line in regards to Unemployment is equal to zero.\n\\(H_A\\): The slope of the line in regards to Unemployment is not equal to zero.\n\n\n\nStep 2 and 3: Compute the test statistic and calculate probability\n\n\nDebt_Payments &lt;- read.csv(\"data/DebtPayments.csv\")\nMultiple &lt;- lm(Debt ~ Income + Unemployment, data = Debt_Payments)\nsummary(Multiple)\n\n\nCall:\nlm(formula = Debt ~ Income + Unemployment, data = Debt_Payments)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.456  -38.454   -5.836   51.156  102.121 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  198.9956   156.3619   1.273    0.216    \nIncome        10.5122     1.4765   7.120 2.98e-07 ***\nUnemployment   0.6186     6.8679   0.090    0.929    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 64.61 on 23 degrees of freedom\nMultiple R-squared:  0.7527,    Adjusted R-squared:  0.7312 \nF-statistic:    35 on 2 and 23 DF,  p-value: 1.054e-07\n\n\n\nSteps 4 and 5: Interpret the probability and write a conclusion\n\n\nWe interpret the probability and write a conclusion after looking at the following:\n\nGoodness of Fit Measures\nThe F-test statistic\nThe p-values from the hypotheses\n\nWe then can interpret the hypotheses and make the regression equation if needed to make predictions.\n\n\n\n11.2.2 Examining the Goodness of Fit Measures\n\nLike the ANOVA, we need the summary() command to see the regression output.\nThis output includes the following:\n\nStandard error of estimate at 64.61 - was previously 63.26 with simple regression.\na \\(R^2\\) of 0.7527 - was previously 0.7526 with simple regression.\nan Adjusted \\(R^2\\) of 0.7312 - was previously 0.7423 with simple regression.\n\nLooking at the goodness of fit indices, they suggest that we are not explaining anything more by adding the unemployment variable over what we had with the Income variable. Our RSE and \\(R^2\\) are about the same, and our adjusted \\(R^2\\) has gone down - again paying the price for including an unnecessary variable.\n\n\n\n11.2.3 Examining the F-Test Statistic\n\nWe see a overall F-statistic in our output with a p-value of 1.054e-07, which is less than .05. This indicates that our overall model is significant - which in multiple regression means that at least one X predictor variable is significant. If this F-statistic is significant, we can interpret the hypotheses.\n\n\n\n11.2.4 Examining the Hypotheses\n\nHypothesis 1: The output shows a p-value on the Income row of 2.98e-07 from a t-value. This was significant in our simple regression model, and is still significant at .05 level here. This significance means our predictor variable does influence the Y variable and that we can reject the null hypothesis and show support for our alternative hypothesis.\nHypothesis 2: The output shows a p-value on the Unemployment row of 0.929 from a t-value. This is NOT significant at any appropriate level of alpha. This lack of significance means our predictor variable does NOT influence the Y variable and that we fail to reject the null hypothesis that there is a slope.\n\n\n\n11.2.5 Interpreting the Hypothesis\n\nBecause Unemployment is not significant, we should drop it from the model before creating the regression equation. The extra variable is only adding noise to our model and not adding anything useful in understanding debt payments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#mr-with-p-value-method",
    "href": "regression.html#mr-with-p-value-method",
    "title": "11  Linear Regression Analysis",
    "section": "11.3 MR with P-value Method",
    "text": "11.3 MR with P-value Method\n\nWhen you have a lot of predictors, it can be difficult to come up with a good model.\nA common criteria is to remove based on the highest p-value.\nRemove indicators until all variables are significant, and none are insignificant.\nAs we drop variables, we also want to keep an eye on the adjusted R-Squared to make sure it does not significantly decrease. It should stay about the same if we drop variables that are not helpful in understanding the model, and could improve because we are decreasing the number of variables being evaluated.\n\n\nlibrary(ISLR)\ndata(\"Credit\")\nstr(Credit)\n\n'data.frame':   400 obs. of  12 variables:\n $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n $ Gender   : Factor w/ 2 levels \" Male\",\"Female\": 1 2 1 2 1 1 2 1 2 2 ...\n $ Student  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 1 1 1 1 2 ...\n $ Married  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 2 1 1 1 1 2 ...\n $ Ethnicity: Factor w/ 3 levels \"African American\",..: 3 2 2 2 3 3 1 2 3 1 ...\n $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n\nlibrary(tidyverse)\n\n## Removing variables that are of no interest to the model based on\n## what we have learned so far.  The following lines removes the\n## categorical variables, Gender, Student, Married, and Ethnicity. It\n## also removed the ID because that is not a helpful variable.\n\nCredit &lt;- select(Credit, -Gender, -Student, -Married, -Ethnicity, -ID)\n\ncreditlm &lt;- lm(Balance ~ ., data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ ., data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.25 -113.15  -42.06   45.82  542.97 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -477.95809   55.06529  -8.680  &lt; 2e-16 ***\nIncome        -7.55804    0.38237 -19.766  &lt; 2e-16 ***\nLimit          0.12585    0.05304   2.373  0.01813 *  \nRating         2.06310    0.79426   2.598  0.00974 ** \nCards         11.59156    7.06670   1.640  0.10174    \nAge           -0.89240    0.47808  -1.867  0.06270 .  \nEducation      1.99828    2.59979   0.769  0.44257    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.6 on 393 degrees of freedom\nMultiple R-squared:  0.8782,    Adjusted R-squared:  0.8764 \nF-statistic: 472.5 on 6 and 393 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8782, Adjusted R-squared: 0.8764\n\n\nEducation has the highest p-value, so it is removed from analysis. We use the minus sign to remove the variable in the lm function.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-231.37 -113.46  -39.55   41.66  544.35 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -449.36101   40.57409 -11.075   &lt;2e-16 ***\nIncome        -7.56211    0.38214 -19.789   &lt;2e-16 ***\nLimit          0.12855    0.05289   2.430   0.0155 *  \nRating         2.02240    0.79208   2.553   0.0110 *  \nCards         11.55272    7.06285   1.636   0.1027    \nAge           -0.88832    0.47781  -1.859   0.0638 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.6 on 394 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8765 \nF-statistic: 567.4 on 5 and 394 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8781, Adjusted R-squared: 0.8765\n\n\nCards has the next highest p-value, so it is removed from analysis using the - minus sign format.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-249.62 -110.89  -39.98   51.87  546.52 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -445.10477   40.57635 -10.970  &lt; 2e-16 ***\nIncome        -7.61268    0.38169 -19.945  &lt; 2e-16 ***\nLimit          0.08183    0.04461   1.834   0.0674 .  \nRating         2.73142    0.66435   4.111 4.79e-05 ***\nAge           -0.85612    0.47841  -1.789   0.0743 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.9 on 395 degrees of freedom\nMultiple R-squared:  0.8772,    Adjusted R-squared:  0.876 \nF-statistic: 705.6 on 4 and 395 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8772, Adjusted R-squared: 0.876\n\n\nAge is next to be removed because it has the next highest p-value at .06270.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards - Age, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards - Age, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-260.93 -113.14  -36.27   49.35  554.23 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -489.72748   32.09892 -15.257  &lt; 2e-16 ***\nIncome        -7.71931    0.37806 -20.418  &lt; 2e-16 ***\nLimit          0.08467    0.04471   1.894    0.059 .  \nRating         2.69858    0.66594   4.052 6.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162.4 on 396 degrees of freedom\nMultiple R-squared:  0.8762,    Adjusted R-squared:  0.8753 \nF-statistic: 934.6 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8762, Adjusted R-squared: 0.8753\n\n\nFinally Limit is the next to be removed because it has the next highest (and still insignificant) p-value at .059.\nAge is next to be removed because it has the next highest p-value at .06270.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards - Age - Limit, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards - Age - Limit, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-278.57 -112.69  -36.21   57.92  575.24 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -534.81215   21.60270  -24.76   &lt;2e-16 ***\nIncome        -7.67212    0.37846  -20.27   &lt;2e-16 ***\nRating         3.94926    0.08621   45.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162.9 on 397 degrees of freedom\nMultiple R-squared:  0.8751,    Adjusted R-squared:  0.8745 \nF-statistic:  1391 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8751,Adjusted R-squared: 0.8745\n\n\nIn our original model with non-helpful insignificant predictors, we had an adjusted R-Squared at 0.8764. After removing insignificant predictors, we are only down to .8745. This means we can explain someones Balance best by understanding Income and Rating.\nThe interpretation is as follows:\nA one unit change in Income causes a negative 7.67 change in Balance.\nA one unit change in Rating causes a 3.949 change in Balance.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#multicollinerity",
    "href": "regression.html#multicollinerity",
    "title": "11  Linear Regression Analysis",
    "section": "11.4 Multicollinerity",
    "text": "11.4 Multicollinerity\n\nMulticollinearity refers to the situation in which two or more predictor variables are closely related to another.\nCollinearity can cause significant factors to be insignificant, or insignificant factors to seem significant. Both are problematic.\nIf sample correlation coefficient between 2 explanatory variables is more than .80 or less than -.80, we could have severe multicollinearity. Seemingly wrong signs of estimated regression coefficients may also indicate multicollinearity. A good remedy may be to simply drop one of the collinear variables if we can justify it as redundant.\nAlternatively, we could try to increase our sample size.\nAnother option would be to try to transform our variables so that they are no longer collinear.\nLast, especially if we are interested only in maintaining a high predictive power, it may make sense to do nothing.\n\n\ncor(Credit$Income, Credit$Rating)  ##close to .8, but still under\n\n[1] 0.7913776\n\n\n\nThe second way to tell if there is multicollinearity is to use the vif() command from the car package.\nThe vif() function in R is used to calculate the Variance Inflation Factor (VIF) for each predictor variable in a multiple regression model. The VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity among the predictors. High VIF values indicate high multicollinearity, which can affect the stability and interpretability of the regression coefficients.\nWe want all scores to be under 10 to indicate the absence of problematic multicollinerity in our model.\n\n\nlibrary(car)  ##need to have car package installed first. \n# Place regression model object in the function to see vif scores.\nvif(creditlm)\n\n Income  Rating \n2.67579 2.67579 \n\n\n\nWhile statisticians differ on what constitutes, the general consenses is that scores need to be under 5 or 10.\nBoth scores are under 10, so we do have an absence of multicollinerity in our final model.\n\nVIF = 1: No correlation between the predictor and the other predictors.\n1 &lt; VIF &lt; 5: Moderate correlation but not severe enough to require correction.\n5 &lt; VIF &lt; 10: High correlation, indicating potential multicollinearity problems.\nVIF &gt;= 10: High correlation, indicating multicollinearity problems.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#summary",
    "href": "regression.html#summary",
    "title": "11  Linear Regression Analysis",
    "section": "11.6 Summary",
    "text": "11.6 Summary\n\nIn this lesson, we learned how to write and interpret the simple linear regression model and the multiple linear regression model. We learned about goodness of fit measures and how to test hypotheses using regression by interpreting the summary output from the lm() command. Finally, we learned how to use regression with both continuous and categorical variables as predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#the-p-value-method",
    "href": "regression.html#the-p-value-method",
    "title": "11  Linear Regression Analysis",
    "section": "11.3 The P-value Method",
    "text": "11.3 The P-value Method\n\nWhen you have a lot of predictors, it can be difficult to come up with a good model.\nA common criteria is to remove based on the highest p-value.\nRemove indicators until all variables are significant, and none are insignificant.\nAs we drop variables, we also want to keep an eye on the adjusted R-Squared to make sure it does not significantly decrease. It should stay about the same if we drop variables that are not helpful in understanding the model, and could improve because we are decreasing the number of variables being evaluated.\n\n\nlibrary(ISLR)\ndata(\"Credit\")\nstr(Credit)\n\n'data.frame':   400 obs. of  12 variables:\n $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n $ Gender   : Factor w/ 2 levels \" Male\",\"Female\": 1 2 1 2 1 1 2 1 2 2 ...\n $ Student  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 1 1 1 1 2 ...\n $ Married  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 2 1 1 1 1 2 ...\n $ Ethnicity: Factor w/ 3 levels \"African American\",..: 3 2 2 2 3 3 1 2 3 1 ...\n $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n\nlibrary(tidyverse)\n\n## Removing variables that are of no interest to the model based on\n## what we have learned so far.  The following lines removes the\n## categorical variables, Gender, Student, Married, and Ethnicity. It\n## also removed the ID because that is not a helpful variable.\n\nCredit &lt;- select(Credit, -Gender, -Student, -Married, -Ethnicity, -ID)\n\ncreditlm &lt;- lm(Balance ~ ., data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ ., data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-227.25 -113.15  -42.06   45.82  542.97 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -477.95809   55.06529  -8.680  &lt; 2e-16 ***\nIncome        -7.55804    0.38237 -19.766  &lt; 2e-16 ***\nLimit          0.12585    0.05304   2.373  0.01813 *  \nRating         2.06310    0.79426   2.598  0.00974 ** \nCards         11.59156    7.06670   1.640  0.10174    \nAge           -0.89240    0.47808  -1.867  0.06270 .  \nEducation      1.99828    2.59979   0.769  0.44257    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.6 on 393 degrees of freedom\nMultiple R-squared:  0.8782,    Adjusted R-squared:  0.8764 \nF-statistic: 472.5 on 6 and 393 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8782, Adjusted R-squared: 0.8764\n\n\nEducation has the highest p-value, so it is removed from analysis. We use the minus sign to remove the variable in the lm function.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-231.37 -113.46  -39.55   41.66  544.35 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -449.36101   40.57409 -11.075   &lt;2e-16 ***\nIncome        -7.56211    0.38214 -19.789   &lt;2e-16 ***\nLimit          0.12855    0.05289   2.430   0.0155 *  \nRating         2.02240    0.79208   2.553   0.0110 *  \nCards         11.55272    7.06285   1.636   0.1027    \nAge           -0.88832    0.47781  -1.859   0.0638 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.6 on 394 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8765 \nF-statistic: 567.4 on 5 and 394 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8781, Adjusted R-squared: 0.8765\n\n\nCards has the next highest p-value, so it is removed from analysis using the - minus sign format.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-249.62 -110.89  -39.98   51.87  546.52 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -445.10477   40.57635 -10.970  &lt; 2e-16 ***\nIncome        -7.61268    0.38169 -19.945  &lt; 2e-16 ***\nLimit          0.08183    0.04461   1.834   0.0674 .  \nRating         2.73142    0.66435   4.111 4.79e-05 ***\nAge           -0.85612    0.47841  -1.789   0.0743 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161.9 on 395 degrees of freedom\nMultiple R-squared:  0.8772,    Adjusted R-squared:  0.876 \nF-statistic: 705.6 on 4 and 395 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8772, Adjusted R-squared: 0.876\n\n\nAge is next to be removed because it has the next highest p-value at .06270.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards - Age, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards - Age, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-260.93 -113.14  -36.27   49.35  554.23 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -489.72748   32.09892 -15.257  &lt; 2e-16 ***\nIncome        -7.71931    0.37806 -20.418  &lt; 2e-16 ***\nLimit          0.08467    0.04471   1.894    0.059 .  \nRating         2.69858    0.66594   4.052 6.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162.4 on 396 degrees of freedom\nMultiple R-squared:  0.8762,    Adjusted R-squared:  0.8753 \nF-statistic: 934.6 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8762, Adjusted R-squared: 0.8753\n\n\nFinally Limit is the next to be removed because it has the next highest (and still insignificant) p-value at .059.\nAge is next to be removed because it has the next highest p-value at .06270.\n\n\ncreditlm &lt;- lm(Balance ~ . - Education - Cards - Age - Limit, data = Credit)\nsummary(creditlm)\n\n\nCall:\nlm(formula = Balance ~ . - Education - Cards - Age - Limit, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-278.57 -112.69  -36.21   57.92  575.24 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -534.81215   21.60270  -24.76   &lt;2e-16 ***\nIncome        -7.67212    0.37846  -20.27   &lt;2e-16 ***\nRating         3.94926    0.08621   45.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 162.9 on 397 degrees of freedom\nMultiple R-squared:  0.8751,    Adjusted R-squared:  0.8745 \nF-statistic:  1391 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n# Multiple R-squared: 0.8751,Adjusted R-squared: 0.8745\n\n\nIn our original model with non-helpful insignificant predictors, we had an adjusted R-Squared at 0.8764. After removing insignificant predictors, we are only down to .8745. This means we can explain someones Balance best by understanding Income and Rating.\nThe interpretation is as follows:\nA one unit change in Income causes a negative 7.67 change in Balance.\nA one unit change in Rating causes a 3.949 change in Balance.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "descriptives.html#using-ai",
    "href": "descriptives.html#using-ai",
    "title": "3  Descriptive Statistics",
    "section": "3.4 Using AI",
    "text": "3.4 Using AI\nUse the following prompts on a generative AI, like chatGPT, to learn more about descriptive statistics.\n\nWhat is the difference between mean, median, and mode in describing data distributions, and how can each be used to understand the shape of a distribution? * How do mean and median help identify whether a distribution is skewed, and what does it tell us about the dataset?\nCan you explain how the mean, median, and mode behave in normal, positively skewed, and negatively skewed distributions?\nWhat are standard deviation (SD) and variance, and how do they measure the spread of data in a distribution?\nExplain the differences between range, interquartile range (IQR), and standard deviation in describing the variability in a dataset.\nHow does a high standard deviation or variance affect the interpretation of a dataset compared to a low standard deviation?\nWhat is skewness, and how does it affect the shape of a distribution? How can we identify positive and negative skew?\nHow is kurtosis defined in the semTools package in R, and what does it tell us about the tails of a distribution?\nHow would you compare and contrast the roles of skewness and kurtosis in identifying the shape and behavior of a distribution?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "13  Recommended Further Reading Materials",
    "section": "",
    "text": "Harris, J. K. (2019). Statistics with R: Solving problems using real-world data. SAGE Publications.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer. (Springer Texts in Statistics)\nJaggia, S., & Kelly, A. (2018). Business statistics: Communicating with numbers (3rd ed.). McGraw-Hill Education. }",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Recommended Further Reading Materials</span>"
    ]
  },
  {
    "objectID": "index.html#accessing-data-files-for-the-course",
    "href": "index.html#accessing-data-files-for-the-course",
    "title": "Probability And Business Statistics",
    "section": "1.5 Accessing Data Files for the Course",
    "text": "1.5 Accessing Data Files for the Course\n\nTo download the data sets, go to our Blackboard datasets page and download them to your computer\nOnce downloaded, unzip the file by right-clicking and selecting “Extract All”, and then move the subfolder named data to your working directory.\nIn the example below, my project folder for the class is called BUAD231 and the subfolder that contains all the data files is called data. You can put your folder anywhere on the hard drive of your computer, but do not download it to places on the cloud like OneDrive.\n\n\n\n\nSaving Our Book Files Into a “data” Folder in Your Working Directory\n\n\n\nWe will use a number of data files plus more for homework/projects, so be prepared to use these files throughout the class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "correlation.html#using-ai",
    "href": "correlation.html#using-ai",
    "title": "10  Correlation Analysis",
    "section": "10.7 Using AI",
    "text": "10.7 Using AI\n\nUse the following prompt on a generative AI, like chatGPT, to learn more about correlation analysis.\nExplain the difference between covariance and correlation. How do you interpret the direction and strength of a correlation coefficient?”\nWhat is covariance, and how does it describe the relationship between two variables? Provide an example with a simple dataset. Describe Pearson’s correlation coefficient. How do you calculate it, and what does it tell you about the relationship between two variables?\nExplain how to create a scatterplot using ggplot2 in R to visualize the relationship between two continuous variables. How can you add a regression line to assess the linearity of the relationship?\nHow can you use the cor.test() function in R to conduct a hypothesis test for correlation? What do the p-value and confidence interval indicate in the context of correlation?\nHow do you interpret the strength and direction of a correlation? What is considered a strong, moderate, or weak correlation in the context of business data?\nExplain the null and alternative hypotheses for correlation testing. How does the t-distribution play a role in testing the significance of a correlation coefficient?\nWhat are some challenges of visualizing relationships between variables when dealing with big data, and how can statistical tests help when scatterplots are not effective?\nDiscuss the limitations of correlation analysis. Why is it important to be cautious when interpreting correlation as causation?\nWhat are the key differences between correlation and causation? How can you determine if a correlation implies causality?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "ttests.html#using-ai",
    "href": "ttests.html#using-ai",
    "title": "8  Hypothesis Testing with t-Tests",
    "section": "8.6 Using AI",
    "text": "8.6 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about t.test’s and confidence intervals.\nCan you explain the difference between the null hypothesis and the alternative hypothesis, and why is it important to set up both correctly before conducting a t-test?\nWhat factors determine whether you should use a one-sample, independent samples, or dependent samples t-test, and how do the results differ based on the test chosen?\nWhen should I use a two-tailed t-test compared to a right- or left-tailed test, and how does the alternative hypothesis guide this decision?\nWhat are the key arguments in the t.test() function in R, and how do I modify these arguments to perform different types of t-tests, such as two-tailed or paired t-tests?\nHow do I interpret the p-value in the context of hypothesis testing, and how does it relate to rejecting or failing to reject the null hypothesis?\nHow does the confidence interval calculated during a t-test help in understanding the range of possible population parameters, and why is it important in hypothesis testing?\nWhat is the difference between an independent samples t-test and a dependent (paired) samples t-test, and in what situations should each be used?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing with t-Tests</span>"
    ]
  },
  {
    "objectID": "anova.html#using-ai",
    "href": "anova.html#using-ai",
    "title": "9  Analysis of Variance (ANOVA)",
    "section": "9.6 Using AI",
    "text": "9.6 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about ANOVA analysis.\nExplain how ANOVA is used to compare group means. What are the assumptions of ANOVA, and how do we interpret the F-statistic and p-value?\nWalk me through the steps of conducting a one-way ANOVA. Explain how to use the aov() function in R and how to interpret the results, including the F-statistic and p-value.\nDescribe how to conduct a two-way ANOVA in R. What are the key differences between one-way and two-way ANOVA, and how do we interpret multiple F-statistics?\nWhat are post-hoc tests, and why are they important after conducting an ANOVA? How do the TukeyHSD and Bonferroni methods differ in terms of reducing Type I and Type II errors?\nExplain Type I and Type II errors in the context of hypothesis testing. How do these errors relate to the significance level (alpha) and the power of a test?\nGiven an R output from the aov() or anova() function, explain how to interpret the degrees of freedom, sum of squares, mean square, F-statistic, and p-value.\nProvide a step-by-step guide to performing a one-way ANOVA in R using the aov() function. Include code examples and explain how to interpret the summary output.”\nCompare the TukeyHSD and Bonferroni post-hoc tests. Which one is more conservative, and how do they help in reducing error in multiple comparisons?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "dataviz.html#using-ai",
    "href": "dataviz.html#using-ai",
    "title": "5  Data Visualization",
    "section": "5.6 Using AI",
    "text": "5.6 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about data visualization capabilities in ggplot2.\nHow can I modify the appearance of a ggplot bar chart to include custom colors for each bar, and what are the best practices for choosing colors in data visualization?\nWhat is the role of layering in ggplot, and how can adding multiple layers, such as labels, themes, and lines, improve the readability of a plot?\nWhen should a density plot be used instead of a histogram, and how does each visualization help in understanding the distribution of continuous data?\nHow can I use a boxplot in R to identify and visualize outliers in my dataset, and what additional steps should I take to handle these outliers?\nHow can I create a scatter plot in ggplot to explore relationships between two continuous variables, and how do I add a trendline to help interpret the results?\nWhat are the steps for creating a grouped bar chart in ggplot, and how does this visualization help in comparing multiple categories or groups within a dataset?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "regression.html#using-ai",
    "href": "regression.html#using-ai",
    "title": "11  Linear Regression Analysis",
    "section": "11.5 Using AI",
    "text": "11.5 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about regression analysis in R.\nExplain how to perform simple linear regression in R. Use the lm() function and discuss how to interpret the slope, intercept, and R-squared values from the regression output.\nWhat is the coefficient of determination (R-squared) in regression analysis, and how does it relate to the goodness of fit? How can the residual standard error (RSE) and adjusted R-squared help in interpreting a model?\nDescribe how to perform multiple linear regression in R using the lm() function. How do you interpret the coefficients for multiple predictors, and what do the p-values tell you about their significance?\nWhat does the F-statistic in a regression output indicate? How does it help in determining the significance of a regression model, and what role does it play when comparing multiple models?\nExplain how ANOVA is related to regression analysis. How can the F-test be used to assess the significance of regression models? What are residuals in a regression model, and how can analyzing them help assess model fit? How is the sum of squared errors (SSE) related to residual analysis?\nWhat is multicollinearity, and why is it problematic in multiple regression? How can you detect multicollinearity in R using the vif() function, and what steps can be taken to address it?\nHow can you use a fitted regression model to make predictions for new data in R? Explain how the predict() function works and provide an example using both simple and multiple regression.\nGiven the output from the summary() function for a regression model in R, explain how to interpret the coefficients, R-squared, adjusted R-squared, residual standard error, and p-values.\nDescribe how to use the p-value method for variable selection in multiple regression. How do you iteratively remove insignificant predictors while monitoring the R-squared and adjusted R-squared values?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression Analysis</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "Intro to RMarkdown",
    "section": "",
    "text": "12.1 yaml Header",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#yaml-header",
    "href": "rmarkdown.html#yaml-header",
    "title": "Intro to RMarkdown",
    "section": "",
    "text": "The YAML header is located at the beginning of the RMD file and contains metadata about the document, such as the title, author, date, and output format. Here is an example of a YAML header:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#markdown-text",
    "href": "rmarkdown.html#markdown-text",
    "title": "Intro to RMarkdown",
    "section": "12.2 markdown Text",
    "text": "12.2 markdown Text\n\nMarkdown is a lightweight markup language with plain text formatting syntax. In an RMD file, you use Markdown to format the text, create lists, headers, links, and more. For example:\n\n\n\n\nStyles Example",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#code-chunks",
    "href": "rmarkdown.html#code-chunks",
    "title": "Intro to RMarkdown",
    "section": "12.3 Code Chunks",
    "text": "12.3 Code Chunks\n\nCode chunks allow you to embed R (or other languages) code within the document. These chunks are executed when the document is rendered, and their output is included in the final document. Code chunks are defined with triple backticks and the language, like this:\n\n\n\n\nCode Chunk Example\n\n\n\nChunk output can be customized with knitr options, arguments set in the {} of a chunk header.\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = “…” adds a caption to graphical results.\n\n\n\n\n\nArguments of Chunk",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#output-formats",
    "href": "rmarkdown.html#output-formats",
    "title": "Intro to RMarkdown",
    "section": "12.4 Output Formats",
    "text": "12.4 Output Formats\n\nRMD files can be rendered into various output formats including HTML, PDF, and Word documents. The output format is specified in the YAML header. To generate the final document, you use the knit function in RStudio or call rmarkdown::render() in your R script.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#visualization-and-plots",
    "href": "rmarkdown.html#visualization-and-plots",
    "title": "Intro to RMarkdown",
    "section": "12.5 Visualization and Plots",
    "text": "12.5 Visualization and Plots\n\nRMD files can include visualizations and plots. For example, you can create and embed a plot using the ggplot2 package.\n\nlibrary(ggplot2) ggplot(mpg, aes(x=displ, y=hwy)) + geom_point()\n\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Intro to RMarkdown</span>"
    ]
  },
  {
    "objectID": "dataprep.html#coercing-data-types",
    "href": "dataprep.html#coercing-data-types",
    "title": "4  Data Preparation",
    "section": "4.2 Coercing Data Types",
    "text": "4.2 Coercing Data Types\n\nIn R, coercion is the process of converting one data type to another, often done automatically by R when necessary. For example, if you combine numeric and character data in a vector, R will coerce the numeric values to characters, as seen in c(1, “A”), which results in c(“1”, “A”). Coercion can also be done manually using functions like as.numeric() or as.factor() to change data types explicitly.\nTo determine whether a variable is numeric or categorical (factor), you can use the class() function. For example, if you have a variable age, running class(age) will tell you if it is stored as “numeric” or “factor”. Numeric variables represent quantities, such as continuous or discrete numbers, while categorical variables represent groups or categories, typically stored as factors. For example, a variable with levels like “Male” and “Female” would be a categorical (factor) variable.\n\n\n4.2.1 Factor data type:\n\nOrdinal: Contain categories that have some logical order (e.g. categories of age).\nNominal: Have categories that have no logical order (e.g., religious affiliation and marital status).\nR will treat each unique value of a factor as a different level.\n\n\n4.2.1.1 Ordinal Variable\n\nOrdinal data may be categorized and ranked with respect to some characteristic or trait.\n\nFor example, instructors are often evaluated on an ordinal scale (excellent, good, fair, poor).\nA scale allows us to code the data based on order, assuming equal distance between scale items (aka likert items).\nYou can make an ordinal factor data type in R or you can convert the order to meaningful numbers.\n\nTo recode numbers in R, we would code poor to excellent = 1, 2, 3, 4 respectively.\nThe recode() function from the dplyr package within the tidyverse ecosystem is used to replace specific values in a vector or variable with new values.\nIt allows you to map old values to new ones in a simple and readable manner.\n\n\nlibrary(tidyverse)\n# Take a vector representing evaluation scores, named evaluate \nevaluate &lt;- c(\"excellent\", \"good\", \"fair\", \"poor\", \"excellent\", \"good\")\n\ndata &lt;- data.frame(evaluate)\ndata &lt;- data %&gt;%\n     mutate(evaluate = recode(evaluate,\n            \"excellent\" = 4,\n            \"good\" = 3,\n            \"fair\" = 2,\n            \"poor\" = 1))\ndata\n\n  evaluate\n1        4\n2        3\n3        2\n4        1\n5        4\n6        3\n\n\n\n\n4.2.1.2 Nominal Variable\n\nWith nominal variables, data are simply categories for grouping.\nFor example, coding race/ethnicity might have a category value of White, Black, Native American, Asian/Pacific Islander, Other.\nQualitative values may be converted to quantitative values for analysis purposes. + White = 1, Black = 2, etc. This conversion to numerical representation of the category would be needed to run some analysis.\n\nSometimes, R does this on our behalf depending on commands used.\n\nWe can force a variable into a factor data type using the as.factor() command.\nIf we use the read.csv() command, we can sometimes do this by setting an argument \\(stringsAsFactors=TRUE\\). We will do this later in the lesson.\n\n\n\n\n4.2.2 Numerical data types:\n\nA numerical data type is a vector of numbers (Real or Integer)\nContinuous (Real) variables can take any value along some continuum, hence continuous.\n\nWe can force a variable into a numerical data type by using the as.numeric() command.\nFor example, we could collect information on a participants age, height, weight, or distance traveled.\n\nTwo ways to create:\n\nWe can create a numeric variable by ensuring our value we assign is a number!\nWe can force a variable into an real number data type by using the as.numeric() command.\n\n\n# Assign Rhode Island limit for medical marijuana in ounces per\n# person\nkOuncesRhode &lt;- 2.5\n# Identify the data type\nclass(x = kOuncesRhode)\n\n[1] \"numeric\"\n\n\nDiscrete (Integer) Variables:\n\nDiscrete variables can only take a countable number of distinct values.\nWe can force a variable into an integer data type by using the as.integer() command.\n\nFor example, we could collect information on the number of children in a family or number of points scored in a basketball game.\n\n\n# Assign the value of 4 to a constant called kTestInteger and set as\n# an integer\nkTestInteger &lt;- as.integer(4)\nclass(kTestInteger)  #Confirm the data type is an integer \n\n[1] \"integer\"\n\n# Use as.integer() to truncate the variable ouncesRhode\nTrunc &lt;- as.integer(kOuncesRhode)\nTrunc\n\n[1] 2\n\n\n\n\n4.2.3 Character data type\n\nWrapped in either single or double quotation marks.\n\nIncludes letters, words, or numbers that cannot logically be included in calculations (e.g., a zip code).\nA quick example is below that shows how to assign a character value to a variable.\n\n\n\n# Make constants\nkFirstName &lt;- \"Pamela\"\nkLastName &lt;- \"Schlosser\"\n# Check the data type\nclass(x = kFirstName)\n\n[1] \"character\"\n\n# Create a zip code constant and check the data type\nkZipCode &lt;- \"23185\"\nclass(x = kZipCode)\n\n[1] \"character\"\n\n\n\n\n4.2.4 Logical data type\n\nValues of TRUE and FALSE\nResult of some expression.\n\nA quick example is below that shows how to assign a logical value to a variable.\n\n\n# Store the result of 6 &gt; 8 in a constant called kSixEight\nkSixEight &lt;- 6 &gt; 8\n# Can use comparison tests with the following == &gt;= &lt;= &gt; &lt; &lt;&gt; !=\nkSixEight  # Print kSixEight\n\n[1] FALSE\n\n# Determine the data type of kSixEight\nclass(x = kSixEight)\n\n[1] \"logical\"\n\n\n\n\n\n4.2.5 Date data type\n\nA variable that should be a date.\n\n\n# Convert date info in format 'mm/dd/yyyy' using as.Date\nstrDates &lt;- c(\"01/05/1965\", \"08/16/1975\")\ndates &lt;- as.Date(strDates, \"%m/%d/%Y\")\nstr(dates)\n\n Date[1:2], format: \"1965-01-05\" \"1975-08-16\"\n\n\n\nlubridate is a package specifically for converting dates. This package makes dates a lot easier to work with.\n\n\n# Convert date info in format 'mm/dd/yyyy' using lubridate\nlibrary(lubridate)\nstrDates &lt;- c(\"01/05/1965\", \"08/16/1975\")\ndates &lt;- mdy(strDates)\nstr(dates)\n\n Date[1:2], format: \"1965-01-05\" \"1975-08-16\"\n\n\n\nIf you are only given a year and a month, you can use the ym() command to turn it to a date. But take note that it will add a day to the value as a placeholder.\n\n\n# Convert date info in format 'yyyymm' using lubridate\nstryyyymm &lt;- c(\"202201\", \"202003\", \"202204\")\ndates &lt;- ym(stryyyymm)\nstr(dates)\n\n Date[1:3], format: \"2022-01-01\" \"2020-03-01\" \"2022-04-01\"\n\n\n\n\n4.2.6 Nominal Example with Dataset\n\nlibrary(tidyverse)\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\n\n\n# Examine the variable types with summary and class functions.\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nclass(gss.2016$grass)  #Check the data type.\n\n[1] \"character\"\n\ngss.2016$grass &lt;- as.factor(gss.2016$grass)  #Turn to a factor.\nclass(gss.2016$grass)  #Confirming it is now correct.\n\n[1] \"factor\"\n\n\n\n\n4.2.7 Numerical Example with Dataset\n\nWe need to ensure data can be coded as numeric before using the as.numeric() command. For example, to handle the variable age, it seems like numerical values except one value of “89 OR OLDER”. If as.numeric() command was used on this variable, it would put all the 89 and older observations as NAs. To force it to be a numerical variable, and keep that the sample participants were the oldest value, we need to recode it and then use the as.numeric() command to coerce it into a number.\nRecoding the 89 and older to 89 does cause the data to lack integrity in its current form because it will treat the people over 89 years old as 89. But, we are limited here because this needs to be a numerical variable for us to proceed. We will learn a step later on in this section to transform the age variable into categories so that we bring back our data integrity.\n\n\nclass(gss.2016$age)\n\n[1] \"character\"\n\n# Recode '89 OR OLDER' into just '89'\ngss.2016$age &lt;- recode(gss.2016$age, `89 OR OLDER` = \"89\")\n# Convert to numeric data type\ngss.2016$age &lt;- as.numeric(gss.2016$age)\nsummary(gss.2016)  #Conduct final check confirming correct data types\n\n       grass           age       \n DK       : 110   Min.   :18.00  \n IAP      : 911   1st Qu.:34.00  \n LEGAL    :1126   Median :49.00  \n NOT LEGAL: 717   Mean   :49.16  \n NA's     :   3   3rd Qu.:62.00  \n                  Max.   :89.00  \n                  NA's   :10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#using-ai",
    "href": "dataprep.html#using-ai",
    "title": "4  Data Preparation",
    "section": "4.5 Using AI",
    "text": "4.5 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about data preparation activities.\nWhat is the difference between ordinal and nominal variables, and how can you recode these data types in R?\nExplain the steps to coerce a character variable into a factor using the as.factor() function in R.\nHow can you use the filter() function to subset a dataset based on multiple conditions using & and | in R?\nHow does subsetting using square brackets [] differ from using the filter() function in R?\nWhat strategies can you use to handle missing data in R, and how does na.omit() differ from drop_na()?\nHow does the mutate() function help in transforming and creating new variables, and what are some practical examples?”\nWhat is the purpose of the group_by() function, and how does it interact with summarize() to create summary statistics in R?”\nExplain how you can use arrange() to sort a dataset by one or more variables and demonstrate sorting both in ascending and descending order.”\nWhy is the piping operator %&gt;% useful in R, and how does it improve the readability and structure of your code? * How would you use summarize() to calculate mean, median, and standard deviation for a numerical variable in R?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "introR.html#using-ai",
    "href": "introR.html#using-ai",
    "title": "2  Introduction to R and RStudio",
    "section": "2.10 Using AI",
    "text": "2.10 Using AI\nUse the following prompts on a generative AI, like chatGPT, to learn more about introductory topics.\n\nExplain the difference between descriptive and inferential statistics and provide real-life examples of both.\nWhat is the purpose of using R in statistical analysis, and what are the key benefits of using RStudio as a graphical interface?\nWhat happens when you assign the same variable multiple values in R? Write an example script that demonstrates this behavior and explains the output.\nCreate a script that demonstrates how to assign values to variables using both numeric and character data types. Then, explain how these assignments are stored in RStudio’s environment.\nIn R, what is the role of the assignment operator &lt;- Demonstrate its use by creating a few variables for numeric and character data types.\nDemonstrate how to create a vector in R using the c() function. Use this vector to perform basic operations like addition and multiplication.\nWrite a script that reads a CSV file into R using read.csv(). Summarize the dataset and explain how the columns and rows are structured.\nHow can you access specific columns of a data frame using the $ operator? Provide an example using a sample dataset in R.\nExplain how to use the summary() function in R to summarize a dataset. Write a script that loads a dataset and runs summary() on it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "contingency.html#using-ai",
    "href": "contingency.html#using-ai",
    "title": "7  Contingency Tables and Chi-Squared",
    "section": "7.4 Using AI",
    "text": "7.4 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about probability and contingency tables.\nWhat is a contingency table, and how is it used to represent the relationship between two categorical variables in probability calculations?\nHow do you calculate joint, marginal, and conditional probabilities using a contingency table, and what is the significance of each type of probability?\nCan you explain how the chi-squared test for independence works, and how it is used to determine whether two categorical variables are independent or dependent?\nWhat is the difference between the chi-squared test for independence and the chi-squared goodness of fit test, and how is each test applied?\nOnce you calculate the chi-squared test statistic, how do you interpret the p-value to determine whether to reject or fail to reject the null hypothesis?\nWhy are degrees of freedom important in a chi-squared test, and how do you calculate them for a contingency table?\nHow do you set up and execute a chi-squared test using the chisq.test() function in R, and what are the key outputs to focus on when interpreting the results?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contingency Tables and Chi-Squared</span>"
    ]
  },
  {
    "objectID": "probability.html#using-ai",
    "href": "probability.html#using-ai",
    "title": "6  Probability and Probability Distributions",
    "section": "6.9 Using AI",
    "text": "6.9 Using AI\n\nUse the following prompts on a generative AI, like chatGPT, to learn more about probability and contingency tables.\nWhat are the key characteristics of a probability distribution, and how do you determine whether a given set of values represents a valid probability distribution?\nWhat is the difference between a discrete and a continuous random variable, and how do the probability distributions differ for each type?\nHow do you calculate the expected value and variance for a discrete random variable, and why are these summary measures important in understanding probability distributions?\nWhat are the properties of a binomial distribution, and how is it used to calculate the probability of a certain number of successes in a fixed number of trials?\nHow do you use the dbinom() and pbinom() functions in R to calculate the probability of exact or cumulative successes in a binomial experiment?\nWhat is a normal distribution, and how do you calculate z-scores to determine how far an observation is from the mean of a normally distributed variable?\nHow do you interpret and calculate cumulative probabilities for both discrete and continuous variables using the cumulative distribution function (CDF)”\nWhat is the Central Limit Theorem (CLT), and why is it important when working with large samples and understanding the distribution of sample means?\nWhen data is not normally distributed, what transformations can you apply to make the data more normally distributed, and how do you determine which transformation is most effective?\nHow do you apply probability functions such as pnorm() and qnorm() in R to solve real-world problems, such as calculating the likelihood of events based on normal distributions?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "correlation.html#interpreting-the-significance-of-the-correlation",
    "href": "correlation.html#interpreting-the-significance-of-the-correlation",
    "title": "10  Correlation Analysis",
    "section": "10.4 Interpreting the Significance of the Correlation",
    "text": "10.4 Interpreting the Significance of the Correlation\n\nCorrelation values should be tested alongside a p-value to confirm whether or not there is a correlation. The null is tested using a t-distribution specifically testing whether \\(r = 0\\) or not, like the one-sample t-test section from the lesson 6.\nThe null and alternative are listed below.\n\n\\(H_0\\): There is no relationship between the two variables (\\(r = 0\\)).\n\\(H_A\\): There is a relationship between the two variables (\\(r \\neq 0\\)).\n\nEven small correlations can be significant: In large datasets, even a small correlation, like .1, can be statistically significant due to the increased power that comes with a high sample size. It’s important to interpret both the strength of the correlation and its practical significance in context.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#statistical-significance-answers-the-question-is-the-effect-real",
    "href": "correlation.html#statistical-significance-answers-the-question-is-the-effect-real",
    "title": "10  Correlation Analysis",
    "section": "10.5 Statistical significance answers the question: “Is the effect real?“",
    "text": "10.5 Statistical significance answers the question: “Is the effect real?“\n\nStatistical Significance:\nA result is statistically significant if it is unlikely to have occurred by random chance, given a pre-defined threshold (usually p &lt; 0.05).\nWith larger sample sizes, even very small effects can become statistically significant because larger samples reduce variability. For example, a correlation of 0.1 can be statistically significant with enough data.\nPractical Significance:\nPractical significance answers the question: “Is the effect meaningful?“\nPractical significance refers to the real-world importance or relevance of a result. It asks, “Does this effect matter in practice?“\nEven if a result is statistically significant, it may not be large enough to have a meaningful impact on business decisions or outcomes.\n\n\n10.5.1 cor.test() Command\n\nThe cor() command gives you just the correlation coefficient. This command can be useful if you are testing many correlations at one time. In the below statement, I can use \\(cor(Variable1, Variable2)\\) to see the correlation between 2 continuous variables.\n\n\ncor(Debt_Payments$Income, Debt_Payments$Debt)\n\n[1] 0.8675115\n\n\n\nThe cor.test() command tests the hypothesis whether \\(r=0\\) or not. This command comes with a p-value and t-test statistic (along with the correlation coefficient).\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Debt)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Debt\nt = 8.544, df = 24, p-value = 9.66e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7231671 0.9392464\nsample estimates:\n      cor \n0.8675115 \n\n\n\nThis test shows a strong positive correlation of .8675 (&gt;.8) which is significant. Our p-value is 9.66e-09 or &lt; .001 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between .723 and .939.\n\n\ncor.test(Debt_Payments$Income, Debt_Payments$Unemployment)\n\n\n    Pearson's product-moment correlation\n\ndata:  Debt_Payments$Income and Debt_Payments$Unemployment\nt = -3.0965, df = 24, p-value = 0.004928\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7636089 -0.1852883\nsample estimates:\n       cor \n-0.5342931 \n\n\n\nThis test shows a moderate negative correlation of -.534 (&lt;.5 and .8) which is significant. Our p-value is 0.004928 or &lt; .01 alpha level. This suggests that we reject the null hypothesis and support the alternative that \\(r \\neq 0\\) which confirms a correlation is present.\nWe also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between -.765 and -.185. This confidence interval is wider than the one listed above. This is due to the noise in the relationship we noted in the scatterplot - the correlation is weaker, the relationship does not look as linear, the confidence decreases. Even though this is true, we must note that we still found a significant correlation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "dataviz.html#layering-in-ggplot",
    "href": "dataviz.html#layering-in-ggplot",
    "title": "5  Data Visualization",
    "section": "5.1 Layering in ggplot",
    "text": "5.1 Layering in ggplot\n\nLayering is a fundamental concept in ggplot2 that allows you to build complex visualizations by adding different components (or “layers”) on top of each other. Each layer in a ggplot2 plot can represent different types of data, aesthetics, or annotations, enabling flexibility and control over how data is visualized. Layers can include geometries (e.g., points, lines, bars), statistical summaries, labels, grids, and themes.\nSeparation of Plot Components: Each layer can handle a different part of the plot. For example, one layer may be used for bars, another for labels, and another for a trend line. This allows you to build up a plot step by step.\nCustomization and Enhancement: By adding multiple layers, you can customize different aspects of the plot such as labels, colors, annotations, and theme elements. Each layer can be independently controlled.\nModularity: Layering allows you to modularize your plot construction, making it easier to add, remove, or modify parts of the plot without changing the entire structure.\nCombining Data Sources: Different layers can use different datasets or aesthetics, which is useful when you need to overlay one dataset on top of another (e.g., adding a regression line over a scatter plot).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  }
]