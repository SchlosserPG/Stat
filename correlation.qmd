# Correlation Analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy.opts = list(width.cutoff = 70), tidy = TRUE, message=FALSE, warning=FALSE)
```

-   The main goal of this lesson is to understand and conduct correlation analysis and interpret its results. In doing so, we will examine Pearson’s r correlation coefficient and learn how to conduct and interpret an inferential statistical test using this common statistic.

```{r tidy=FALSE}
####################################
# Project name: Covariance and Measures of Association
# Data used: Debt_Payments.csv
# Libraries used: tidyverse, ggplot2
####################################
```

```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
```

### Lesson Objectives

-   Compute and interpret Pearson’s r correlation coefficient.
-   Conduct an inferential statistical test for Pearson’s r correlation coefficient.

### Consider While Reading

-   In the lesson, we are looking at inference for correlation. We will learn how to conduct a correlation test. This will close the loop on scatterplots, which we learned in the Data Visualization lesson help us describe visually a relationship between variables. Consider what we learned and perhaps revisit the scatterplot lecture notes to then determine how visualizations can help in making inferences alongside regression and correlation results.

## Covariance

-   Covariance ($s_{xy}$ or $cov_{xy}$) is a numerical measure that describes the direction of the linear relationship between two variables, x and y and reveals the direction of that linear relationship.
-   The formula for covariance is as follows:
    -   $cov_{xy} = \sum^n_{i=1}(x_i-m_x)*(y_i-m_y)/(n-1)$
    -   Where $x_i$ and $y_i$ are the observed values for each observation, $m_x$ and $m_y$ are the mean values for each variable, $i$ represents an individual observation, and $n$ represents the sample size.

```{r}

x <- c(3, 8, 5, 2)
y <- c(12, 14, 8, 4)

devX <- x-mean(x)
devY <- y-mean(y)

covXY <- sum(devX * devY)/(length(x)-1); covXY

#We can verify this by using cov() function in R.
cov(x,y)
```

## Correlation Coefficient

-   A correlation coefficient ($r_{xy}$) describes both the direction and strength of the relationship between $x$ and $y$. $r_{xy} = cov_{sy}/(s_xs_y)$ or using the standardized formula in the book:
    -   $r_{xy} = \sum^n_{i=1}(z_x*z_y)/(n-1)$

```{r}
#Calculated manually
covXY/(sd(x)*sd(y))

#We can verify this by using cor() function in R.
cor(x,y)

```

### Rules for the Correlation Coefficient

-   The correlation coefficient has the same sign as the covariance; however, its value ranges between −1 and +1 whereas $-1 \le r_{xy} \le +1$.
-   The absolute value of the coefficient reflects the strength of the correlation. So a correlation of −.70 is stronger than a correlation of +.50.

![Correlation Coefficient](Pictures/Ch8/Correlation.png "Correlation Coefficient")

## Interpreting the Direction of the Correlation

-   Negative correlations occur when one variable goes up and the other goes down.
-   No correlation happens when there is no discernible pattern in how two variables vary.
-   Positive correlations occur when one variable goes up, and the other one also goes up (or when one goes down, the other one does too); both variables move together in the same direction.

![Correlation Scatterplots](Pictures/Ch8/RelationshipScatter.png "Correlation Scatterplots")

### Scatterplots to Visualize Relationship

-   Let's do an example to first visualize the data, and then to calculate the correlation coefficient.

-   First, read in a .csv called *DebtPayments.csv*. This data set has 26 observations and 4 variables:

    -   A character variable with a bunch of metropolitan areas listed;
    -   An integer numeric debt;
    -   A numeric variable Income;
    -   A numeric variable Unemployment.

```{r}
Debt_Payments <- read.csv("data/DebtPayments.csv")
str(Debt_Payments)
```

-   Next, plot the relationship between 2 continuous variables.

    -   There are a few ways to write the plot command using ggplot. We went over these in the Data Visualization lesson. Again we said:
    -   Layer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.
    -   Layer 2: geom_point() command to add the observations as indicators in the chart.
    -   Layer 3 or more: many other optional additions like labs() command (for labels) or stat_smooth() command to generate a regression line.

    ```{r, fig.alt="Scatterplot Using ggplot2 Generated by R"}
    Debt_Payments %>% ggplot(aes(Income, Debt))+ 
      geom_point(color="#183028", shape=2) + 
      stat_smooth(method="lm", color="#789F90") + 
      theme_minimal()
    ```

-   In the above plot, there is a strong positive relationship (upward trend) that should be confirmed with a correlation test.

-   In a second example below, we look at Unemployment as the X variable. This scatterplot is much more difficult to use in determining whether the correlation will be significant. It looks negative, but there is not a strong linear trend to the data. This will also need to be confirmed with a correlation test.

```{r, fig.alt="Scatterplot Using ggplot2 Generated by R"}
Debt_Payments %>% ggplot(aes(Unemployment, Debt))+
  geom_point(color="#183028", shape=2) + 
  stat_smooth(method="lm", color="#789F90") + 
  theme_minimal()
```

-   In many scatterplots using big data, the observations are too numerous to see a good relationship. In that case, the statistical test can trump this visual aid. However, in a lot of cases the scatterplot does help visualize the relationship between 2 continuous variables.

### Interpreting the Strength of the Correlation

-   Statisticians differ on what is called a strong correlation versus weak correlation, and it depends on the context. A .9 may be required for a strong correlation in one field, and a .5 in another. Generally speaking in business, the absolute value of a correlation .8 or above is considered strong, between .5 and .8 is considered moderate, and between a .2 and .5 is considered weak.
-   The following is consistent with what is most generally used:

![Correlation Strength](Pictures/Ch8/CorrelationStrength.png "Correlation Strength")

## Hypothesis Testing with Correlations

-   Correlation values should be tested alongside a p-value to confirm whether or not there is a correlation. The null is tested using a t-distribution specifically testing whether $r = 0$ or not, like the one-sample t-test section from the lesson 6.
-   The null and alternative are listed below.
    -   $H_0$: There is no relationship between the two variables ($r = 0$).
    -   $H_A$: There is a relationship between the two variables ($r \neq 0$).

### cor.test() Command

-   The cor() command gives you just the correlation coefficient. This command can be useful if you are testing many correlations at one time. In the below statement, I can use $cor(Variable1, Variable2)$ to see the correlation between 2 continuous variables.

```{r}
cor(Debt_Payments$Income, Debt_Payments$Debt) 
```

-   The cor.test() command tests the hypothesis whether $r=0$ or not. This command comes with a p-value and t-test statistic (along with the correlation coefficient).

```{r}
cor.test(Debt_Payments$Income, Debt_Payments$Debt)
```

-   This test shows a strong positive correlation of .8675 (\>.8) which is significant. Our p-value is 9.66e-09 or \< .001 alpha level. This suggests that we reject the null hypothesis and support the alternative that $r \neq 0$ which confirms a correlation is present.
-   We also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between .723 and .939.

```{r}
cor.test(Debt_Payments$Income, Debt_Payments$Unemployment)
```

-   This test shows a moderate negative correlation of -.534 (\<.5 and .8) which is significant. Our p-value is 0.004928 or \< .01 alpha level. This suggests that we reject the null hypothesis and support the alternative that $r \neq 0$ which confirms a correlation is present.
-   We also see a confidence interval listed. It suggests that we are 95% confident that the correlation is between -.765 and -.185. This confidence interval is wider than the one listed above. This is due to the noise in the relationship we noted in the scatterplot - the correlation is weaker, the relationship does not look as linear, the confidence decreases. Even though this is true, we must note that we still found a significant correlation.

## Limitations of Correlation Analysis

-   To determine a causal model, you need the following:

1.  Significant Correlation: Statistically significant relationship between the variables.
2.  Temporal Precedence: Causal variable occurred prior to the other variable.
3.  Eliminate Alternate Variables: No other factors can account for the cause.

-   Some limitations are as follows:
    -   The correlation coefficient captures only a linear relationship.
    -   The correlation coefficient may not be a reliable measure in the presence of outliers.
    -   Even if two variables are highly correlated, one does not necessarily cause the other.
-   Note that a correlation is the first step in understanding causality.

## Using AI

* Use the following prompt on a generative AI, like chatGPT to learn more about correlation analysis. 

     + Understand the difference between correlation and causation. Use functions like cor(), cor.test(), and ggplot(). Reflect on the importance of considering other factors before concluding causation.


## Summary

-   In this lesson, we learned about correlation coefficient and how to evaluate a strong, moderate, or weak/positive or negative correlation. We also learned how to visualize a relationship using a scatterplot. We also learned how to test for a correlation being significant or not.
